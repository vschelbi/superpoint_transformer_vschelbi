<div align="center">

# Superpoint Transformer

[![python](https://img.shields.io/badge/-Python_3.8+-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_1.12+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![lightning](https://img.shields.io/badge/-Lightning_1.6+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![hydra](https://img.shields.io/badge/Config-Hydra_1.2-89b8cd)](https://hydra.cc/)
[![license](https://img.shields.io/badge/License-MIT-green.svg?labelColor=gray)](https://github.com/ashleve/lightning-hydra-template#license)

[//]: # ([![Paper]&#40;http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg&#41;]&#40;https://www.nature.com/articles/nature14539&#41;)
[//]: # ([![Conference]&#40;http://img.shields.io/badge/AnyConference-year-4b44ce.svg&#41;]&#40;https://papers.nips.cc/paper/2020&#41;)


Official implementation for
<br>
_Efficient 3D Semantic Segmentation with Superpoint Transformer_
<br>
ğŸš€âš¡ğŸ”¥
<br>


</div>

<p align="center">
    <img width="90%" src="./media/teaser.jpg">
</p>

## ğŸ“ŒÂ Â Description

SPT is a superpoint-based transformer ğŸ¤– architecture that efficiently âš¡ 
performs semantic segmentation on large-scale 3D scenes. This method includes a 
fast algorithm that partitions ğŸ§© point clouds into a hierarchical superpoint 
structure, as well as a self-attention mechanism to exploit the relationships 
between superpoints at multiple scales. 

## ğŸ“°Â Â Updates

- **15.06.2023 Official release** ğŸŒ±

## ğŸ—Â Â Installation
Simply run `install.sh` to install all dependencies in a new conda environment 
named `spt`. 
```bash
# Creates a conda env named 'spt' env and installs dependencies
./install.sh
```
See the [Datasets page](docs/dataset.md) for further details on supported 
datasets and the data structure. 

### ğŸ”©Â Â Project structure
```
â””â”€â”€ superpoint_transformer
    â”‚
    â”œâ”€â”€ configs                   # Hydra configs
    â”‚   â”œâ”€â”€ callbacks                 # Callbacks configs
    â”‚   â”œâ”€â”€ data                      # Data configs
    â”‚   â”œâ”€â”€ debug                     # Debugging configs
    â”‚   â”œâ”€â”€ experiment                # Experiment configs
    â”‚   â”œâ”€â”€ extras                    # Extra utilities configs
    â”‚   â”œâ”€â”€ hparams_search            # Hyperparameter search configs
    â”‚   â”œâ”€â”€ hydra                     # Hydra configs
    â”‚   â”œâ”€â”€ local                     # Local configs
    â”‚   â”œâ”€â”€ logger                    # Logger configs
    â”‚   â”œâ”€â”€ model                     # Model configs
    â”‚   â”œâ”€â”€ paths                     # Project paths configs
    â”‚   â”œâ”€â”€ trainer                   # Trainer configs
    â”‚   â”‚
    â”‚   â”œâ”€â”€ eval.yaml                 # Main config for evaluation
    â”‚   â””â”€â”€ train.yaml                # Main config for training
    â”‚
    â”œâ”€â”€ data                      # Project data
    â”‚
    â”œâ”€â”€ docs                      # Documentation
    â”‚
    â”œâ”€â”€ logs                      # Logs generated by hydra and lightning loggers
    â”‚
    â”œâ”€â”€ media                     # Media illustrating the project
    â”‚
    â”œâ”€â”€ notebooks                 # Jupyter notebooks
    â”‚
    â”œâ”€â”€ scripts                   # Shell scripts
    â”‚
    â”œâ”€â”€ src                       # Source code
    â”‚   â”œâ”€â”€ data                      # Data structure for hierarchical partitions
    â”‚   â”œâ”€â”€ datamodules               # Lightning DataModules
    â”‚   â”œâ”€â”€ datasets                  # Datasets
    â”‚   â”œâ”€â”€ dependencies              # Compiled dependencies
    â”‚   â”œâ”€â”€ loader                    # DataLoader
    â”‚   â”œâ”€â”€ loss                      # Loss
    â”‚   â”œâ”€â”€ metrics                   # Metrics
    â”‚   â”œâ”€â”€ models                    # Model architecture
    â”‚   â”œâ”€â”€ nn                        # Model building blocks
    â”‚   â”œâ”€â”€ optim                     # Optimization 
    â”‚   â”œâ”€â”€ transforms                # Functions for transforms, pre-transforms, etc
    â”‚   â”œâ”€â”€ utils                     # Utilities
    â”‚   â”œâ”€â”€ visualization             # Interactive visualization tool
    â”‚   â”‚
    â”‚   â”œâ”€â”€ eval.py                   # Run evaluation
    â”‚   â””â”€â”€ train.py                  # Run training
    â”‚
    â”œâ”€â”€ tests                     # Tests of any kind
    â”‚
    â”œâ”€â”€ .env.example              # Example of file for storing private environment variables
    â”œâ”€â”€ .gitignore                # List of files ignored by git
    â”œâ”€â”€ .pre-commit-config.yaml   # Configuration of pre-commit hooks for code formatting
    â”œâ”€â”€ install.sh                # Installation script
    â”œâ”€â”€ LICENSE                   # Project license
    â””â”€â”€ README.md

```

### Setting up `data/` and `logs/`
The `data/` and `logs/` directories will store all your datasets and training 
logs. By default, these are placed in the repository directory. 

Since this may take some space, or your heavy data may be stored elsewhere, you 
may specify other paths for these directories by creating a 
`configs/local/defaults.yaml` file containing the following:

```yaml
# @package paths

# path to data directory
data_dir: /path/to/your/data/

# path to logging directory
log_dir: /path/to/your/logs/
```

### Structure of `data/` 
See the [Datasets page](docs/dataset.md) for further details. 

### Structure of `logs/` 
See the [Logs page](docs/logs.md) for further details.

## ğŸš€Â Â Reproducing our results
See the [Datasets page](docs/dataset.md) for further details on supported 
datasets and the data structure. 

### Training SPT
Use the following commands to train SPT:
```bash
# Train SPT on S3DIS Fold 5
python src/train.py trainer=gpu model=spt_s3dis datamodule=s3dis datamodule.fold=5 trainer.max_epochs=2000

# Train SPT on KITTI-360 Val
# âš ï¸ KITTI-360 does not support automatic download, follow prompted instructions
python src/train.py trainer=gpu model=spt_kitti360 datamodule=kitti360 trainer.max_epochs=200 

# Train SPT on DALES
python src/train.py trainer=gpu model=spt_dales datamodule=dales trainer.max_epochs=400
```

You may use [Weights and Biases](https://wandb.ai) to track your experiments, 
by adding the adequate argument:

```bash
# Log S3DIS experiments to W&B
python src/train.py logger=wandb_s3dis ...

# Log KITTI-360 experiments to W&B
python src/train.py logger=wandb_kitti360 ...

# Log DALES experiments to W&B
python src/train.py logger=wandb_dales ...
```

### Evaluating SPT
Use the following commands to evaluate SPT from a checkpoint file 
`checkpoint.ckpt`. 
```bash
# Train SPT on S3DIS Fold 5
python src/eval.py ckpt_path=checkpoint.ckpt trainer=gpu model=spt_s3dis datamodule=s3dis datamodule.fold=5 trainer.max_epochs=2000

# Train SPT on KITTI-360 Val
# âš ï¸ KITTI-360 does not support automatic download, follow prompted instructions
python src/eval.py ckpt_path=checkpoint.ckpt trainer=gpu model=spt_kitti360 datamodule=kitti360 trainer.max_epochs=200 

# Train SPT on DALES
python src/eval.py ckpt_path=checkpoint.ckpt trainer=gpu model=spt_dales datamodule=dales trainer.max_epochs=400
```


## ğŸ’³Â Â Credits
- This project was built using [Lightning-Hydra template](https://github.com/ashleve/lightning-hydra-template).
- The main data structures of this work rely on [PyToch Geometric](https://github.com/pyg-team/pytorch_geometric)
- Some point cloud operations were inspired from the [Torch-Points3D framework](https://github.com/nicolas-chaulet/torch-points3d), although not merged with the official project at this point. 
- For the KITTI-360 dataset, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) was used.
- Some superpoint-graph-related operations were inspired from [Superpoint Graph](https://github.com/loicland/superpoint_graph)
- The hierarchical superpoint partition is computed using [Parallel Cut-Pursuit](https://gitlab.com/1a7r0ch3/parallel-cut-pursuit)

## Citing our work
If your work uses all or part of the present code, please include the following a citation:

```
@inproceedings{robert2023spt,
  title={Efficient 3D Semantic Segmentation with Superpoint Transformer},
  author={Robert, Damien and Raguet, Hugo and Landrieu, Loic},
  booktitle={arxiv},
  year={2023}
}
```