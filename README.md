<div align="center">

# Superpoint Transformer

[![python](https://img.shields.io/badge/-Python_3.8+-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_1.12+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![lightning](https://img.shields.io/badge/-Lightning_1.6+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![hydra](https://img.shields.io/badge/Config-Hydra_1.2-89b8cd)](https://hydra.cc/)
[![license](https://img.shields.io/badge/License-MIT-green.svg?labelColor=gray)](https://github.com/ashleve/lightning-hydra-template#license)

[//]: # ([![Paper]&#40;http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg&#41;]&#40;https://www.nature.com/articles/nature14539&#41;)
[//]: # ([![Conference]&#40;http://img.shields.io/badge/AnyConference-year-4b44ce.svg&#41;]&#40;https://papers.nips.cc/paper/2020&#41;)


Official implementation for
<br>
_Efficient 3D Semantic Segmentation with Superpoint Transformer_
<br>
üöÄ‚ö°üî•
<br>


</div>

<p align="center">
    <img width="90%" src="./media/teaser.jpg">
</p>

<br>

## üìå¬†¬†Description

SPT is a superpoint-based transformer ü§ñ architecture that efficiently ‚ö° 
performs semantic segmentation on large-scale 3D scenes. This method includes a 
fast algorithm that partitions üß© point clouds into a hierarchical superpoint 
structure, as well as a self-attention mechanism to exploit the relationships 
between superpoints at multiple scales. 

<div align="center">

| SPT in numbers |
| :---: |
| **üìä SOTA on S3DIS 6-Fold (76.0 mIoU)** |
| **üìä SOTA on KITTI-360 Val (63.5 mIoU)** |
| **üìä Near SOTA on DALES (79.6 mIoU)** | 
| **ü¶ã 212k parameters ([PointNeXt](https://github.com/guochengqian/PointNeXt) √∑ 200, [Stratified Transformer](https://github.com/dvlab-research/Stratified-Transformer) √∑ 40)** | 
| **‚ö° S3DIS training in 3h on 1 GPU** | 
| **‚ö° Preprocessing x7 faster than [SPG](https://github.com/loicland/superpoint_graph)** |

</div>

<br>

## üì∞¬†¬†Updates

- **15.06.2023 Official release** üå±

<br>

## üèó¬†¬†Installation
Simply run `install.sh` to install all dependencies in a new conda environment 
named `spt`. 
```bash
# Creates a conda env named 'spt' env and installs dependencies
./install.sh
```

> **Note**: See the [Datasets page](docs/dataset.md) for setting up your dataset
> path and file structure.

<br>

### üî©¬†¬†Project structure
```
‚îî‚îÄ‚îÄ superpoint_transformer
    ‚îÇ
    ‚îú‚îÄ‚îÄ configs                   # Hydra configs
    ‚îÇ   ‚îú‚îÄ‚îÄ callbacks                 # Callbacks configs
    ‚îÇ   ‚îú‚îÄ‚îÄ data                      # Data configs
    ‚îÇ   ‚îú‚îÄ‚îÄ debug                     # Debugging configs
    ‚îÇ   ‚îú‚îÄ‚îÄ experiment                # Experiment configs
    ‚îÇ   ‚îú‚îÄ‚îÄ extras                    # Extra utilities configs
    ‚îÇ   ‚îú‚îÄ‚îÄ hparams_search            # Hyperparameter search configs
    ‚îÇ   ‚îú‚îÄ‚îÄ hydra                     # Hydra configs
    ‚îÇ   ‚îú‚îÄ‚îÄ local                     # Local configs
    ‚îÇ   ‚îú‚îÄ‚îÄ logger                    # Logger configs
    ‚îÇ   ‚îú‚îÄ‚îÄ model                     # Model configs
    ‚îÇ   ‚îú‚îÄ‚îÄ paths                     # Project paths configs
    ‚îÇ   ‚îú‚îÄ‚îÄ trainer                   # Trainer configs
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îú‚îÄ‚îÄ eval.yaml                 # Main config for evaluation
    ‚îÇ   ‚îî‚îÄ‚îÄ train.yaml                # Main config for training
    ‚îÇ
    ‚îú‚îÄ‚îÄ data                      # Project data
    ‚îÇ
    ‚îú‚îÄ‚îÄ docs                      # Documentation
    ‚îÇ
    ‚îú‚îÄ‚îÄ logs                      # Logs generated by hydra and lightning loggers
    ‚îÇ
    ‚îú‚îÄ‚îÄ media                     # Media illustrating the project
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks                 # Jupyter notebooks
    ‚îÇ
    ‚îú‚îÄ‚îÄ scripts                   # Shell scripts
    ‚îÇ
    ‚îú‚îÄ‚îÄ src                       # Source code
    ‚îÇ   ‚îú‚îÄ‚îÄ data                      # Data structure for hierarchical partitions
    ‚îÇ   ‚îú‚îÄ‚îÄ datamodules               # Lightning DataModules
    ‚îÇ   ‚îú‚îÄ‚îÄ datasets                  # Datasets
    ‚îÇ   ‚îú‚îÄ‚îÄ dependencies              # Compiled dependencies
    ‚îÇ   ‚îú‚îÄ‚îÄ loader                    # DataLoader
    ‚îÇ   ‚îú‚îÄ‚îÄ loss                      # Loss
    ‚îÇ   ‚îú‚îÄ‚îÄ metrics                   # Metrics
    ‚îÇ   ‚îú‚îÄ‚îÄ models                    # Model architecture
    ‚îÇ   ‚îú‚îÄ‚îÄ nn                        # Model building blocks
    ‚îÇ   ‚îú‚îÄ‚îÄ optim                     # Optimization 
    ‚îÇ   ‚îú‚îÄ‚îÄ transforms                # Functions for transforms, pre-transforms, etc
    ‚îÇ   ‚îú‚îÄ‚îÄ utils                     # Utilities
    ‚îÇ   ‚îú‚îÄ‚îÄ visualization             # Interactive visualization tool
    ‚îÇ   ‚îÇ
    ‚îÇ   ‚îú‚îÄ‚îÄ eval.py                   # Run evaluation
    ‚îÇ   ‚îî‚îÄ‚îÄ train.py                  # Run training
    ‚îÇ
    ‚îú‚îÄ‚îÄ tests                     # Tests of any kind
    ‚îÇ
    ‚îú‚îÄ‚îÄ .env.example              # Example of file for storing private environment variables
    ‚îú‚îÄ‚îÄ .gitignore                # List of files ignored by git
    ‚îú‚îÄ‚îÄ .pre-commit-config.yaml   # Configuration of pre-commit hooks for code formatting
    ‚îú‚îÄ‚îÄ install.sh                # Installation script
    ‚îú‚îÄ‚îÄ LICENSE                   # Project license
    ‚îî‚îÄ‚îÄ README.md

```

> **Note**: See the [Datasets page](docs/dataset.md) for further details on `data/``. 

> **Note**: See the [Logs page](docs/logs.md) for further details on `logs/``. 

<br>

## üöÄ¬†¬†Usage
### Training SPT
Use the following commands to train SPT:
```bash
# Train SPT on S3DIS Fold 5
python src/train.py trainer=gpu model=spt_s3dis datamodule=s3dis datamodule.fold=5 trainer.max_epochs=2000

# Train SPT on KITTI-360 Val
# ‚ö†Ô∏è KITTI-360 does not support automatic download, follow prompted instructions
python src/train.py trainer=gpu model=spt_kitti360 datamodule=kitti360 trainer.max_epochs=200 

# Train SPT on DALES
python src/train.py trainer=gpu model=spt_dales datamodule=dales trainer.max_epochs=400
```

You may use [Weights and Biases](https://wandb.ai) to track your experiments, 
by adding the adequate argument:

```bash
# Log S3DIS experiments to W&B
python src/train.py logger=wandb_s3dis ...

# Log KITTI-360 experiments to W&B
python src/train.py logger=wandb_kitti360 ...

# Log DALES experiments to W&B
python src/train.py logger=wandb_dales ...
```

### Evaluating SPT
Use the following commands to evaluate SPT from a checkpoint file 
`checkpoint.ckpt`. 
```bash
# Train SPT on S3DIS Fold 5
python src/eval.py ckpt_path=checkpoint.ckpt trainer=gpu model=spt_s3dis datamodule=s3dis datamodule.fold=5 trainer.max_epochs=2000

# Train SPT on KITTI-360 Val
# ‚ö†Ô∏è KITTI-360 does not support automatic download, follow prompted instructions
python src/eval.py ckpt_path=checkpoint.ckpt trainer=gpu model=spt_kitti360 datamodule=kitti360 trainer.max_epochs=200 

# Train SPT on DALES
python src/eval.py ckpt_path=checkpoint.ckpt trainer=gpu model=spt_dales datamodule=dales trainer.max_epochs=400
```

### Visualization
We provide an interactive visualization tool which can be used to produce 
shareable HTMLs. Examples of how to use this tool are provided in 
`notebooks/`.

<br>

## üí≥¬†¬†Credits
- This project was built using [Lightning-Hydra template](https://github.com/ashleve/lightning-hydra-template).
- The main data structures of this work rely on [PyToch Geometric](https://github.com/pyg-team/pytorch_geometric)
- Some point cloud operations were inspired from the [Torch-Points3D framework](https://github.com/nicolas-chaulet/torch-points3d), although not merged with the official project at this point. 
- For the KITTI-360 dataset, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) was used.
- Some superpoint-graph-related operations were inspired from [Superpoint Graph](https://github.com/loicland/superpoint_graph)
- The hierarchical superpoint partition is computed using [Parallel Cut-Pursuit](https://gitlab.com/1a7r0ch3/parallel-cut-pursuit)

<br>

## Citing our work
If your work uses all or part of the present code, please include the following a citation:

```
@inproceedings{robert2023spt,
  title={Efficient 3D Semantic Segmentation with Superpoint Transformer},
  author={Robert, Damien and Raguet, Hugo and Landrieu, Loic},
  booktitle={arxiv},
  year={2023}
}
```