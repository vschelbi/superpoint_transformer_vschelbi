{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading FOR-instance dataset and creating Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Plots + Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the project's files to the python path\n",
    "# file_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # for .py script\n",
    "file_path = os.path.dirname(os.path.abspath(''))  # for .ipynb notebook\n",
    "sys.path.append(file_path)\n",
    "\n",
    "import laspy\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src.data import Data, InstanceData\n",
    "from torch_geometric.nn.pool.consecutive import consecutive_cluster\n",
    "\n",
    "from src.transforms import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label mapping and colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORInstance_NUM_CLASSES = 2\n",
    "\n",
    "ID2TRAINID = np.asarray([2, 0, 0, 2, 1, 1, 1])\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'Ground and low vegetation',  # 2 Ground, 1 Low vegetation\n",
    "    'Tree',                       # 4 Stem, 5 Live branches, 6 Woody branches\n",
    "    'Unknown'                     # 0 Unclassified, 3 Out-points\n",
    "]\n",
    "\n",
    "CLASS_COLORS = np.asarray([\n",
    "    [243, 214, 171],    # Ground and Low vegetation\n",
    "    [ 70, 115,  66],    # Tree\n",
    "    [  0,   8, 116]     # Unknown\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def read_FORinstance_plot(filepath, xyz=True, intensity=True, \n",
    "                           semantic=True, instance=True, remap=True, \n",
    "                           max_intensity=None):\n",
    "    \"\"\"\n",
    "    Read a FORinstance plot from a LAS file and return the data object.\n",
    "\n",
    "    :param filepath: str\n",
    "        Absolute path to the LAS file\n",
    "    :param xyz: bool\n",
    "        Whether XYZ coordinates should be saved in the output Data.pos\n",
    "    :param intensity: bool\n",
    "        Whether intensity should be saved in the output Data.rgb\n",
    "    :param semantic: bool\n",
    "        Whether semantic labels should be saved in the output Data.y\n",
    "    :param instance: bool\n",
    "        Whether instance labels should be saved in the output Data.obj\n",
    "    :param remap: bool\n",
    "        Whether semantic labels should be mapped from their FORinstance ID\n",
    "        to their train ID\n",
    "    :param max_intensity: float\n",
    "        Maximum value used to clip intensity signal before normalizing \n",
    "        to [0, 1]\n",
    "    \"\"\"\n",
    "    data = Data()\n",
    "    las = laspy.read(filepath)\n",
    "\n",
    "    if xyz:\n",
    "        pos = torch.stack([\n",
    "            torch.as_tensor(np.array(las[axis]))\n",
    "            for axis in [\"X\", \"Y\", \"Z\"]], dim=-1)\n",
    "        pos *= las.header.scale\n",
    "        pos_offset = pos[0]\n",
    "        data.pos = (pos - pos_offset).float()\n",
    "        data.pos_offset = pos_offset\n",
    "    \n",
    "    intensity_remaped = True\n",
    "    if intensity:\n",
    "        data.intensity = torch.FloatTensor(\n",
    "            las['intensity'].astype('float32')\n",
    "        )\n",
    "        if intensity_remaped:\n",
    "            if max_intensity is None:\n",
    "                max_intensity = data.intensity.max()\n",
    "            data.intensity = data.intensity.clip(min=0, max=max_intensity) / max_intensity\n",
    "\n",
    "    if semantic:\n",
    "        y = torch.LongTensor(np.array(las['classification']))\n",
    "        data.y = torch.from_numpy(ID2TRAINID)[y] if remap else y\n",
    "\n",
    "    if instance:\n",
    "        idx = torch.arange(data.num_points)\n",
    "        obj = torch.LongTensor(np.array(las['treeID']))\n",
    "        \n",
    "        y = torch.LongTensor(np.array(las['classification']))\n",
    "        y = torch.from_numpy(ID2TRAINID)[y] if remap else y\n",
    "\n",
    "        if remap:\n",
    "            ground_mask = (obj == 0) & (y == 0)\n",
    "            low_veg_mask = (obj == 0) & (y == 1)\n",
    "            if low_veg_mask.any() or ground_mask.any():\n",
    "                ground_instance_label = obj.max().item() + 1\n",
    "                low_veg_instance_label = ground_instance_label  # for separate ground and low vegetation classes: ground_instance_label + 1\n",
    "                obj[ground_mask] = ground_instance_label\n",
    "                obj[low_veg_mask] = low_veg_instance_label\n",
    "\n",
    "        obj = consecutive_cluster(obj)[0]\n",
    "        count = torch.ones_like(obj)\n",
    "\n",
    "        data.obj = InstanceData(idx, obj, count, y, dense=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMIT_filepath = \"/home/valerio/git/superpoint_transformer_vschelbi/data/forinstance/raw/RMIT/train.las\"\n",
    "RMIT_data = read_FORinstance_plot(RMIT_filepath, instance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMIT_data.show(class_names=CLASS_NAMES, class_colors=CLASS_COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CULS_filepath = \"/home/valerio/git/superpoint_transformer_vschelbi/data/FORinstance/raw/CULS/plot_3_annotated.las\"\n",
    "CULS_data = read_FORinstance_plot(CULS_filepath, instance=True)\n",
    "NIBIO_filepath = \"/home/valerio/git/superpoint_transformer_vschelbi/data/FORinstance/raw/NIBIO/plot_10_annotated.las\"\n",
    "NIBIO_data = read_FORinstance_plot(NIBIO_filepath, instance=True)\n",
    "RMIT_filepath = \"/home/valerio/git/superpoint_transformer_vschelbi/data/FORinstance/raw/RMIT/train.las\"\n",
    "RMIT_data = read_FORinstance_plot(RMIT_filepath, instance=True)\n",
    "SCION_filepath = \"/home/valerio/git/superpoint_transformer_vschelbi/data/FORinstance/raw/SCION/plot_35_annotated.las\"\n",
    "SCION_data = read_FORinstance_plot(SCION_filepath, instance=True)\n",
    "TUWIEN_filepath = \"/home/valerio/git/superpoint_transformer_vschelbi/data/FORinstance/raw/TUWIEN/train.las\"\n",
    "TUWIEN_data = read_FORinstance_plot(TUWIEN_filepath, instance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show:\n",
    "    CULS_data.show(keys=['intensity'], class_names=FOR_Instance_CLASS_NAMES, class_colors=FOR_Instance_CLASS_COLORS)\n",
    "    NIBIO_data.show(keys=['intensity'], class_names=FOR_Instance_CLASS_NAMES, class_colors=FOR_Instance_CLASS_COLORS)\n",
    "    RMIT_data.show(keys=['intensity'], class_names=FOR_Instance_CLASS_NAMES, class_colors=FOR_Instance_CLASS_COLORS)\n",
    "    SCION_data.show(keys=['intensity'], class_names=FOR_Instance_CLASS_NAMES, class_colors=FOR_Instance_CLASS_COLORS)\n",
    "    TUWIEN_data.show(keys=['intensity'], class_names=FOR_Instance_CLASS_NAMES, class_colors=FOR_Instance_CLASS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking intensity and x, y, z values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(data.intensity)\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # plot title\n",
    "    plt.title('Intensity distribution')\n",
    "    plt.hist(data.intensity.numpy(), bins=10)\n",
    "    plt.show()\n",
    "    print(data.intensity.min())\n",
    "\n",
    "    # plot title\n",
    "    plt. title('non remaped intensity distribution')\n",
    "    las = laspy.read(filepath)\n",
    "    intensity = torch.FloatTensor(\n",
    "                las['intensity'].astype('float32'))\n",
    "    plt.hist(intensity.numpy(), bins=10)\n",
    "    plt.show()\n",
    "\n",
    "    print(intensity.min())\n",
    "\n",
    "    # x,y, z values\n",
    "    print(data.pos[:, 2].min())\n",
    "    print(data.pos[:, 2].max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiling\n",
    "The FOR-instance dataset has already quite small tiles in each file, thus additional tiling of the point cloud in every file is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxelization\n",
    "`voxel`: size of the voxels in the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_point_density(voxel_size=0.1, data=None):\n",
    "    \"\"\"Calculate the point density for each point in the data.\n",
    "\n",
    "    :param voxel_size: float\n",
    "        The size of the voxel used to calculate the point density\n",
    "    :param data: Data\n",
    "        The data object containing the point cloud\n",
    "    :return: vol_density\n",
    "        points per m^3\n",
    "    :return: point_ratio\n",
    "        estimated ratio of points in the voxelized data to the original data\n",
    "    \"\"\"\n",
    "    \n",
    "    data_voxelized = GridSampling3D(size=voxel_size)(data)\n",
    "    voxel_ratio = data.num_nodes / data_voxelized.num_nodes\n",
    "    data_voxelized_1m = GridSampling3D(size=1)(data)\n",
    "    vol_density = data_voxelized.num_nodes / data_voxelized_1m.num_nodes\n",
    "\n",
    "    return vol_density, voxel_ratio\n",
    "\n",
    "def point_density_experiments(voxel_size = 0.1):\n",
    "    \"\"\"Calculate the point density for each point cloud in the dataset.\n",
    "    :param voxel_size: float\n",
    "        The size of the voxel used to calculate the point density\n",
    "    \"\"\"\n",
    "    CULS_density, CULS_ratio = calc_point_density(voxel_size, CULS_data)\n",
    "    NIBIO_density, NIBIO_ratio = calc_point_density(voxel_size, NIBIO_data) \n",
    "    RMIT_density, RMIT_ratio = calc_point_density(voxel_size, RMIT_data)\n",
    "    SCION_density, SCION_ratio = calc_point_density(voxel_size, SCION_data)\n",
    "    TUWIEN_density, TUWIEN_ratio = calc_point_density(voxel_size, TUWIEN_data)\n",
    "\n",
    "    print(f\"---------------------- \")\n",
    "    print(f\"voxel_size = {voxel_size}\")\n",
    "    print(f\"CULS: density = {CULS_density:.2f} points/m^3, voxel ratio = {CULS_ratio:.2f}\")\n",
    "    print(f\"NIBIO: density = {NIBIO_density:.2f} points/m^3, voxel ratio = {NIBIO_ratio:.2f}\")\n",
    "    print(f\"RMIT: density = {RMIT_density:.2f} points/m^3, voxel ratio = {RMIT_ratio:.2f}\")\n",
    "    print(f\"SCION: density = {SCION_density:.2f} points/m^3, voxel ratio = {SCION_ratio:.2f}\")\n",
    "    print(f\"TUWIEN: density = {TUWIEN_density:.2f} points/m^3, voxel ratio = {TUWIEN_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show:\n",
    "    point_density_experiments(voxel_size = 0.05)\n",
    "    point_density_experiments(voxel_size = 0.1)\n",
    "    point_density_experiments(voxel_size = 0.2)\n",
    "    point_density_experiments(voxel_size = 0.5)\n",
    "    point_density_experiments(voxel_size = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxelize_all_data(voxel_size):\n",
    "    \"\"\"Voxelize all the data in the dataset using the given voxel size.\n",
    "    :param voxel_size: float\n",
    "        The size of the voxel used to calculate the point density\n",
    "    \"\"\"\n",
    "    CULS_voxelized = GridSampling3D(size=voxel_size, hist_key='y', hist_size=FOR_Instance_num_classes + 1)(CULS_data)\n",
    "    NIBIO_voxelized = GridSampling3D(size=voxel_size, hist_key='y', hist_size=FOR_Instance_num_classes + 1)(NIBIO_data)\n",
    "    RMIT_voxelized = GridSampling3D(size=voxel_size, hist_key='y', hist_size=FOR_Instance_num_classes + 1)(RMIT_data)\n",
    "    SCION_voxelized = GridSampling3D(size=voxel_size, hist_key='y', hist_size=FOR_Instance_num_classes + 1)(SCION_data)\n",
    "    TUWIEN_voxelized = GridSampling3D(size=voxel_size, hist_key='y', hist_size=FOR_Instance_num_classes + 1)(TUWIEN_data)\n",
    "\n",
    "    return CULS_voxelized, NIBIO_voxelized, RMIT_voxelized, SCION_voxelized, TUWIEN_voxelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_size = 0.5 # try with 0.05, 0.1, 0.2\n",
    "CULS_voxelized, NIBIO_voxelized, RMIT_voxelized, SCION_voxelized, TUWIEN_voxelized = voxelize_all_data(voxel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show:\n",
    "    RMIT_voxelized.show(max_points=RMIT_voxelized.num_points, class_names=FOR_Instance_CLASS_NAMES, class_colors=FOR_Instance_CLASS_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbor search\n",
    "`k`: num nearest neighbors  \n",
    "`r_max`: search nearest neighbors within this radius\n",
    "\n",
    "\n",
    "Searches for `k` nearest neighbors of each point, within a maximum radius of `r_max`. Contrary to basic K-NN search, the radius constraint prevents spurious neighborhoods for very sparse areas of the point cloud. By design, this approach implies **points may not all have the same number of neighbors**, depending on the local geometry and density.\n",
    "\n",
    "The neigbors are used for two things in the preprocessing pipeline:\n",
    "- computing local geometric features with `PointFeatures`, later used by `CutPursuitPartition` as pointwise signal for the superpoint partition\n",
    "- computing the adjacency graph with `AdjacencyGraph`, later used by `CutPursuitPartition` as the graph on which the superpoint partition is computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_all_data(k=25, r_max=2):\n",
    "    \"\"\"Find the k-nearest neighbors for all points in the dataset.\n",
    "    :param k: int\n",
    "        The number of neighbors to find\n",
    "    :param r_max: float\n",
    "        The maximum distance to consider when searching for neighbors\n",
    "    \"\"\"\n",
    "    CULS_knn = KNN(k, r_max)(CULS_voxelized)\n",
    "    NIBIO_knn = KNN(k, r_max)(NIBIO_voxelized)\n",
    "    RMIT_knn = KNN(k, r_max)(RMIT_voxelized)\n",
    "    SCION_knn = KNN(k, r_max)(SCION_voxelized)\n",
    "    TUWIEN_knn = KNN(k, r_max)(TUWIEN_voxelized)\n",
    "\n",
    "    return CULS_knn, NIBIO_knn, RMIT_knn, SCION_knn, TUWIEN_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 25\n",
    "r_max = 2\n",
    "CULS_knn, NIBIO_knn, RMIT_knn, SCION_knn, TUWIEN_knn = knn_all_data(k, r_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation Estimation\n",
    "\n",
    "`threshold`: ground as a planar surface located within `threshold` of the lowest point in the cloud.  \n",
    "`scale`: Pointwise distance to the plane is normalized by `scale`\n",
    "\n",
    "`GroundElevation` is used to look for the ground among the points, to then infer point `elevation`. Indeed, the elevation is a more informative feature than the `z` coordinate of points for semantic parsing. For real-life large point cloud acquisitions, the absolute `z` value usually carries no meaning, but the _relative `z`_ with respect to the ground does (the same holds for absolute `x` and `y` values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elevation_all_data(threshold=5, scale=20):\n",
    "    \"\"\"Calculate the elevation features for all points in the dataset.\n",
    "    :param threshold: float\n",
    "        The threshold used to calculate the elevation features\n",
    "    :param scale: float\n",
    "        The scale used to calculate the elevation features\n",
    "    \"\"\"\n",
    "    CULS_elevation = GroundElevation(threshold, scale)(CULS_knn)\n",
    "    NIBIO_elevation = GroundElevation(threshold, scale)(NIBIO_knn)\n",
    "    RMIT_elevation = GroundElevation(threshold, scale)(RMIT_knn)\n",
    "    SCION_elevation = GroundElevation(threshold, scale)(SCION_knn)\n",
    "    TUWIEN_elevation = GroundElevation(threshold, scale)(TUWIEN_knn)\n",
    "\n",
    "    return CULS_elevation, NIBIO_elevation, RMIT_elevation, SCION_elevation, TUWIEN_elevation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5\n",
    "scale = 20\n",
    "CULS_elevation, NIBIO_elevation, RMIT_elevation, SCION_elevation, TUWIEN_elevation = elevation_all_data(threshold, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Plot for CULS elevation\n",
    "    g = sns.displot(CULS_elevation.elevation)\n",
    "    g.figure.suptitle(\"CULS Elevation Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot for NIBIO elevation\n",
    "    g = sns.displot(NIBIO_elevation.elevation)\n",
    "    g.figure.suptitle(\"NIBIO Elevation Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot for RMIT elevation\n",
    "    g = sns.displot(RMIT_elevation.elevation)\n",
    "    g.figure.suptitle(\"RMIT Elevation Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot for SCION elevation\n",
    "    g = sns.displot(SCION_elevation.elevation)\n",
    "    g.figure.suptitle(\"SCION Elevation Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot for TUWIEN elevation\n",
    "    g = sns.displot(TUWIEN_elevation.elevation)\n",
    "    g.figure.suptitle(\"TUWIEN Elevation Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "    CULS_elevation.show(keys=['elevation'], class_names=FOR_Instance_CLASS_NAMES, class_colors=FOR_Instance_CLASS_COLORS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise local geometric features\n",
    "`PointFeatures` computes some handcrafted geometric features characterizing each point's neighborhood. The following features are currently supported:\n",
    "- density\n",
    "- linearity\n",
    "- planarity\n",
    "- scattering\n",
    "- verticality\n",
    "- normal\n",
    "- length\n",
    "- surface\n",
    "- volume\n",
    "- curvature\n",
    "- (RGB color)  \n",
    "- (HSV color)  \n",
    "- (LAB color)  \n",
    "\n",
    "These features should be computed with the superpoint partition in mind: these will be the **criteria based on which points will or will not grouped together** by the cut-pursuit algorithm.\n",
    "\n",
    "Note that the robustness and expressivity of these computed geometric features will depend on your `KNN` parametrization.\n",
    "\n",
    "`PointFeatures` supports various strategies for geometric computation. By default, all neighbors produced by `KNN` will be used.\n",
    "\n",
    ">One may also specify `PointFeatures(k_min=...)` below which a point will receive `0` geometric features, to mitigate the low-quality features for too-small neighborhoods. Besides, `PointFeatures(k_step=..., k_min_search=...)` will search for the optimal neighborhood size among available neighbors for each point, based on eigenfeatures entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointfeatures_all_data(features):\n",
    "    \"\"\"\n",
    "    :params features: list of str\n",
    "    \"\"\"\n",
    "    CULS_features = PointFeatures(keys=features)(CULS_elevation)\n",
    "    NIBIO_features = PointFeatures(keys=features)(NIBIO_elevation)\n",
    "    RMIT_features = PointFeatures(keys=features)(RMIT_elevation)\n",
    "    SCION_features = PointFeatures(keys=features)(SCION_elevation)\n",
    "    TUWIEN_features = PointFeatures(keys=features)(TUWIEN_elevation)\n",
    "\n",
    "    return CULS_features, NIBIO_features, RMIT_features, SCION_features, TUWIEN_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ('density', 'linearity', 'planarity', 'scattering', 'verticality', )\n",
    "CULS_features, NIBIO_features, RMIT_features, SCION_features, TUWIEN_features = pointfeatures_all_data(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    CULS_features.show(keys=CULS_features.keys, class_names=FOR_Instance_CLASS_NAMES, class_colors=FOR_Instance_CLASS_COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.displot(CULS_features.density)\n",
    "g.figure.suptitle(\"CULS Denstiy Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmax = CULS_features.neighbor_distance.max(dim=1).values\n",
    "k = CULS_features.neighbor_index.ge(0).sum(dim=1)\n",
    "CULS_features.density = (k / 4 ** 2).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(dmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency graph\n",
    "`k`: use edges of the `k` nearest neighbors  \n",
    "`w`: edge weights\n",
    "\n",
    "computes the adjacency graph based on which the superpoint partition will be computed. It is relying on the output of `KNN` to find neighbors for each point. `AdjacencyGraph(k=..., w=...)` will store edges for the `k`-NN graph in `Data.edge_index`, along with edge weights in `Data.edge_attr` to be used in the partition (the larger an edge's weight the harder to separate the corresponding points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_graph_all_data(k=10, w=1):\n",
    "    \"\"\"Create the adjacency graph for all points in the dataset.\n",
    "    :param k: int\n",
    "        The number of neighbors to consider when creating the graph\n",
    "    :param w: float\n",
    "        The weight to assign to the edges\n",
    "    \"\"\"\n",
    "    CULS_graph = AdjacencyGraph(k, w)(CULS_features)\n",
    "    NIBIO_graph = AdjacencyGraph(k, w)(NIBIO_features)\n",
    "    RMIT_graph = AdjacencyGraph(k, w)(RMIT_features)\n",
    "    SCION_graph = AdjacencyGraph(k, w)(SCION_features)\n",
    "    TUWIEN_graph = AdjacencyGraph(k, w)(TUWIEN_features)\n",
    "\n",
    "    return CULS_graph, NIBIO_graph, RMIT_graph, SCION_graph, TUWIEN_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "w = 1\n",
    "CULS_graph, NIBIO_graph, RMIT_graph, SCION_graph, TUWIEN_graph = adjacency_graph_all_data(k, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add keys to x\n",
    "Before computing the partition, we need to move to the `x` attribute all the features that we want to use for the partition (`CutPursuitPartition` will blindly use whatever it finds `x`). To this end, we will use the `AddKeysTo` transform.\n",
    "\n",
    "You can play with the features used with `AddKeysTo` and `CutPursuitPartition` parameters, and see how it impacts your partition metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_keys_all_data(features):\n",
    "    \"\"\"Add the keys to the data object for all points in the dataset.\"\"\"\n",
    "    CULS_graph_x = AddKeysTo(keys=features, to='x', delete_after=False)(CULS_graph)\n",
    "    NIBIO_graph_x = AddKeysTo(keys=features, to='x', delete_after=False)(NIBIO_graph)\n",
    "    RMIT_graph_x = AddKeysTo(keys=features, to='x', delete_after=False)(RMIT_graph)\n",
    "    SCION_graph_x = AddKeysTo(keys=features, to='x', delete_after=False)(SCION_graph)\n",
    "    TUWIEN_graph_x  = AddKeysTo(keys=features, to='x', delete_after=False)(TUWIEN_graph)\n",
    "\n",
    "    return CULS_graph_x, NIBIO_graph_x, RMIT_graph_x, SCION_graph_x, TUWIEN_graph_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_x = ('elevation', 'linearity', 'planarity', 'scattering', 'verticality')\n",
    "CULS_graph_x, NIBIO_graph_x, RMIT_graph_x, SCION_graph_x, TUWIEN_graph_x = add_keys_all_data(features_to_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical partition\n",
    "`regularization`: List of increasing float values determining the granularity of hierarchical superpoint partitions.  \n",
    "`spatial_weight`: Float value indicating the importance of point coordinates relative to point features in grouping points.  \n",
    "`k_adjacency`: Integer preventing superpoints from being isolated.  \n",
    "`cutoff`: Integer specifying the minimum number of points in each superpoint, ensuring small superpoints are merged with others.  \n",
    "\n",
    "\n",
    "`CutPursuitPartition` is where the actual superpoint partition occurs. A regularization term rules the trade-off between \"many-superpoint-with-homogeneous-content\" and \"few-superpoints-with-heterogenous-content\".\n",
    "\n",
    "In `CutPursuitPartition(regularization=..., spatial_weight=..., k_adjacency=..., cutoff=...)`, `regularization` carries a list of increasing float values for coarser and coarser hierarchical superpoint partition levels. `spatial_weight` indicates how much importance the point coordinates play with respect point features, when grouping points: the larger the weight, the more spatial coordinates take over, the more tesselated-looking the partition. `k_adjacency` prevents superpoints from staying isolated. `cutoff` rules the minimum number of points in each superpoint partition level: too-small superpoint will be merged with other superpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nag_all_data(regularization, spatial_weight, cutoff, iterations, k_adjacency):\n",
    "    CULS_nag = CutPursuitPartition(regularization=regularization, spatial_weight=spatial_weight, cutoff=cutoff, iterations=iterations, k_adjacency=k_adjacency)(CULS_graph_x)\n",
    "    NIBIO_nag = CutPursuitPartition(regularization=regularization, spatial_weight=spatial_weight, cutoff=cutoff, iterations=iterations, k_adjacency=k_adjacency)(NIBIO_graph_x)\n",
    "    RMIT_nag = CutPursuitPartition(regularization=regularization, spatial_weight=spatial_weight, cutoff=cutoff, iterations=iterations, k_adjacency=k_adjacency)(RMIT_graph_x)\n",
    "    SCION_nag = CutPursuitPartition(regularization=regularization, spatial_weight=spatial_weight, cutoff=cutoff, iterations=iterations, k_adjacency=k_adjacency)(SCION_graph_x)\n",
    "    TUWIEN_nag = CutPursuitPartition(regularization=regularization, spatial_weight=spatial_weight, cutoff=cutoff, iterations=iterations, k_adjacency=k_adjacency)(TUWIEN_graph_x)\n",
    "    \n",
    "    return CULS_nag, NIBIO_nag, RMIT_nag, SCION_nag, TUWIEN_nag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization=[0.1, 0.2]\n",
    "spatial_weight=[0.1, 0.01]\n",
    "cutoff=[10, 30]\n",
    "iterations=15\n",
    "k_adjacency=10\n",
    "CULS_nag, NIBIO_nag, RMIT_nag, SCION_nag, TUWIEN_nag = create_nag_all_data(regularization, spatial_weight, cutoff, iterations, k_adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_level_ratios():\n",
    "    print(\"LEVEL RATIOS\")\n",
    "    print(\"CULS: \", CULS_nag.level_ratios)\n",
    "    print(\"NIBIO: \", NIBIO_nag.level_ratios)\n",
    "    print(\"RMIT: \", RMIT_nag.level_ratios)\n",
    "    print(\"SCION: \", SCION_nag.level_ratios)\n",
    "    print(\"TUWIEN: \", TUWIEN_nag.level_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_level_ratios()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_performance_all_data():\n",
    "    print(\"CULS:\", CULS_nag[1].semantic_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"NIBIO:\", NIBIO_nag[1].semantic_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"RMIT:\", RMIT_nag[1].semantic_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"SCION:\", SCION_nag[1].semantic_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"TUWIEN:\", TUWIEN_nag[1].semantic_segmentation_oracle(FOR_Instance_num_classes))\n",
    "\n",
    "    print(\"CULS:\", CULS_nag[1].instance_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"NIBIO:\", NIBIO_nag[1].instance_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"RMIT:\", RMIT_nag[1].instance_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"SCION:\", SCION_nag[1].instance_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"TUWIEN:\", TUWIEN_nag[1].instance_segmentation_oracle(FOR_Instance_num_classes))\n",
    "\n",
    "    print(\"CULS:\", CULS_nag[1].panoptic_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"NIBIO:\", NIBIO_nag[1].panoptic_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"RMIT:\", RMIT_nag[1].panoptic_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"SCION:\", SCION_nag[1].panoptic_segmentation_oracle(FOR_Instance_num_classes))\n",
    "    print(\"TUWIEN:\", TUWIEN_nag[1].panoptic_segmentation_oracle(FOR_Instance_num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_performance_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CULS_nag.show(class_names=FOR_Instance_CLASS_NAMES, class_colors=FOR_Instance_CLASS_COLORS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
