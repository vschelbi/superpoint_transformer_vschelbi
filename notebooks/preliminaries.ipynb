{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea55bdc-f415-4f5c-9563-303ed17cbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys, os\n",
    "import socket\n",
    "\n",
    "HOST = socket.gethostname()\n",
    "if HOST == 'DEL2001W017':\n",
    "    DATA_ROOT = '/media/drobert-admin/DATA2/datasets'\n",
    "elif HOST == 'HP-2010S002':\n",
    "    DATA_ROOT = '/var/data/drobert/datasets'\n",
    "elif HOST == '9c81b1a54ad8':\n",
    "    DATA_ROOT = '/raid/dataset/pointcloud/data'\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unknown host '{HOST}', cannot set DATA_ROOT\")\n",
    "\n",
    "# file_path = os.path.dirname(os.path.abspath(__file__)) # this is for the .py script but does not work in a notebook\n",
    "file_path = os.path.dirname(os.path.abspath(''))\n",
    "sys.path.append(file_path)\n",
    "# sys.path.append(os.path.join(file_path, \"grid-graph/python/bin\"))\n",
    "# sys.path.append(os.path.join(file_path, \"parallel-cut-pursuit/python/wrappers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60125085-189f-4172-a95b-46fb07a6f970",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a15d1e7-b767-4f28-b22d-80dfecd266aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data 1/342: 0.103s\n",
      "Number of loaded points: 3201318 (3.00M)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import glob\n",
    "from superpoint_transformer.data import Data\n",
    "from superpoint_transformer.datasets.kitti360 import read_kitti360_window\n",
    "from superpoint_transformer.datasets.kitti360_config import KITTI360_NUM_CLASSES\n",
    "\n",
    "i_window = 0\n",
    "all_filepaths = sorted(glob.glob(os.path.join(DATA_ROOT, 'kitti360/shared/data_3d_semantics/*/static/*.ply')))\n",
    "filepath = all_filepaths[i_window]\n",
    "\n",
    "start = time()\n",
    "data = read_kitti360_window(filepath, semantic=True, instance=False, remap=True)\n",
    "print(f'Loading data {i_window+1}/{len(all_filepaths)}: {time() - start:0.3f}s')\n",
    "print(f'Number of loaded points: {data.num_nodes} ({data.num_nodes // 10**6:0.2f}M)')\n",
    "\n",
    "#TODO Offset labels by 1 to account for unlabelled points -> !!!!!!!!!!!!!!!! IMPORTANT !!!!!!!!!!!!!!!!\n",
    "data.y[data.y == -1] = KITTI360_NUM_CLASSES\n",
    "KITTI360_NUM_CLASSES += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91920327-6541-4ef6-a9c3-503023654f3a",
   "metadata": {},
   "source": [
    "# Voxelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b243bd8d-4ed2-45ec-844b-1a58d134815c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f599db-b9c5-4e93-b4d3-2568c7216dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 0.154s\n",
      "Number of sampled points: 3201318 (3.00M, 100.0%)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.pool import voxel_grid\n",
    "\n",
    "start = time()\n",
    "data_sub = voxel_grid(data.pos, size=0.1)\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data_sub.shape[0]} ({data_sub.shape[0] / 10**6:0.2f}M, {100 * data_sub.shape[0] / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c89c00-81ee-4f91-b620-13acfbdbd82a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TorchPoints3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62d17bdf-2ef7-495f-837f-cf22087e8615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### import torch\n",
    "import re\n",
    "from torch_geometric.nn.pool import voxel_grid\n",
    "from torch_cluster import grid_cluster\n",
    "from torch_scatter import scatter_mean, scatter_add\n",
    "from torch_geometric.nn.pool.consecutive import consecutive_cluster\n",
    "\n",
    "\n",
    "def shuffle_data(data):\n",
    "    \"\"\" Shuffle the order of nodes in Data. Only `torch.Tensor` \n",
    "    attributes of size `Data.num_nodes` are affected.  \n",
    "    \n",
    "    Warning: this modifies the input Data object in-place\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Data\n",
    "    \"\"\"\n",
    "    num_points = data.pos.shape[0]\n",
    "    shuffle_idx = torch.randperm(num_points)\n",
    "    for key in set(data.keys):\n",
    "        item = data[key]\n",
    "        if torch.is_tensor(item) and num_points == item.shape[0]:\n",
    "            data[key] = item[shuffle_idx]\n",
    "    return data\n",
    "\n",
    "\n",
    "def group_data(\n",
    "        data, cluster=None, unique_pos_indices=None, mode=\"mean\", skip_keys=[], \n",
    "        bins={}):\n",
    "    \"\"\" Group data based on indices in cluster. The option ``mode`` \n",
    "    controls how data gets aggregated within each cluster.\n",
    "    \n",
    "    Warning: this modifies the input Data object in-place\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Data\n",
    "        [description]\n",
    "    cluster : torch.Tensor\n",
    "        Tensor of the same size as the number of points in data. Each \n",
    "        element is the cluster index of that point.\n",
    "    unique_pos_indices : torch.tensor\n",
    "        Tensor containing one index per cluster, this index will be used\n",
    "        to select features and labels\n",
    "    mode : str\n",
    "        Option to select how the features and labels for each voxel is \n",
    "        computed. Can be ``last`` or ``mean``. ``last`` selects the last \n",
    "        point falling in a voxel as the representent, ``mean`` takes the\n",
    "        average.\n",
    "    skip_keys: list\n",
    "        Keys of attributes to skip in the grouping\n",
    "    bins: dict\n",
    "        Dictionary holding ``{'key': n_bins}`` where ``key`` is a Data \n",
    "        attribute for which we would like to aggregate values into an \n",
    "        histogram and ``n_bins`` accounts for the corresponding number \n",
    "        of bins. This is typically needed when we want to aggregate \n",
    "        point labels without losing the distribution, as opposed to \n",
    "        majority voting.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keys for which voxel aggregation will be based on majority voting\n",
    "    _VOTING_KEYS = [\"y\", \"instance_labels\"]\n",
    "\n",
    "    # Keys for which voxel aggregation will be based on majority voting\n",
    "    _LAST_KEYS = [\"batch\", SaveOriginalPosId.KEY]\n",
    "\n",
    "    assert mode in [\"mean\", \"last\"]\n",
    "    if mode == \"mean\" and cluster is None:\n",
    "        raise ValueError(\n",
    "            \"In mean mode the cluster argument needs to be specified\")\n",
    "    if mode == \"last\" and unique_pos_indices is None:\n",
    "        raise ValueError(\n",
    "            \"In last mode the unique_pos_indices argument needs to be specified\")\n",
    "    \n",
    "    # Save the number of nodes here because the subsequent in-place \n",
    "    # modifications will affect it\n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    # Aggregate Data attributes for same-cluster points\n",
    "    for key, item in data:\n",
    "        \n",
    "        # `skip_keys` are not aggregated\n",
    "        if key in skip_keys:\n",
    "            continue\n",
    "        \n",
    "        # Edges cannot be aggregated\n",
    "        if bool(re.search(\"edge\", key)):\n",
    "            raise ValueError(\"Edges not supported. Wrong data type.\")\n",
    "        \n",
    "        # Only torch.Tensor attributes of size Data.num_nodes are \n",
    "        # considered for aggregation\n",
    "        if torch.is_tensor(item) and item.size(0) == num_nodes:\n",
    "                        \n",
    "            # For 'last' mode, use unique_pos_indices to pick values from \n",
    "            # a single point within each cluster. The same behavior is \n",
    "            # expected for the _LAST_KEYS\n",
    "            if mode == \"last\" or key in _LAST_KEYS:\n",
    "                data[key] = item[unique_pos_indices]\n",
    "            \n",
    "            # For 'mean' mode, the attributes will be aggregated \n",
    "            # depending on their nature\n",
    "            elif mode == \"mean\":\n",
    "                \n",
    "                # If the attribute is a boolean, temporarily convert is \n",
    "                # to integer to facilitate aggregation \n",
    "                is_item_bool = item.dtype == torch.bool\n",
    "                if is_item_bool:\n",
    "                    item = item.int()\n",
    "                \n",
    "                # For keys requiring a voting scheme or a histogram\n",
    "                if key in _VOTING_KEYS or key in bins.keys():\n",
    "                    \n",
    "                    assert item.ge(0).all(), \"Mean aggregation only supports positive integers\"\n",
    "                    assert item.dtype in [torch.uint8, torch.int, torch.long], \"Mean aggregation only supports positive integers\"\n",
    "                                        \n",
    "                    # Initialization\n",
    "                    voting = key not in bins.keys()\n",
    "                    n_bins = item.max() if voting else bins[key]\n",
    "                    \n",
    "                    # Convert values to one-hot encoding. Values are \n",
    "                    # temporarily offset to 0 to save some memory and \n",
    "                    # compute in one-hot encoding and scatter_add\n",
    "                    offset = item.min()\n",
    "                    item = torch.nn.functional.one_hot(item - offset)\n",
    "\n",
    "                    # Count number of occurrence of each value\n",
    "                    hist = scatter_add(item, cluster, dim=0)\n",
    "                    N = hist.shape[0]\n",
    "                    device = hist.device\n",
    "                        \n",
    "                    # Prepend 0 columns to the histogram for bins \n",
    "                    # removed due to offsetting\n",
    "                    bins_before = torch.zeros(\n",
    "                        (N, offset), device=device).long()\n",
    "                    hist = torch.cat((bins_before, hist), dim=1)\n",
    "                        \n",
    "                    # Append columns to the histogram for unobserved \n",
    "                    # classes/bins\n",
    "                    bins_after = torch.zeros(\n",
    "                        (N, n_bins - hist.shape[1]), device=device).long()\n",
    "                    hist = torch.cat((hist, bins_after), dim=1)\n",
    "                    \n",
    "                    # Either save the histogram or the majority vote\n",
    "                    data[key] = hist.argmax(dim=-1) if voting else hist\n",
    "                \n",
    "                # Standard behavior, where attributes are simply \n",
    "                # averaged across the clusters\n",
    "                else:\n",
    "                    data[key] = scatter_mean(item, cluster, dim=0)\n",
    "                    \n",
    "                # Convert back to boolean if need be \n",
    "                if is_item_bool:\n",
    "                    data[key] = data[key].bool()\n",
    "                    \n",
    "    return data\n",
    "\n",
    "\n",
    "class SaveOriginalPosId:\n",
    "    \"\"\"Adds the index of the point to the Data object attributes. This \n",
    "    allows tracking this point from the output back to the input\n",
    "    data object\n",
    "    \"\"\"\n",
    "\n",
    "    KEY = \"origin_id\"\n",
    "\n",
    "    def __init__(self, key=None):\n",
    "        self.KEY = key if key is not None else self.KEY\n",
    "\n",
    "    def _process(self, data):\n",
    "        if hasattr(data, self.KEY):\n",
    "            return data\n",
    "\n",
    "        setattr(data, self.KEY, torch.arange(0, data.pos.shape[0]))\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in data]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class GridSampling3D:\n",
    "    \"\"\" Clusters 3D points into voxels with size :attr:`size`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    size: float\n",
    "        Size of a voxel (in each dimension).\n",
    "    quantize_coords: bool\n",
    "        If True, it will convert the points into their associated sparse\n",
    "        coordinates within the grid and store the value into a new\n",
    "        `coords` attribute.\n",
    "    mode: string:\n",
    "        The mode can be either `last` or `mean`.\n",
    "        If mode is `mean`, all the points and their features within a\n",
    "        cell will be averaged. If mode is `last`, one random points per\n",
    "        cell will be selected with its associated features.\n",
    "     bins: dict\n",
    "        Dictionary holding ``{'key': n_bins}`` where ``key`` is a Data \n",
    "        attribute for which we would like to aggregate values into an \n",
    "        histogram and ``n_bins`` accounts for the corresponding number \n",
    "        of bins. This is typically needed when we want to aggregate \n",
    "        point labels without losing the distribution, as opposed to \n",
    "        majority voting.\n",
    "    inplace: bool\n",
    "        Whether the input Data object should be modified in-place\n",
    "    verbose: bool\n",
    "        Verbosity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, size, quantize_coords=False, mode=\"mean\", bins={}, \n",
    "            inplace=False, verbose=False):\n",
    "        self.grid_size = size\n",
    "        self.quantize_coords = quantize_coords\n",
    "        self.mode = mode\n",
    "        self.bins = bins\n",
    "        self.inplace = inplace\n",
    "        if verbose:\n",
    "            log.warning(\n",
    "                \"If you need to keep track of the position of your points, use \"\n",
    "                \"SaveOriginalPosId transform before using GridSampling3D.\")\n",
    "\n",
    "            if self.mode == \"last\":\n",
    "                log.warning(\n",
    "                    \"The tensors within data will be shuffled each time this \"\n",
    "                    \"transform is applied. Be careful that if an attribute \"\n",
    "                    \"doesn't have the size of num_nodes, it won't be shuffled\")\n",
    "\n",
    "    def _process(self, data_in):\n",
    "        # In-place option will modify the input Data object directly\n",
    "        data = data_in if self.inplace else data_in.clone()\n",
    "        \n",
    "        # If the aggregation mode is 'last', shuffle the point order.\n",
    "        # Note that voxelization of point attributes will be stochastic\n",
    "        if self.mode == \"last\":\n",
    "            data = shuffle_data(data)\n",
    "        \n",
    "        # Convert point coordinates to the voxel grid coordinates\n",
    "        coords = torch.round((data.pos) / self.grid_size)\n",
    "        \n",
    "        # Match each point with a voxel identifier\n",
    "        if \"batch\" not in data:\n",
    "            cluster = grid_cluster(coords, torch.ones(3, device=coords.device))\n",
    "        else:\n",
    "            cluster = voxel_grid(coords, data.batch, 1)\n",
    "            \n",
    "        # Reindex the clusters to make sure the indices used are \n",
    "        # consecutive. Basically, we do not want cluster indices to span \n",
    "        # [0, i_max] without all in-between indices to be used, because\n",
    "        # this will affect the speed and output size of torch_scatter \n",
    "        # operations \n",
    "        cluster, unique_pos_indices = consecutive_cluster(cluster)\n",
    "        \n",
    "        # Perform voxel aggregation \n",
    "        data = group_data(\n",
    "            data, cluster, unique_pos_indices, mode=self.mode, bins=self.bins)\n",
    "        \n",
    "        # Optionally convert quantize the coordinates. This is useful \n",
    "        # for sparse convolution models \n",
    "        if self.quantize_coords:\n",
    "            data.coords = coords[unique_pos_indices].int()\n",
    "        \n",
    "        # Save the grid size in the Data attributes\n",
    "        data.grid_size = torch.tensor([self.grid_size])\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in data]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}(grid_size={}, quantize_coords={}, mode={})\".format(\n",
    "            self.__class__.__name__, self.grid_size, self.quantize_coords, \n",
    "            self.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8daf6b4d-a4cc-440a-96f4-539e51899066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 2.559s\n",
      "Number of sampled points: 2480151 (2.00M, 77.5%)\n"
     ]
    }
   ],
   "source": [
    "# CPU\n",
    "start = time()\n",
    "data_sub = GridSampling3D(size=voxel)(data)\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data_sub.num_nodes} ({data_sub.num_nodes / 10**6:0.2f}M, {100 * data_sub.num_nodes / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "e7e2fed1-f95e-43d6-8393-5256ee74d83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 0.120s\n",
      "Number of sampled points: 2480168 (2.00M, 77.5%)\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "data_sub = GridSampling3D(size=voxel)(data.cuda()).cpu()\n",
    "torch.cuda.synchronize()\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data_sub.num_nodes} ({data_sub.num_nodes / 10**6:0.2f}M, {100 * data_sub.num_nodes / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd9504-ee3f-4d55-92cc-de76f11769a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SPG C implem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f788d00b-6ddf-4f61-b708-41edc2c2669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 8.355s\n",
      "Number of sampled points: 2479935 (2.00M, 77.5%)\n"
     ]
    }
   ],
   "source": [
    "import superpoint_transformer.partition.utils.libpoint_utils as point_utils\n",
    "\n",
    "# WARNING: the pruning must know the number of classes. All labels are \n",
    "# offset to account for the -1 unlabeled points !\n",
    "start = time()\n",
    "xyz, rgb, labels, dump = point_utils.prune(data.pos.float().numpy(), voxel, (data.rgb * 255).byte().numpy(), data.y.byte().numpy() + 1, np.zeros(1, dtype='uint8'), KITTI360_NUM_CLASSES + 1, 0)\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {xyz.shape[0]} ({xyz.shape[0] // 10**6:0.2f}M, {100 * xyz.shape[0] / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff552778-a2ae-4a65-a313-ec805cc48a90",
   "metadata": {},
   "source": [
    "So it seems the C-based voxelization is not that fast. Can we somehow make it faster with more CPU cores ? Otherwise, will fallback to a custom implementation based on TP3D or PyG and keeping track of the in-voxel label distribution.\n",
    "\n",
    "And even increasing the number of CPU cores (on AI4GEO) gave the same results.\n",
    "\n",
    "The fastest is GPU-based TP3D-based computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecebd3-2791-4366-9483-efec84c96221",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0f8fa5-cdce-40d1-b907-50677eb6f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data voxelization at 0.2m: 1.637s\n",
      "Number of sampled points: 507287 (0.51M, 15.8%)\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.transforms import GridSampling3D\n",
    "\n",
    "voxel = 0.05\n",
    "# voxel = 0.2\n",
    "# voxel = 1\n",
    "\n",
    "# GPU\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "n_in = data.num_nodes\n",
    "data = GridSampling3D(size=voxel, bins={'y': KITTI360_NUM_CLASSES})(data.cuda()).cpu()\n",
    "torch.cuda.synchronize()\n",
    "print(f'Data voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data.num_nodes} ({data.num_nodes / 10**6:0.2f}M, {100 * data.num_nodes / n_in:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1154d-83fe-4ca7-96d7-b75aa05f2d50",
   "metadata": {},
   "source": [
    "# Neighbour search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9cf13-0001-4836-9dc7-3cddb0a896aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2f1226-bc57-45af-9d0a-0d3b8ca6c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 15.029s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "start = time()\n",
    "kdt = KDTree(x.numpy(), leaf_size=30, metric='euclidean')\n",
    "neighbors = kdt.query(x.numpy(), k=k, return_distance=False)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d1cf9-5120-45d7-b542-9121a7e78e1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FAISS-GPU\n",
    "```\n",
    "conda install -c pytorch faiss-gpu cudatoolkit=10.2\n",
    "pip install faiss-gpu cudatoolkit==10.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c50e50-ed80-4b20-af5c-3420dfe0fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def find_neighbours(x, y, k=10, ncells=None, nprobes=10):\n",
    "    # if batch_x is not None or batch_y is not None:\n",
    "    #     raise NotImplementedError(\n",
    "    #         \"FAISSGPUKNNNeighbourFinder does not support batches yet\")\n",
    "\n",
    "    x = x.view(-1, 1) if x.dim() == 1 else x\n",
    "    y = y.view(-1, 1) if y.dim() == 1 else y\n",
    "    x, y = x.contiguous(), y.contiguous()\n",
    "\n",
    "    # FAISS-GPU consumes numpy arrays\n",
    "    x_np = x.cpu().numpy()\n",
    "    y_np = y.cpu().numpy()\n",
    "\n",
    "    # Initialization\n",
    "    n_fit = x_np.shape[0]\n",
    "    d = x_np.shape[1]\n",
    "    gpu = faiss.StandardGpuResources()\n",
    "\n",
    "    # Heuristics to prevent k from being too large\n",
    "    k_max = 1024\n",
    "    k = min(k, n_fit, k_max)\n",
    "\n",
    "    # Heuristic to parameterize the number of cells for FAISS index,\n",
    "    # if not provided\n",
    "    if ncells is None:\n",
    "        f1 = 3.5 * np.sqrt(n_fit)\n",
    "        f2 = 1.6 * np.sqrt(n_fit)\n",
    "        if n_fit > 2 * 10 ** 6:\n",
    "            p = 1 / (1 + np.exp(2 * 10 ** 6 - n_fit))\n",
    "        else:\n",
    "            p = 0\n",
    "        ncells = int(p * f1 + (1 - p) * f2)\n",
    "\n",
    "    # Building a GPU IVFFlat index + Flat quantizer\n",
    "    torch.cuda.empty_cache()\n",
    "    quantizer = faiss.IndexFlatL2(d)  # the quantizer index\n",
    "    index = faiss.IndexIVFFlat(quantizer, d, ncells, faiss.METRIC_L2)  # the main index\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(gpu, 0, index)  # pass index it to GPU\n",
    "    gpu_index_flat.train(x_np)  # fit the cells to the training set distribution\n",
    "    gpu_index_flat.add(x_np)\n",
    "\n",
    "    # Querying the K-NN\n",
    "    gpu_index_flat.setNumProbes(nprobes)\n",
    "    return torch.LongTensor(gpu_index_flat.search(y_np, k)[1]).to(x.device)\n",
    "\n",
    "start = time()\n",
    "out = find_neighbours(x, x, k=k, ncells=None, nprobes=10)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64251d5-1bc9-48b2-996a-8e4f16af9ec4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### PyKeOps\n",
    "```\n",
    "pip install pykeops\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96a27be1-ae38-47c3-bd86-8a9c0847f3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 39.054s\n"
     ]
    }
   ],
   "source": [
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "start = time()\n",
    "# K-NN search with KeOps. If the number of points is greater\n",
    "# than 16 millions, KeOps requires double precision.\n",
    "xyz_query = x.contiguous()\n",
    "xyz_search = x.contiguous()\n",
    "if xyz_search.shape[0] > 1.6e7:\n",
    "    xyz_query_keops = LazyTensor(xyz_query[:, None, :].double())\n",
    "    xyz_search_keops = LazyTensor(xyz_search[None, :, :].double())\n",
    "else:\n",
    "    xyz_query_keops = LazyTensor(xyz_query[:, None, :])\n",
    "    xyz_search_keops = LazyTensor(xyz_search[None, :, :])\n",
    "d_keops = ((xyz_query_keops - xyz_search_keops) ** 2).sum(dim=2)\n",
    "neighbors = d_keops.argKmin(k, dim=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f2b55-d631-4462-a002-19ef72890144",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FLANN\n",
    "```\n",
    "conda install -c conda-forge pyflann -y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68542da-735f-4ec5-9d05-1eb978297738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Cannot load dynamic library. Did you compile FLANN?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyflann\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m flann \u001b[38;5;241m=\u001b[39m pyflann\u001b[38;5;241m.\u001b[39mFLANN()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/__init__.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load, save\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/index.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2010  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2010  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbindings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflann_ctypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_rn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/bindings/__init__.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#from pyflann_parameters import parameter_list, algorithm_names\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#from pyflann_parameters import centers_init_names, log_level_names\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflann_ctypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/bindings/flann_ctypes.py:173\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m flannlib \u001b[38;5;241m=\u001b[39m load_flann_library()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flannlib \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot load dynamic library. Did you compile FLANN?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFlannLib\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Cannot load dynamic library. Did you compile FLANN?"
     ]
    }
   ],
   "source": [
    "import pyflann\n",
    "\n",
    "start = time()\n",
    "flann = pyflann.FLANN()\n",
    "result, dists = flann.nn(x.numpy(), x.numpy(), k, algorithm=\"kmeans\", branching=32, iterations=7, checks=16)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0697c4b-15ae-42c8-b174-1c4135a3b83c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36414778-4228-4107-841c-c1e381684775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.466s\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import knn\n",
    "\n",
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23c967fe-b273-4717-b5be-ee20ac3b50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.215s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=2)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8abf861-b022-4f2e-aef0-3b1cd09326d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.076s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=4)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df017adf-57eb-4275-9a6d-8cfff59742af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.438s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=8)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d3b44-456c-407c-b5d5-abc2cf4fc534",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cuda = x.cuda()\n",
    "start = time()\n",
    "out = knn(x_cuda, x_cuda, k, batch_x=None, batch_y=None, num_workers=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')\n",
    "del x_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c7676-bc84-4464-a12c-ee992985c660",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GriSPy\n",
    "```\n",
    "pip install grispy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea9ee609-fd8d-4944-a52b-d8052fc98f97",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m grid \u001b[38;5;241m=\u001b[39m gsp\u001b[38;5;241m.\u001b[39mGriSPy(x\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m----> 5\u001b[0m dist, ind \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnearest_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeighbor search: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime() \u001b[38;5;241m-\u001b[39m start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:1206\u001b[0m, in \u001b[0;36mGriSPy.nearest_neighbors\u001b[0;34m(self, centres, n, kind)\u001b[0m\n\u001b[1;32m   1204\u001b[0m neighbors_distances \u001b[38;5;241m=\u001b[39m [EMPTY_ARRAY\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_centres)]\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(n_found):\n\u001b[0;32m-> 1206\u001b[0m     ndis_tmp, nidx_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell_neighbors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcentres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower_distance_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper_distance_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i_tmp, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(centres_lookup_ind[\u001b[38;5;241m~\u001b[39mn_found]):\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(nidx_tmp[i_tmp]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(neighbors_indices[i]):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:1089\u001b[0m, in \u001b[0;36mGriSPy.shell_neighbors\u001b[0;34m(self, centres, distance_lower_bound, distance_upper_bound, sorted, kind)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     vlds\u001b[38;5;241m.\u001b[39mvalidate_equalsize(centres, distance_upper_bound)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Get neighbors\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m neighbor_cells \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_neighbor_cells\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentres\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1096\u001b[0m neighbors_distances, neighbors_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_neighbor_distance(\n\u001b[1;32m   1097\u001b[0m     centres, neighbor_cells\n\u001b[1;32m   1098\u001b[0m )\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;66;03m# We need to generate mirror centres for periodic boundaries...\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:798\u001b[0m, in \u001b[0;36mGriSPy._get_neighbor_cells\u001b[0;34m(self, centres, distance_upper_bound, distance_lower_bound, shell_flag)\u001b[0m\n\u001b[1;32m    792\u001b[0m k_grids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    793\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(k_cell_min[i, k], k_cell_max[i, k] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m    795\u001b[0m ]\n\u001b[1;32m    796\u001b[0m k_grids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(\u001b[38;5;241m*\u001b[39mk_grids)\n\u001b[1;32m    797\u001b[0m neighbor_cells \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 798\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_grids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    799\u001b[0m ]\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Calculo la distancia de cada centro i a sus celdas vecinas,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# luego descarto las celdas que no toca el circulo definido por\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# la distancia\u001b[39;00m\n\u001b[1;32m    804\u001b[0m cells_physical \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_bins[neighbor_cells[i][:, k], k] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m cell_size[k]\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m    807\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import grispy as gsp\n",
    "\n",
    "start = time()\n",
    "grid = gsp.GriSPy(x.numpy())\n",
    "dist, ind = grid.nearest_neighbors(x.numpy(), n=k)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21847b0a-38e3-4a40-8e5c-fe8c889a5912",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FRNN = Final\n",
    "https://github.com/lxxue/FRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ff6e5-54a5-46c9-9ffb-7ef616e0f34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from superpoint_transformer.partition.FRNN import frnn\n",
    "import torch\n",
    "\n",
    "\n",
    "def _search_outliers(\n",
    "        xyz_query, xyz_search, k_min, r_max=1, recursive=False, q_in_s=False):\n",
    "    \"\"\"\n",
    "    Optionally recursive outlier search. The `xyz_query` and `xyz_search`\n",
    "    Search for points with less than `k_min` neighbors within a radius \n",
    "    of `r_max`. \n",
    "    \n",
    "    Since removing outliers may cause some points to become outliers \n",
    "    themselves, this problem can be tackled with the `recursive` option. \n",
    "    Note that this recursive search holds no garantee of reasonable \n",
    "    convergence as one could design a point cloud for given `k_min` and \n",
    "    `r_max` whose points would all recursively end up as outliers.  \n",
    "    \"\"\"\n",
    "    # Data initialization\n",
    "    xyz_query = xyz_query.view(1, -1, 3)\n",
    "    xyz_search = xyz_search.view(1, -1, 3)\n",
    "    device = xyz_query.device\n",
    "    \n",
    "    # KNN on GPU. Actual neighbor search now\n",
    "    neighbors = frnn.frnn_grid_points(\n",
    "        xyz_query, xyz_search, K=k_min + q_in_s, r=r_max)[1]\n",
    "    \n",
    "    # If the Query points are included in the Search points, remove each\n",
    "    # point from its own neighborhood\n",
    "    if q_in_s:\n",
    "        neighbors = neighbors[0][:, 1:]\n",
    "    \n",
    "    # Get the number of found neighbors for each point. Indeed, \n",
    "    # depending on the cloud properties and the chosen K and radius, \n",
    "    # some points may receive \"-1\" neighbors\n",
    "    n_found_nn = (neighbors != -1).sum(dim=1)\n",
    "\n",
    "    # Identify points which have less than k_min neighbor. Those are \n",
    "    # treated as outliers\n",
    "    mask_outliers = n_found_nn < k_min\n",
    "    idx_outliers = torch.where(mask_outliers)[0]\n",
    "    idx_inliers = torch.where(~mask_outliers)[0]\n",
    "    \n",
    "    # Exit here if not recursively searching for outliers \n",
    "    if not recursive:\n",
    "        return idx_outliers, idx_inliers\n",
    "    \n",
    "    # Identify the points affected by the removal of the outliers. Those\n",
    "    # inliers are potential outliers\n",
    "    idx_potential = torch.where(\n",
    "        torch.isin(neighbors[idx_inliers], idx_outliers).any(dim=1))[0]\n",
    "        \n",
    "    # Exit here if there are no potential new outliers among the inliers\n",
    "    if idx_potential.shape[0] == 0:\n",
    "        return idx_outliers, idx_inliers\n",
    "    \n",
    "    # Recursviely search actual outliers among the potential\n",
    "    xyz_query_sub = xyz_query[0, idx_inliers[idx_potential]]\n",
    "    xyz_search_sub = xyz_search[0, idx_inliers]\n",
    "    idx_outliers_sub, idx_inliers_sub = _search_outliers(\n",
    "        xyz_query_sub, xyz_search_sub, k_min, r_max=r_max, recursive=True, \n",
    "        q_in_s=True)\n",
    "    \n",
    "    # Update the outliers mask\n",
    "    mask_outliers[idx_inliers[idx_potential][idx_outliers_sub]] = True\n",
    "    idx_outliers = torch.where(mask_outliers)[0]\n",
    "    idx_inliers = torch.where(~mask_outliers)[0]\n",
    "    \n",
    "    return idx_outliers, idx_inliers\n",
    "\n",
    "\n",
    "def search_outliers(data, k_min, r_max=1, recursive=False):\n",
    "    \"\"\"\n",
    "    Search for points with less than `k_min` neighbors within a radius \n",
    "    of `r_max`. \n",
    "    \n",
    "    Since removing outliers may cause some points to become outliers \n",
    "    themselves, this problem can be tackled with the `recursive` option. \n",
    "    Note that this recursive search holds no garantee of reasonable \n",
    "    convergence as one could design a point cloud for given `k_min` and \n",
    "    `r_max` whose points would all recursively end up as outliers.  \n",
    "    \"\"\"       \n",
    "    # Actual outlier search, optionally recursive\n",
    "    idx_outliers, idx_inliers = _search_outliers(\n",
    "        data.pos, data.pos, k_min, r_max=r_max, recursive=recursive, \n",
    "        q_in_s=True)\n",
    "    \n",
    "    # Create a Data object for the inliers and outliers    \n",
    "    # Save the index for these isolated points in the Data object. This\n",
    "    # will help properly handle neighborhoods, features and adjacency  \n",
    "    # graph for those specific points. \n",
    "    # NB: it is important this attribute follows the \"*index\" naming \n",
    "    # convention, see:\n",
    "    # https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
    "    data_in = Data()\n",
    "    data_out = Data(outliers_index=idx_outliers)\n",
    "    for key, item in data:\n",
    "        if torch.is_tensor(item) and item.size(0) == data.num_nodes:\n",
    "            data_in[key] = data[key][idx_inliers]\n",
    "            data_out[key] = data[key][idx_outliers]\n",
    "\n",
    "    return data_in, data_out\n",
    "\n",
    "\n",
    "def oversample_partial_neighborhoods(neighbors, distances, k):\n",
    "    \"\"\"\n",
    "    Oversample partial neighborhoods with less than k points. Missing \n",
    "    neighbors are indicated by the \"-1\" index.\n",
    "    \n",
    "    Remarks\n",
    "      - Neighbors and distances are assumed to be sorted in order of \n",
    "      increasing distance\n",
    "      - All neighbors are assumed to have at least one valid neighbor. \n",
    "      See `search_outliers` to remove points with not enough neighbors \n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    assert neighbors.dim() == distances.dim() == 2\n",
    "    device = neighbors.device\n",
    "    \n",
    "    # Get the number of found neighbors for each point. Indeed, \n",
    "    # depending on the cloud properties and the chosen K and radius, \n",
    "    # some points may receive `-1` neighbors\n",
    "    n_found_nn = (neighbors != -1).sum(dim=1)\n",
    "        \n",
    "    # Identify points which have more than k_min and less than k \n",
    "    # neighbors within R. For those, we oversample the neighbors to \n",
    "    # reach k\n",
    "    idx_partial = torch.where(n_found_nn < k)[0]\n",
    "    neighbors_partial = neighbors[idx_partial]\n",
    "    distances_partial = distances[idx_partial]\n",
    "\n",
    "    # Since the neighbors are sorted by increasing distance, the missing \n",
    "    # neighbors will always be the last ones. This helps finding their \n",
    "    # number and position, for oversampling.\n",
    "\n",
    "    #*******************************************************************\n",
    "    # The above statement is actually INCORRECT because the outlier \n",
    "    # removal may produce \"-1\" neighbors at unexpected positions. So \n",
    "    # either we manage to treat this in a clean vectorized way, or we \n",
    "    # fall back to the 2-searches solution...\n",
    "    # Honestly, this feels like it is getting out of hand, let's keep \n",
    "    # things simple, since we are not going to save so much computation \n",
    "    # time with KNN wrt the partition.\n",
    "    #*******************************************************************\n",
    "    \n",
    "    # For each missing neighbor, compute the size of the discrete set to \n",
    "    # oversample from.\n",
    "    n_valid = n_found_nn[idx_partial].repeat_interleave(\n",
    "        k - n_found_nn[idx_partial])\n",
    "\n",
    "    # Compute the oversampling row indices.\n",
    "    idx_x_sampling = torch.arange(neighbors_partial.shape[0], device=device\n",
    "        ).repeat_interleave(k - n_found_nn[idx_partial])\n",
    "    \n",
    "    # Compute the oversampling column indices. The 0.9999 factor is a \n",
    "    # security to handle the case where torch.rand is to close to 1.0, \n",
    "    # which would yield incorrect sampling coordinates that would in \n",
    "    # result in sampling '-1' indices (ie all we try to avoid here)\n",
    "    idx_y_sampling = (n_valid * torch.rand(\n",
    "        n_valid.shape[0], device=device) * 0.9999).floor().long()\n",
    "\n",
    "    # Apply the oversampling\n",
    "    idx_missing = torch.where(neighbors_partial == -1)\n",
    "    neighbors_partial[idx_missing] = neighbors_partial[\n",
    "        idx_x_sampling, idx_y_sampling]\n",
    "    distances_partial[idx_missing] = distances_partial[\n",
    "        idx_x_sampling, idx_y_sampling]\n",
    "\n",
    "    # Restore the oversampled neighborhods with the rest\n",
    "    neighbors[idx_partial] = neighbors_partial\n",
    "    distances[idx_partial] = distances_partial\n",
    "    \n",
    "    return neighbors, distances\n",
    "    \n",
    "\n",
    "def search_neighbors(data, k, r_max=1):\n",
    "    # Data initialization\n",
    "    xyz_query = data.pos.view(1, -1, 3)\n",
    "    xyz_search = data.pos.view(1, -1, 3)\n",
    "    \n",
    "#     #--------------------------------\n",
    "#     # KNN on GPU. Search for outliers first\n",
    "#     _, neighbors, _, _ = frnn.frnn_grid_points(\n",
    "#         xyz_query, xyz_search, K=k_min + 1, r=r_max)\n",
    "    \n",
    "#     # Remove each point from its own neighborhood\n",
    "#     neighbors = neighbors[0][:, 1:]\n",
    "    \n",
    "#     # Get the number of found neighbors for each point. Indeed, \n",
    "#     # depending on the cloud properties and the chosen K and radius, \n",
    "#     # some points may receive `-1` neighbors\n",
    "#     n_found_nn = (neighbors != -1).sum(dim=1)\n",
    "\n",
    "#     # Identify points which have less than k_min neighbors within R. \n",
    "#     # Those are treated as outliers and will be discarded\n",
    "#     idx_isolated = torch.where(n_found_nn < k_min)[0]\n",
    "    \n",
    "#     # Save the outliers in a separate Data object\n",
    "#     outliers = Data(\n",
    "#         pos=data.pos[idx_isolated], rgb=data.rgb[idx_isolated], \n",
    "#         y=data.y[idx_isolated], idx_isolated=idx_isolated)\n",
    "    \n",
    "#     # KNN on GPU. Search for outliers first\n",
    "#     _, neighbors, _, _ = frnn.frnn_grid_points(\n",
    "#         xyz_query, xyz_search, K=k_min + 1, r=r_max)\n",
    "#     #--------------------------------\n",
    "    \n",
    "    # KNN on GPU. Actual neighbor search now\n",
    "    distances, neighbors, _, _ = frnn.frnn_grid_points(\n",
    "        xyz_query, xyz_search, K=k + 1, r=r_max)\n",
    "    \n",
    "    # Remove each point from its own neighborhood\n",
    "    neighbors = neighbors[0][:, 1:]\n",
    "    distances = distances[0][:, 1:]\n",
    "\n",
    "    # Oversample the neighborhoods where less than k points were found\n",
    "    neighbors, distances = oversample_partial_neighborhoods(\n",
    "        neighbors, distances, k)\n",
    "    \n",
    "    # Store the neighbors and distances as a Data object attribute\n",
    "    data.neighbors = neighbors.cpu()\n",
    "    data.distances = distances.cpu()\n",
    "    \n",
    "    # Save the index for these isolated points in the Data object. This\n",
    "    # will help properly handle neighborhoods, features and adjacency  \n",
    "    # graph for those specific points. \n",
    "    # NB: it is important this attribute follows the \"*index\" naming \n",
    "    # convention, see:\n",
    "    # https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
    "    # data.isolated_index = idx_isolated.cpu()\n",
    "    \n",
    "    return data\n",
    "\n",
    "# IMPORTANT !!!\n",
    "#   - points with no neighbors within radius -> set to 0-feature !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0a73b-a530-4e0e-9479-d4ac6d92411f",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17541b2-7dd3-4a7d-8adf-56c84474996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers search: 0.078s\n",
      "Neighbor search: 0.263s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.transforms import search_outliers, search_neighbors\n",
    "\n",
    "radius = 1\n",
    "# radius = 2\n",
    "# radius = 10\n",
    "k_min = 5\n",
    "k_feat = 30\n",
    "k_adjacency = 10\n",
    "\n",
    "data = data.cuda()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "data, data_outliers = search_outliers(data, k_min, r_max=radius, recursive=True)\n",
    "data_outliers = data_outliers.cpu()\n",
    "torch.cuda.synchronize()\n",
    "print(f'Outliers search: {time() - start:0.3f}s')\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "data = search_neighbors(data, k_feat, r_max=radius)\n",
    "# Make sure all points have k neighbors (no \"-1\" missing neighbors)\n",
    "assert (data.neighbors != -1).all(), \"Some points have incomplete neighborhoods, make sure to remove the outliers to avoid this issue.\"\n",
    "torch.cuda.synchronize()\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b83d5-26d9-483a-8af6-e82aa12dc219",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pytorch3D\n",
    "```\n",
    "pip install -U fvcore\n",
    "pip install -U iopath\n",
    "pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py38_cu113_pyt1110/download.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da3c782a-6ded-4fa8-99bc-97abf128b634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch3d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch3d\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m pytorch3d\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mknn_points(x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), K\u001b[38;5;241m=\u001b[39mk)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch3d'"
     ]
    }
   ],
   "source": [
    "import pytorch3d\n",
    "\n",
    "start = time()\n",
    "pytorch3d.ops.knn_points(x.view(1, -1, 3), x.view(1, -1, 3), K=k)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b6976-3574-43c2-880f-7cc0b0c6ba0d",
   "metadata": {},
   "source": [
    "So it seems FRNN on GPU is the clear winner here !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8fc2a8-cc69-40a1-9583-47e8ea497f54",
   "metadata": {},
   "source": [
    "# !!! ___ CAREFUL WITH CPU-CUDA MOVES ___ !!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5d1c5-db97-4d56-9b4e-2a7ec809270f",
   "metadata": {},
   "source": [
    "# Geometric features computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c92e36-c264-4b57-ba96-27f9b7deed01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SPG C implem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ba8949-851e-4c0b-b307-bdc029562c01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric features: 1.909s\n"
     ]
    }
   ],
   "source": [
    "import superpoint_transformer.partition.utils.libpoint_utils as point_utils\n",
    "\n",
    "data = data.cpu()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "geof = point_utils.compute_geometric_features(\n",
    "    data.pos.numpy(), data.neighbors.flatten().numpy().astype('uint32'), \n",
    "    np.arange(data.pos.shape[0] + 1).astype('uint32') * k_feat, False).astype('float32')  # IMPORTANT CAREFUL WITH UINT32 = 4G MAX\n",
    "print(f'Geometric features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ba227-7cd4-4129-b184-2a034f87416a",
   "metadata": {},
   "source": [
    "This is the fasest way of computing the geometric features. Surprisingly, the CPU implementation is faster than the TP3D-based GPU one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794dc55-e621-4087-b82e-259278253ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import superpoint_transformer.partition.utils.libpoint_utils as point_utils\n",
    "\n",
    "def compute_pointfeatures(\n",
    "        data, pos=True, radius=5, rgb=True, linearity=True, planarity=True,\n",
    "        scattering=True, verticality=True, normal=True, length=False, \n",
    "        surface=False, volume=False):\n",
    "    \"\"\" Compute the pointwise features that will be used for the \n",
    "    partition.\n",
    "    \n",
    "    All local geometric features assume the input ``Data`` has a \n",
    "    ``neighbors`` attribute, holding a ``(num_nodes, k)`` tensor of \n",
    "    indices. All k neighbors will be used for local geometric features \n",
    "    computation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pos: bool\n",
    "        Use point position.\n",
    "    radius: bool\n",
    "        Radius used to scale the point position features, to mitigate \n",
    "        the maximum superpoint size.\n",
    "    rgb: bool\n",
    "        Use rgb color. Assumes Data.rgb holds either [0, 1] flaots or \n",
    "        [0, 255] integers\n",
    "    linearity: bool\n",
    "        Use local linearity. Assumes ``Data.neighbors``.\n",
    "    lanarity: bool\n",
    "        Use local lanarity. Assumes ``Data.neighbors``.\n",
    "    scattering: bool\n",
    "        Use local scattering. Assumes ``Data.neighbors``.\n",
    "    verticality: bool\n",
    "        Use local verticality. Assumes ``Data.neighbors``.\n",
    "    normal: bool\n",
    "        Use local normal. Assumes ``Data.neighbors``.\n",
    "    length: bool\n",
    "        Use local length. Assumes ``Data.neighbors``.\n",
    "    surface: bool\n",
    "        Use local surface. Assumes ``Data.neighbors``.\n",
    "    volume: bool\n",
    "        Use local volume. Assumes ``Data.neighbors``.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Add xyz normalized. The scaling factor drives the maximum cluster\n",
    "    # size the partition may produce\n",
    "    if 'pos' in data.keys:\n",
    "        features.append(data.pos / radius)\n",
    "    \n",
    "    # Add rgb to the features. If colors are stored in int, we assume \n",
    "    # they are encoded in  [0, 255] and normalize them. Otherwise, we \n",
    "    # assume they have already been [0, 1] normalized\n",
    "    if rgb:\n",
    "        f = data.rgb\n",
    "        if f.type in [torch.uint8, torch.int, torch.long]:\n",
    "            f = f.float() / 255\n",
    "        features.append(f)\n",
    "    \n",
    "    # Add local geometric features\n",
    "    if any((linearity, planarity, scattering, verticality, normal)):\n",
    "        \n",
    "        # Prepare data for numpy boost interface\n",
    "        xyz = data.pos.cpu().numpy()\n",
    "        nn = data.neighbors.flatten().cpu().numpy().astype('uint32')  # !!!! IMPORTANT CAREFUL WITH UINT32 = 4 BILLION points MAXIMUM !!!!\n",
    "        k = data.neighbors.shape[1]\n",
    "        nn_ptr = np.arange(xyz.shape[0] + 1).astype('uint32') * k  # !!!! IMPORTANT CAREFUL WITH UINT32 = 4 BILLION points MAXIMUM !!!!\n",
    "        \n",
    "        # C++ geometric features computation on CPU\n",
    "        f = point_utils.compute_geometric_features(xyz, nn, nn_ptr, False)\n",
    "        f = torch.from_numpy(f.astype('float32'))\n",
    "        \n",
    "        # Heuristic to increase the importance of verticality\n",
    "        f[:, 3] *= 2\n",
    "        \n",
    "        # Select only required features\n",
    "        mask = (\n",
    "            [linearity, planarity, scattering, verticality] \n",
    "            + [normal] * 3 \n",
    "            + [length, surface, volume])\n",
    "        features.append(f[:, mask].to(data.pos.device))\n",
    "        \n",
    "    # Save all features in the Data.x attribute\n",
    "    data.x = torch.cat(features, dim=1).to(data.pos.device)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15cb47-c547-45b0-ae27-914a9bcf1812",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TP3D-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29229ed-a53b-4c5b-a31e-bb7e53383ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from superpoint_transformer.data import Data\n",
    "\n",
    "def batch_pca(xyz):\n",
    "    \"\"\"\n",
    "    Compute the PCA of a batch of point clouds of size (*, N, M).\n",
    "    \"\"\"\n",
    "    assert 2 <= xyz.dim() <= 3\n",
    "    xyz = xyz.unsqueeze(0) if xyz.dim() == 2 else xyz\n",
    "\n",
    "    pos_centered = xyz - xyz.mean(dim=1).unsqueeze(1)\n",
    "    cov_matrix = pos_centered.transpose(1, 2).bmm(pos_centered) / pos_centered.shape[1]\n",
    "    eigenval, eigenvect = torch.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # If Nan values are computed, return equal eigenvalues and\n",
    "    # Identity eigenvectors\n",
    "    idx_nan = torch.where(torch.logical_and(\n",
    "        eigenval.isnan().any(1), eigenvect.flatten(1).isnan().any(1)))\n",
    "    eigenval[idx_nan] = torch.ones(3, dtype=eigenval.dtype, device=xyz.device)\n",
    "    eigenvect[idx_nan] = torch.eye(3, dtype=eigenvect.dtype, device=xyz.device)\n",
    "\n",
    "    # Precision errors may cause close-to-zero eigenvalues to be\n",
    "    # negative. Hard-code these to zero\n",
    "    eigenval[torch.where(eigenval < 0)] = 0\n",
    "\n",
    "    return eigenval, eigenvect\n",
    "\n",
    "\n",
    "class PCAComputePointwise(object):\n",
    "    \"\"\"\n",
    "    Compute PCA for the local neighborhood of each point in the cloud.\n",
    "\n",
    "    Input data is expected to be stored in DENSE format.\n",
    "\n",
    "    Results are saved in `eigenvalues` and `eigenvectors` attributes.\n",
    "    `data.eigenvalues` is a tensor\n",
    "    :math:`(\\lambda_1, \\lambda_2, \\lambda_3)` such that\n",
    "    :math:`\\lambda_1 \\leq \\lambda_2 \\leq \\lambda_3`.\n",
    "    `data.eigenvectors` is 1x9 tensor containing the eigenvectors\n",
    "    associated with `data.eigenvalues`, concatenated in the same order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_neighbors: int, optional\n",
    "        Controls the maximum number of neighbors on which to compute\n",
    "        PCA. If `r=None`, `num_neighbors` will be used as K for\n",
    "        K-nearest neighbor search. Otherwise, `num_neighbors` will be\n",
    "        the maximum number of neighbors used in radial neighbor search.\n",
    "    r: float, optional\n",
    "        If not `None`, neighborhoods will be computed with a\n",
    "        radius-neighbor approach. If `None`, K-nearest neighbors will\n",
    "        be used.\n",
    "    use_full_pos: bool, optional\n",
    "        If True, the neighborhood search will be carried on the point\n",
    "        positions found in the `data.full_pos`. An error will be raised\n",
    "        if data carries no such attribute. See `GridSampling3D` for\n",
    "        producing `data.full_pos`.\n",
    "        If False, the neighbor search will be computed on `data.pos`.\n",
    "    use_cuda: bool, optional\n",
    "        If True, the computation will be carried on CUDA.\n",
    "    workers: int, optional\n",
    "        If not `None`, the features computation will be distributed\n",
    "        across the provided number of workers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, num_neighbors=40, r=None, use_full_pos=False, use_cuda=False,\n",
    "            use_faiss=True, ncells=None, nprobes=10, chunk_size=1000000):\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.r = r\n",
    "        self.use_full_pos = use_full_pos\n",
    "        self.use_cuda = use_cuda and torch.cuda.is_available()\n",
    "        self.use_faiss = use_faiss and torch.cuda.is_available()\n",
    "        self.ncells = ncells\n",
    "        self.nprobes = nprobes\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def _process(self, data: Data):\n",
    "        assert getattr(data, 'pos', None) is not None, \\\n",
    "            \"Data must contain a 'pos' attribute.\"\n",
    "        assert not self.use_full_pos \\\n",
    "               or getattr(data, 'full_pos', None) is not None, \\\n",
    "            \"Data must contain a 'full_pos' attribute.\"\n",
    "\n",
    "        # Recover the query and search clouds\n",
    "        xyz_query = data.pos\n",
    "        xyz_search = data.full_pos if self.use_full_pos else data.pos\n",
    "\n",
    "        # Move computation to CUDA if required\n",
    "        input_device = xyz_query.device\n",
    "        if self.use_cuda and not xyz_query.is_cuda and not self.use_faiss:\n",
    "            xyz_query = xyz_query.cuda()\n",
    "            xyz_search = xyz_search.cuda()\n",
    "\n",
    "        # Compute the neighborhoods\n",
    "        if self.r is not None:\n",
    "            # Radius-NN search with torch_points_kernel\n",
    "            sampler = RadiusNeighbourFinder(\n",
    "                self.r, self.num_neighbors, conv_type='DENSE')\n",
    "            neighbors = sampler.find_neighbours(\n",
    "                xyz_search.unsqueeze(0), xyz_query.unsqueeze(0))[0]\n",
    "        elif self.use_faiss:\n",
    "            # K-NN search with FAISS\n",
    "            nn_finder = FAISSGPUKNNNeighbourFinder(\n",
    "                self.num_neighbors, ncells=self.ncells, nprobes=self.nprobes)\n",
    "            neighbors = nn_finder(xyz_search, xyz_query, None, None)\n",
    "        else:\n",
    "            # K-NN search with KeOps. If the number of points is greater\n",
    "            # than 16 millions, KeOps requires double precision.\n",
    "            xyz_query = xyz_query.contiguous()\n",
    "            xyz_search = xyz_search.contiguous()\n",
    "            if xyz_search.shape[0] > 1.6e7:\n",
    "                xyz_query_keops = LazyTensor(xyz_query[:, None, :].double())\n",
    "                xyz_search_keops = LazyTensor(xyz_search[None, :, :].double())\n",
    "            else:\n",
    "                xyz_query_keops = LazyTensor(xyz_query[:, None, :])\n",
    "                xyz_search_keops = LazyTensor(xyz_search[None, :, :])\n",
    "            d_keops = ((xyz_query_keops - xyz_search_keops) ** 2).sum(dim=2)\n",
    "            neighbors = d_keops.argKmin(self.num_neighbors, dim=1)\n",
    "            # raise NotImplementedError(\n",
    "            #     \"Fast K-NN search has not been implemented yet. Please \"\n",
    "            #     \"consider using radius search instead.\")\n",
    "\n",
    "        # Compute PCA for each neighborhood\n",
    "        # Note: this is surprisingly slow on GPU, so better run on CPU\n",
    "        eigenvalues = []\n",
    "        eigenvectors = []\n",
    "        n_chunks = math.ceil(neighbors.shape[0] / self.chunk_size)\n",
    "        for i in range(n_chunks):\n",
    "            xyz_neigh_batch = xyz_search[\n",
    "                neighbors[i * self.chunk_size: (i + 1) * self.chunk_size]]\n",
    "            eval, evec = batch_pca(xyz_neigh_batch.cpu())\n",
    "            evec = evec.transpose(2, 1).flatten(1)\n",
    "            eigenvalues.append(eval)\n",
    "            eigenvectors.append(evec)\n",
    "        eigenvalues = torch.cat(eigenvalues, dim=0)\n",
    "        eigenvectors = torch.cat(eigenvectors, dim=0)\n",
    "\n",
    "        # Save eigendecomposition results in data attributes\n",
    "        data.eigenvalues = eigenvalues.to(input_device)\n",
    "        data.eigenvectors = eigenvectors.to(input_device)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in tq(data)]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        attr_repr = ', '.join([f'{k}={v}' for k, v in self.__dict__.items()])\n",
    "        return f'{self.__class__.__name__}({attr_repr})'\n",
    "\n",
    "\n",
    "class EigenFeatures(object):\n",
    "    \"\"\"\n",
    "    Compute local geometric features based on local eigenvalues and\n",
    "    eigenvectors.\n",
    "\n",
    "    The following local geometric features are computed and saved in\n",
    "    dedicated data attributes: `normal`, `scattering`, `linearity` and\n",
    "    `planarity`. The formulation of those is inspired from\n",
    "    \"Hierarchical extraction of urban objects from mobile laser\n",
    "    scanning data\" [Yang et al. 2015]\n",
    "\n",
    "    Data is expected to carry `eigenvectors` and `eigenvectors`\n",
    "    attributes:\n",
    "    `data.eigenvalues` is a tensor\n",
    "    :math:`(\\lambda_1, \\lambda_2, \\lambda_3)` such that\n",
    "    :math:`\\lambda_1 \\leq \\lambda_2 \\leq \\lambda_3`.\n",
    "    `data.eigenvectors` is 1x9 tensor containing the eigenvectors\n",
    "    associated with `data.eigenvalues`, concatenated in the same order.\n",
    "    See `PCAComputePointwise` for generating those.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    normal: bool, optional\n",
    "        If True, the normal to the local surface will be computed.\n",
    "    linearity: bool, optional\n",
    "        If True, the local linearity will be computed.\n",
    "    planarity: bool, optional\n",
    "        If True, the local planarity will be computed.\n",
    "    scattering: bool, optional\n",
    "        If True, the local scattering will be computed.\n",
    "    temperature: float, optional\n",
    "        If set to a float value, the returned features will be run\n",
    "        through a scaled softmax with temperature being the scale. Set\n",
    "        to None by default.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normal=True, linearity=True, planarity=True,\n",
    "                 scattering=True, verticality=True, temperature=None):\n",
    "        self.normal = normal\n",
    "        self.linearity = linearity\n",
    "        self.planarity = planarity\n",
    "        self.scattering = scattering\n",
    "        self.verticality = verticality\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def _process(self, data: Data):\n",
    "        assert getattr(data, 'eigenvalues', None) is not None, \\\n",
    "            \"Data must contain an 'eigenvalues' attribute.\"\n",
    "        assert getattr(data, 'eigenvectors', None) is not None, \\\n",
    "            \"Data must contain an 'eigenvectors' attribute.\"\n",
    "\n",
    "        if self.normal:\n",
    "            # The normal is the eigenvector carried by the smallest\n",
    "            # eigenvalue\n",
    "            data.normal = data.eigenvectors[:, :3]\n",
    "\n",
    "        # Eigenvalues: 0 <= l0 <= l1 <= l2\n",
    "        # Following, [Yang et al. 2015] we use the sqrt of eigenvalues\n",
    "        v0 = data.eigenvalues[:, 0].sqrt().squeeze()\n",
    "        v1 = data.eigenvalues[:, 1].sqrt().squeeze()\n",
    "        v2 = data.eigenvalues[:, 2].sqrt().squeeze() + 1e-6\n",
    "        \n",
    "        e0 = eigenvectors[:, :, 0].abs() * eigenvalues[:, [0]]\n",
    "        e1 = eigenvectors[:, :, 1].abs() * eigenvalues[:, [1]]\n",
    "        e2 = eigenvectors[:, :, 2].abs() * eigenvalues[:, [2]]\n",
    "        u = e0 + e1 + e2\n",
    "\n",
    "        # Compute the eigen features\n",
    "        linearity = (v2 - v1) / v2\n",
    "        planarity = (v1 - v0) / v2\n",
    "        scattering = v0 / v2\n",
    "        verticality = u[:, 2] / torch.linalg.norm(u, dim=1)\n",
    "\n",
    "        # Compute the softmax version of the features, for more\n",
    "        # opinionated geometric information. As a heuristic, set\n",
    "        # temperature=5 for clouds of 30 points or more.\n",
    "        if self.temperature:\n",
    "            values = (self.temperature * torch.cat([\n",
    "                linearity.view(-1, 1),\n",
    "                planarity.view(-1, 1),\n",
    "                scattering.view(-1, 1)], dim=1)).exp()\n",
    "            values = values / values.sum(dim=1).view(-1, 1)\n",
    "            linearity = values[:, 0]\n",
    "            planarity = values[:, 1]\n",
    "            scattering = values[:, 2]\n",
    "\n",
    "        if self.linearity:\n",
    "            data.linearity = linearity\n",
    "\n",
    "        if self.planarity:\n",
    "            data.planarity = planarity\n",
    "\n",
    "        if self.scattering:\n",
    "            data.scattering = scattering\n",
    "        \n",
    "        if self.verticality:\n",
    "            data.verticality = verticality\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in data]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        attr_repr = ', '.join([f'{k}={v}' for k, v in self.__dict__.items()])\n",
    "        return f'{self.__class__.__name__}({attr_repr})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8150a7-6b3d-43b8-974b-ab231fe12e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: 54.819s\n"
     ]
    }
   ],
   "source": [
    "# On GPU\n",
    "xyz = torch.rand(10**6, 50, 3).cuda()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "batch_pca(xyz)\n",
    "torch.cuda.synchronize()\n",
    "print(f'PCA: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f2ae5b-98d2-424b-8dae-445c378a72d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: 1.981s\n"
     ]
    }
   ],
   "source": [
    "# On CPU\n",
    "xyz = torch.rand(10**6, 50, 3)\n",
    "\n",
    "start = time()\n",
    "batch_pca(xyz)\n",
    "print(f'PCA: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e7c6ab32-ad94-4e51-8213-684d2b8e2eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA CPU: 2.021s\n",
      "Geometric Features: 0.012s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.data import Data\n",
    "\n",
    "# On CPU\n",
    "xyz = torch.rand(10**6, 50, 3)\n",
    "\n",
    "start = time()\n",
    "eigenvalues, eigenvectors = batch_pca(xyz)\n",
    "print(f'PCA CPU: {time() - start:0.3f}s')\n",
    "\n",
    "# On CPU\n",
    "start = time()\n",
    "d = Data(x=xyz, eigenvalues=eigenvalues, eigenvectors=eigenvectors)\n",
    "d = EigenFeatures(normal=True, linearity=True, planarity=True, scattering=True, verticality=True, temperature=None)(d)\n",
    "print(f'Geometric Features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a579163f-8a89-4151-b286-96b8816bde1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA CPU: 2.037s\n",
      "Geometric Features: 0.137s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.data import Data\n",
    "\n",
    "# On CPU\n",
    "xyz = torch.rand(10**6, 50, 3)\n",
    "\n",
    "start = time()\n",
    "eigenvalues, eigenvectors = batch_pca(xyz)\n",
    "print(f'PCA CPU: {time() - start:0.3f}s')\n",
    "\n",
    "# On GPU\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "d = Data(x=xyz.cuda(), eigenvalues=eigenvalues.cuda(), eigenvectors=eigenvectors.cuda())\n",
    "d = EigenFeatures(normal=True, linearity=True, planarity=True, scattering=True, verticality=True, temperature=None)(d)\n",
    "torch.cuda.synchronize()\n",
    "print(f'Geometric Features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63f8a0ee-901b-470a-8d52-e963ef278925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA CPU: 5.287s\n",
      "Geometric Features: 0.093s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.data import Data\n",
    "\n",
    "# On CPU\n",
    "start = time()\n",
    "eigenvalues, eigenvectors = batch_pca(data.pos[data.neighbors])\n",
    "data.eigenvalues = eigenvalues\n",
    "data.eigenvectors = eigenvectors\n",
    "print(f'PCA CPU: {time() - start:0.3f}s')\n",
    "\n",
    "# On CPU\n",
    "start = time()\n",
    "data = EigenFeatures(normal=True, linearity=True, planarity=True, scattering=True, verticality=True, temperature=None)(data)\n",
    "print(f'Geometric Features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1eea8-cde4-4dbd-86c9-9a1c35a07efc",
   "metadata": {},
   "source": [
    "Surprisingly, torch's CPU implementation is faster both for computing PCA and geometric features is faster on CPU overall !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73d639-0171-4d78-a313-9d53b8d3f976",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "620370cd-ac41-4b09-b943-4beb7d6b4c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric features: 0.528s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.transforms import compute_pointfeatures\n",
    "\n",
    "data = data.cpu()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "start = time()\n",
    "data = compute_pointfeatures(data, pos=True, radius=5, rgb=True, linearity=True, planarity=True, scattering=True, verticality=True, normal=False, length=False, surface=False, volume=False)\n",
    "print(f'Geometric features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99aa378-67a1-4f67-8a91-15f312ac4a09",
   "metadata": {},
   "source": [
    "# Point adjacency graph computation\n",
    "This graph is based on the nearest neighbor graph computed for geometric features. However, although features may require 30-50 neighbors to produce good partition, the adjacency graph benefits from using fewer neighbors (eg 10 in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfc4ee9-1a9e-4206-8024-65e1c888a4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency graph: 0.087s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.transforms import compute_ajacency_graph\n",
    "\n",
    "k_adjacency = 10\n",
    "lambda_edge_weight = 1\n",
    "\n",
    "start = time()\n",
    "data = compute_ajacency_graph(data, k_adjacency, lambda_edge_weight)\n",
    "print(f'Adjacency graph: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad16898-7426-484e-825f-e6ecff4839ca",
   "metadata": {},
   "source": [
    "# Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f0f53c-ef68-4f14-9fa6-1124674c55ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut-pursuit initialization:\n",
      "\t1 connected component(s), 0 saturated, and at most 1 reduced edge(s).\n",
      "\telapsed time 0.0 s.\n",
      "\n",
      "Cut-pursuit iteration 1 (max. 10): \n",
      "\tSplit... 29857 new activated edge(s).\n",
      "\tCompute connected components... 191 connected component(s), 0 saturated.\n",
      "\tCompute reduced graph... 172 reduced edge(s).\n",
      "\tSolve reduced problem: \n",
      "\tMerge... 20618 deactivated edge(s).\n",
      "\trelative iterate evolution 1.80e+01 (tol. 1.00e-02)\n",
      "\t125 connected component(s), 0 saturated, and at most 42 reduced edge(s).\n",
      "\telapsed time 0.6 s.\n",
      "\n",
      "Cut-pursuit iteration 2 (max. 10): \n",
      "\tSplit... 20199 new activated edge(s).\n",
      "\tCompute connected components... 261 connected component(s), 89 saturated.\n",
      "\tCompute reduced graph... 278 reduced edge(s).\n",
      "\tSolve reduced problem: \n",
      "\tMerge... 11636 deactivated edge(s).\n",
      "\trelative iterate evolution 4.79e+00 (tol. 1.00e-02)\n",
      "\t185 connected component(s), 93 saturated, and at most 123 reduced edge(s).\n",
      "\telapsed time 1.2 s.\n",
      "\n",
      "Cut-pursuit iteration 3 (max. 10): \n",
      "\tSplit... 14238 new activated edge(s).\n",
      "\tCompute connected components... 289 connected component(s), 136 saturated.\n",
      "\tCompute reduced graph... 332 reduced edge(s).\n",
      "\tSolve reduced problem: \n",
      "\tMerge... 5264 deactivated edge(s).\n",
      "\trelative iterate evolution 3.64e+00 (tol. 1.00e-02)\n",
      "\t240 connected component(s), 142 saturated, and at most 221 reduced edge(s).\n",
      "\telapsed time 1.5 s.\n",
      "\n",
      "Cut-pursuit iteration 4 (max. 10): \n",
      "\tSplit... 13730 new activated edge(s).\n",
      "\tCompute connected components... 354 connected component(s), 166 saturated.\n",
      "\tCompute reduced graph... 438 reduced edge(s).\n",
      "\tSolve reduced problem: \n",
      "\tMerge... 252 deactivated edge(s).\n",
      "\trelative iterate evolution 7.32e-01 (tol. 1.00e-02)\n",
      "\t319 connected component(s), 175 saturated, and at most 346 reduced edge(s).\n",
      "\telapsed time 1.8 s.\n",
      "\n",
      "Cut-pursuit iteration 5 (max. 10): \n",
      "\tSplit... 18011 new activated edge(s).\n",
      "\tCompute connected components... 485 connected component(s), 212 saturated.\n",
      "\tCompute reduced graph... 662 reduced edge(s).\n",
      "\tSolve reduced problem: \n",
      "\tMerge... 255 deactivated edge(s).\n",
      "\trelative iterate evolution 7.80e-01 (tol. 1.00e-02)\n",
      "\t437 connected component(s), 226 saturated, and at most 556 reduced edge(s).\n",
      "\telapsed time 2.2 s.\n",
      "\n",
      "Cut-pursuit iteration 6 (max. 10): \n",
      "\tSplit... 23444 new activated edge(s).\n",
      "\tCompute connected components... 668 connected component(s), 291 saturated.\n",
      "\tCompute reduced graph... 998 reduced edge(s).\n",
      "\tSolve reduced problem: \n",
      "\tMerge... 313 deactivated edge(s).\n",
      "\trelative iterate evolution 7.68e-01 (tol. 1.00e-02)\n",
      "\t611 connected component(s), 310 saturated, and at most 884 reduced edge(s).\n",
      "\telapsed time 2.5 s.\n",
      "\n",
      "Cut-pursuit iteration 7 (max. 10): \n",
      "\tSplit... 36062 new activated edge(s).\n",
      "\tCompute connected components... 962 connected component(s), 388 saturated.\n",
      "\tCompute reduced graph... 1580 reduced edge(s).\n",
      "\tSolve reduced problem: \n",
      "\tMerge... 944 deactivated edge(s).\n",
      "\trelative iterate evolution 6.08e-01 (tol. 1.00e-02)\n",
      "\t886 connected component(s), 401 saturated, and at most 1447 reduced edge(s).\n",
      "\telapsed time 2.8 s.\n",
      "\n",
      "Cut-pursuit iteration 8 (max. 10): \n",
      "\tSplit... 45111 new activated edge(s).\n",
      "\tCompute connected components... 1365 connected component(s), 532 saturated.\n",
      "\tCompute reduced graph... 2448 reduced edge(s).\n",
      "\tSolve reduced problem: \n",
      "\tMerge... 3388 deactivated edge(s).\n",
      "\trelative iterate evolution 5.24e-01 (tol. 1.00e-02)\n",
      "\t1243 connected component(s), 543 saturated, and at most 2264 reduced edge(s).\n",
      "\telapsed time 3.1 s.\n",
      "\n",
      "Cut-pursuit iteration 9 (max. 10): \n",
      "\tSplit... 46693 new activated edge(s).\n",
      "\tCompute connected components... 1836 connected component(s), 740 saturated.\n",
      "\tCompute reduced graph... 3549 reduced edge(s).\n",
      "\tSolve reduced problem: \n",
      "\tMerge... 4614 deactivated edge(s).\n",
      "\trelative iterate evolution 4.21e-01 (tol. 1.00e-02)\n",
      "\t1658 connected component(s), 780 saturated, and at most 3312 reduced edge(s).\n",
      "\telapsed time 3.4 s.\n",
      "\n",
      "Cut-pursuit iteration 10 (max. 10): \n",
      "\tSplit... 39480 new activated edge(s).\n",
      "\tCompute connected components... 2250 connected component(s), 1155 saturated.\n",
      "\tCompute reduced graph... 4496 reduced edge(s).\n",
      "\tSolve reduced problem: \n",
      "\tMerge... 8486 deactivated edge(s).\n",
      "\trelative iterate evolution 2.56e-01 (tol. 1.00e-02)\n",
      "\t2012 connected component(s), 1201 saturated, and at most 4198 reduced edge(s).\n",
      "\telapsed time 3.8 s.\n",
      "\n",
      "Iteration times: [0.61 0.54 0.38 0.29 0.38 0.33 0.28 0.3  0.32 0.33]\n",
      "d=Data(x=[2012, 10], sub=Cluster(num_clusters=2012, num_points=507093, device=cpu))\n",
      "d.num_points=2012\n",
      "self[i - 1].num_super=2012\n",
      "Partition num_nodes=507093, num_edges=5070930: 3.957s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.transforms import compute_partition\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "\n",
    "# Parallel cut-pursuit\n",
    "nag = compute_partition(data, 0.5, cutoff=10, verbose=True, iterations=10)\n",
    "# nag = compute_partition(data, 0.5, cutoff=10, verbose=True, iterations=5)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print(f'Partition num_nodes={data.num_nodes}, num_edges={data.num_edges}: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f8416-adc6-49e0-9a78-3d6abd20cb16",
   "metadata": {},
   "source": [
    "# Superpoint graph computation\n",
    "In the original SPG implementation, the SP graph would be computed based on the pointwise Delaunay triangulation graph. This is super inefficient. Instead, we will compute the Delaunay triangulation on the superpoint level, which sould be much faster. However, to account for large and long-shaped superpoints, we will not work with the SP centroids only (Delaunay triangulation would not capture all adjacent SPs), but on random/farthest point samplings inside the SPs (as function of SP area/volume/number of points).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9698a2be-926d-4799-b077-be2b9e5e10e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP Graph computation: 2.617s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.transforms import compute_cluster_graph\n",
    "\n",
    "start = time()\n",
    "compute_cluster_graph(nag, high_node=32, high_edge=64, low=5)\n",
    "print(f'SP Graph computation: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c6cc4-e3ee-4e3b-a8fd-a2eea2498166",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a2d2df-6065-4b43-a395-4a2c5a0cabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "\n",
    "# temp_dir = os.path.join(DATA_ROOT, 'kitti360/shared/temp') \n",
    "# os.makedirs(temp_dir, exist_ok=True) \n",
    "\n",
    "# # torch.save(nag, os.path.join(temp_dir, 'nag.pt'))\n",
    "# torch.save(nag, os.path.join(temp_dir, 'nag_20cm.pt'))\n",
    "# # torch.save((data, data_c), os.path.join(temp_dir, 'preliminaries.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f472969-deff-44a9-add0-3294812967ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from superpoint_transformer.visualization import show\n",
    "\n",
    "# nag = torch.load(os.path.join(DATA_ROOT, 'kitti360/shared/temp', 'nag.pt'))\n",
    "nag = torch.load(os.path.join(DATA_ROOT, 'kitti360/shared/temp', 'nag_20cm.pt'))\n",
    "# data, data_c = torch.load(os.path.join(DATA_ROOT, 'kitti360/shared/temp', 'preliminaries.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1ecb684-996b-4769-afd4-378c4c1c9a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512fee00-be9d-4bbb-9761-1edb6b2ecf40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376aefd-6082-47f6-9276-eb56ba51de30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f7914-4ba4-4e99-b7b5-c1f0008180c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b27a5c-6025-4823-af81-8a21de816b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248d79e-c88d-4368-8ef3-0691d0763463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20ce487c-44fc-4a58-807e-6a20b1366d65",
   "metadata": {},
   "source": [
    "questions\n",
    "- give courses for post doc ETH ?\n",
    "- paper says Z normalized over point cloud... but code seems to just take z*2 ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spt] *",
   "language": "python",
   "name": "conda-env-spt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
