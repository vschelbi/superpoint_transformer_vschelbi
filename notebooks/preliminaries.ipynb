{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea55bdc-f415-4f5c-9563-303ed17cbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "\n",
    "# file_path = os.path.dirname(os.path.abspath(__file__)) # this is for the .py script but does not work in a notebook\n",
    "file_path = os.path.dirname(os.path.abspath(''))\n",
    "sys.path.append(file_path)\n",
    "# sys.path.append(os.path.join(file_path, \"grid-graph/python/bin\"))\n",
    "# sys.path.append(os.path.join(file_path, \"parallel-cut-pursuit/python/wrappers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60125085-189f-4172-a95b-46fb07a6f970",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a15d1e7-b767-4f28-b22d-80dfecd266aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data 1/342: 0.038s\n",
      "Number of loaded points: 3201318 (3.00M)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import glob\n",
    "from torch_geometric.data import Data\n",
    "from superpoint_transformer.datasets.kitti360 import read_kitti360_window\n",
    "from superpoint_transformer.datasets.kitti360_config import KITTI360_NUM_CLASSES\n",
    "\n",
    "# DATA_ROOT\n",
    "DATA_ROOT = '/media/drobert-admin/DATA2'\n",
    "# DATA_ROOT = '/var/data/drobert\n",
    "\n",
    "i_window = 0\n",
    "all_filepaths = sorted(glob.glob(DATA_ROOT + '/datasets/kitti360/shared/data_3d_semantics/*/static/*.ply'))\n",
    "filepath = all_filepaths[i_window]\n",
    "\n",
    "start = time()\n",
    "data = read_kitti360_window(filepath, semantic=True, instance=False, remap=True)\n",
    "print(f'Loading data {i_window+1}/{len(all_filepaths)}: {time() - start:0.3f}s')\n",
    "print(f'Number of loaded points: {data.num_nodes} ({data.num_nodes // 10**6:0.2f}M)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91920327-6541-4ef6-a9c3-503023654f3a",
   "metadata": {},
   "source": [
    "# Voxelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "197855e3-11ee-43f1-b79d-afbd1713a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxel = 0.05\n",
    "voxel = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b243bd8d-4ed2-45ec-844b-1a58d134815c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f599db-b9c5-4e93-b4d3-2568c7216dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 0.133s\n",
      "Number of sampled points: 3201318 (3.00M, 100.0%)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.pool import voxel_grid\n",
    "\n",
    "start = time()\n",
    "data_sub = voxel_grid(data.pos, size=0.1)\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data_sub.shape[0]} ({data_sub.shape[0] // 10**6:0.2f}M, {100 * data_sub.shape[0] / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c89c00-81ee-4f91-b620-13acfbdbd82a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TorchPoints3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b396ee7-b7b3-4c9f-9605-93dcf3d9ebb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from torch_geometric.nn.pool import voxel_grid\n",
    "from torch_cluster import grid_cluster\n",
    "from torch_scatter import scatter_mean, scatter_add\n",
    "from torch_geometric.nn.pool.consecutive import consecutive_cluster\n",
    "MAPPING_KEY = 'mapping_index'\n",
    "\n",
    "# Label will be the majority label in each voxel\n",
    "_INTEGER_LABEL_KEYS = [\"y\", \"instance_labels\"]\n",
    "\n",
    "def group_data(data, cluster=None, unique_pos_indices=None, mode=\"last\", skip_keys=[]):\n",
    "    \"\"\" Group data based on indices in cluster.\n",
    "    The option ``mode`` controls how data gets agregated within each cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Data\n",
    "        [description]\n",
    "    cluster : torch.Tensor\n",
    "        Tensor of the same size as the number of points in data. Each element is the cluster index of that point.\n",
    "    unique_pos_indices : torch.tensor\n",
    "        Tensor containing one index per cluster, this index will be used to select features and labels\n",
    "    mode : str\n",
    "        Option to select how the features and labels for each voxel is computed. Can be ``last`` or ``mean``.\n",
    "        ``last`` selects the last point falling in a voxel as the representent, ``mean`` takes the average.\n",
    "    skip_keys: list\n",
    "        Keys of attributes to skip in the grouping\n",
    "    \"\"\"\n",
    "\n",
    "    assert mode in [\"mean\", \"last\"]\n",
    "    if mode == \"mean\" and cluster is None:\n",
    "        raise ValueError(\"In mean mode the cluster argument needs to be specified\")\n",
    "    if mode == \"last\" and unique_pos_indices is None:\n",
    "        raise ValueError(\"In last mode the unique_pos_indices argument needs to be specified\")\n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    for key, item in data:\n",
    "        if bool(re.search(\"edge\", key)):\n",
    "            raise ValueError(\"Edges not supported. Wrong data type.\")\n",
    "        if key in skip_keys:\n",
    "            continue\n",
    "\n",
    "        if torch.is_tensor(item) and item.size(0) == num_nodes:\n",
    "            if mode == \"last\" or key == \"batch\" \\\n",
    "                    or key == SaveOriginalPosId.KEY \\\n",
    "                    or key == MAPPING_KEY:\n",
    "                data[key] = item[unique_pos_indices]\n",
    "            elif mode == \"mean\":\n",
    "                is_item_bool = item.dtype == torch.bool\n",
    "                if is_item_bool:\n",
    "                    item = item.int()\n",
    "                if key in _INTEGER_LABEL_KEYS:\n",
    "                    item_min = item.min()\n",
    "                    item = torch.nn.functional.one_hot(item - item_min)\n",
    "                    item = scatter_add(item, cluster, dim=0)\n",
    "                    data[key] = item.argmax(dim=-1) + item_min\n",
    "                else:\n",
    "                    data[key] = scatter_mean(item, cluster, dim=0)\n",
    "                if is_item_bool:\n",
    "                    data[key] = data[key].bool()\n",
    "    return data\n",
    "\n",
    "class SaveOriginalPosId:\n",
    "    \"\"\" Transform that adds the index of the point to the data object\n",
    "    This allows us to track this point from the output back to the input data object\n",
    "    \"\"\"\n",
    "\n",
    "    KEY = \"origin_id\"\n",
    "\n",
    "    def __init__(self, key=None):\n",
    "        self.KEY = key if key is not None else self.KEY\n",
    "\n",
    "    def _process(self, data):\n",
    "        if hasattr(data, self.KEY):\n",
    "            return data\n",
    "\n",
    "        setattr(data, self.KEY, torch.arange(0, data.pos.shape[0]))\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in data]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "class GridSampling3D:\n",
    "    \"\"\" Clusters points into voxels with size :attr:`size`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    size: float\n",
    "        Size of a voxel (in each dimension).\n",
    "    quantize_coords: bool\n",
    "        If True, it will convert the points into their associated sparse\n",
    "        coordinates within the grid and store the value into a new\n",
    "        `coords` attribute.\n",
    "    mode: string:\n",
    "        The mode can be either `last` or `mean`.\n",
    "        If mode is `mean`, all the points and their features within a\n",
    "        cell will be averaged. If mode is `last`, one random points per\n",
    "        cell will be selected with its associated features.\n",
    "    setattr_full_pos: bool\n",
    "        If True, the input point positions will be saved into a new\n",
    "        'full_pos' attribute. This memory-costly step may reveal\n",
    "        necessary for subsequent local feature computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, quantize_coords=False, mode=\"mean\", verbose=False,\n",
    "                 setattr_full_pos=False):\n",
    "        self._grid_size = size\n",
    "        self._quantize_coords = quantize_coords\n",
    "        self._mode = mode\n",
    "        self._setattr_full_pos = setattr_full_pos\n",
    "        if verbose:\n",
    "            log.warning(\n",
    "                \"If you need to keep track of the position of your points, use \"\n",
    "                \"SaveOriginalPosId transform before using GridSampling3D.\")\n",
    "\n",
    "            if self._mode == \"last\":\n",
    "                log.warning(\n",
    "                    \"The tensors within data will be shuffled each time this \"\n",
    "                    \"transform is applied. Be careful that if an attribute \"\n",
    "                    \"doesn't have the size of num_points, it won't be shuffled\")\n",
    "\n",
    "    def _process(self, data):\n",
    "        if self._mode == \"last\":\n",
    "            data = shuffle_data(data)\n",
    "\n",
    "        full_pos = data.pos\n",
    "        coords = torch.round((data.pos) / self._grid_size)\n",
    "        if \"batch\" not in data:\n",
    "            cluster = grid_cluster(coords, torch.ones(3, device=coords.device))\n",
    "        else:\n",
    "            cluster = voxel_grid(coords, data.batch, 1)\n",
    "        cluster, unique_pos_indices = consecutive_cluster(cluster)\n",
    "\n",
    "        data = group_data(data, cluster, unique_pos_indices, mode=self._mode)\n",
    "        if self._quantize_coords:\n",
    "            data.coords = coords[unique_pos_indices].int()\n",
    "\n",
    "        data.grid_size = torch.tensor([self._grid_size])\n",
    "\n",
    "        # Keep track of the initial full-resolution point cloud for\n",
    "        # later use. Typically needed for local features computation.\n",
    "        # However, for obvious memory-wary considerations, it is\n",
    "        # recommended to delete the 'full_pos' attribute as soon as it\n",
    "        # is no longer needed.\n",
    "        if self._setattr_full_pos:\n",
    "            data.full_pos = full_pos\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in data]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}(grid_size={}, quantize_coords={}, mode={})\".format(\n",
    "            self.__class__.__name__, self._grid_size, self._quantize_coords, self._mode\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8daf6b4d-a4cc-440a-96f4-539e51899066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 2.559s\n",
      "Number of sampled points: 2480151 (2.00M, 77.5%)\n"
     ]
    }
   ],
   "source": [
    "# CPU\n",
    "start = time()\n",
    "data_sub = GridSampling3D(size=voxel)(data.clone())\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data_sub.num_nodes} ({data_sub.num_nodes // 10**6:0.2f}M, {100 * data_sub.num_nodes / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7e2fed1-f95e-43d6-8393-5256ee74d83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 0.130s\n",
      "Number of sampled points: 2480168 (2.00M, 77.5%)\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "data_sub = GridSampling3D(size=voxel)(data.clone().cuda()).cpu()\n",
    "torch.cuda.synchronize()\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data_sub.num_nodes} ({data_sub.num_nodes // 10**6:0.2f}M, {100 * data_sub.num_nodes / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd9504-ee3f-4d55-92cc-de76f11769a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SPG C implem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f788d00b-6ddf-4f61-b708-41edc2c2669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 7.307s\n",
      "Number of sampled points: 2479935 (2.00M, 77.5%)\n"
     ]
    }
   ],
   "source": [
    "import superpoint_transformer.partition.utils.libpoint_utils as point_utils\n",
    "\n",
    "# WARNING: the pruning must know the number of classes. All labels are \n",
    "# offset to account for the -1 unlabeled points !\n",
    "start = time()\n",
    "xyz, rgb, labels, dump = point_utils.prune(data.pos.float().numpy(), voxel, (data.rgb * 255).byte().numpy(), data.y.byte().numpy() + 1, np.zeros(1, dtype='uint8'), KITTI360_NUM_CLASSES + 1, 0)\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {xyz.shape[0]} ({xyz.shape[0] // 10**6:0.2f}M, {100 * xyz.shape[0] / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff552778-a2ae-4a65-a313-ec805cc48a90",
   "metadata": {},
   "source": [
    "So it seems the C-based voxelization is not that fast. Can we somehow make it faster with more CPU cores ? Otherwise, will fallback to a custom implementation based on TP3D or PyG and keeping track of the in-voxel label distribution.\n",
    "\n",
    "And even increasing the number of CPU cores (on AI4GEO) gave the same results.\n",
    "\n",
    "The fastest is GPU-based TP3D-based computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecebd3-2791-4366-9483-efec84c96221",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9411a05f-adf9-46b6-8ae9-8aaed42a1149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.5m: 0.047s\n",
      "Number of sampled points: 96291 (0.00M, 3.0%)\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "n_in = data.num_nodes\n",
    "data = GridSampling3D(size=voxel)(data.clone().cuda()).cpu()\n",
    "torch.cuda.synchronize()\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data.num_nodes} ({data.num_nodes // 10**6:0.2f}M, {100 * data.num_nodes / n_in:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1154d-83fe-4ca7-96d7-b75aa05f2d50",
   "metadata": {},
   "source": [
    "# Neighbour search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9fe250a-43b1-4ded-b3f0-8de9b288adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.pos\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9cf13-0001-4836-9dc7-3cddb0a896aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2f1226-bc57-45af-9d0a-0d3b8ca6c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 15.029s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "start = time()\n",
    "kdt = KDTree(x.numpy(), leaf_size=30, metric='euclidean')\n",
    "neighbors = kdt.query(x.numpy(), k=k, return_distance=False)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d1cf9-5120-45d7-b542-9121a7e78e1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FAISS-GPU\n",
    "```\n",
    "conda install -c pytorch faiss-gpu cudatoolkit=10.2\n",
    "pip install faiss-gpu cudatoolkit==10.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c50e50-ed80-4b20-af5c-3420dfe0fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def find_neighbours(x, y, k=10, ncells=None, nprobes=10):\n",
    "    # if batch_x is not None or batch_y is not None:\n",
    "    #     raise NotImplementedError(\n",
    "    #         \"FAISSGPUKNNNeighbourFinder does not support batches yet\")\n",
    "\n",
    "    x = x.view(-1, 1) if x.dim() == 1 else x\n",
    "    y = y.view(-1, 1) if y.dim() == 1 else y\n",
    "    x, y = x.contiguous(), y.contiguous()\n",
    "\n",
    "    # FAISS-GPU consumes numpy arrays\n",
    "    x_np = x.cpu().numpy()\n",
    "    y_np = y.cpu().numpy()\n",
    "\n",
    "    # Initialization\n",
    "    n_fit = x_np.shape[0]\n",
    "    d = x_np.shape[1]\n",
    "    gpu = faiss.StandardGpuResources()\n",
    "\n",
    "    # Heuristics to prevent k from being too large\n",
    "    k_max = 1024\n",
    "    k = min(k, n_fit, k_max)\n",
    "\n",
    "    # Heuristic to parameterize the number of cells for FAISS index,\n",
    "    # if not provided\n",
    "    if ncells is None:\n",
    "        f1 = 3.5 * np.sqrt(n_fit)\n",
    "        f2 = 1.6 * np.sqrt(n_fit)\n",
    "        if n_fit > 2 * 10 ** 6:\n",
    "            p = 1 / (1 + np.exp(2 * 10 ** 6 - n_fit))\n",
    "        else:\n",
    "            p = 0\n",
    "        ncells = int(p * f1 + (1 - p) * f2)\n",
    "\n",
    "    # Building a GPU IVFFlat index + Flat quantizer\n",
    "    torch.cuda.empty_cache()\n",
    "    quantizer = faiss.IndexFlatL2(d)  # the quantizer index\n",
    "    index = faiss.IndexIVFFlat(quantizer, d, ncells, faiss.METRIC_L2)  # the main index\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(gpu, 0, index)  # pass index it to GPU\n",
    "    gpu_index_flat.train(x_np)  # fit the cells to the training set distribution\n",
    "    gpu_index_flat.add(x_np)\n",
    "\n",
    "    # Querying the K-NN\n",
    "    gpu_index_flat.setNumProbes(nprobes)\n",
    "    return torch.LongTensor(gpu_index_flat.search(y_np, k)[1]).to(x.device)\n",
    "\n",
    "start = time()\n",
    "out = find_neighbours(x, x, k=k, ncells=None, nprobes=10)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64251d5-1bc9-48b2-996a-8e4f16af9ec4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### PyKeOps\n",
    "```\n",
    "pip install pykeops\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96a27be1-ae38-47c3-bd86-8a9c0847f3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 39.054s\n"
     ]
    }
   ],
   "source": [
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "start = time()\n",
    "# K-NN search with KeOps. If the number of points is greater\n",
    "# than 16 millions, KeOps requires double precision.\n",
    "xyz_query = x.contiguous()\n",
    "xyz_search = x.contiguous()\n",
    "if xyz_search.shape[0] > 1.6e7:\n",
    "    xyz_query_keops = LazyTensor(xyz_query[:, None, :].double())\n",
    "    xyz_search_keops = LazyTensor(xyz_search[None, :, :].double())\n",
    "else:\n",
    "    xyz_query_keops = LazyTensor(xyz_query[:, None, :])\n",
    "    xyz_search_keops = LazyTensor(xyz_search[None, :, :])\n",
    "d_keops = ((xyz_query_keops - xyz_search_keops) ** 2).sum(dim=2)\n",
    "neighbors = d_keops.argKmin(k, dim=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f2b55-d631-4462-a002-19ef72890144",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FLANN\n",
    "```\n",
    "conda install -c conda-forge pyflann -y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68542da-735f-4ec5-9d05-1eb978297738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Cannot load dynamic library. Did you compile FLANN?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyflann\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m flann \u001b[38;5;241m=\u001b[39m pyflann\u001b[38;5;241m.\u001b[39mFLANN()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/__init__.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load, save\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/index.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2010  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2010  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbindings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflann_ctypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_rn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/bindings/__init__.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#from pyflann_parameters import parameter_list, algorithm_names\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#from pyflann_parameters import centers_init_names, log_level_names\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflann_ctypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/bindings/flann_ctypes.py:173\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m flannlib \u001b[38;5;241m=\u001b[39m load_flann_library()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flannlib \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot load dynamic library. Did you compile FLANN?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFlannLib\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Cannot load dynamic library. Did you compile FLANN?"
     ]
    }
   ],
   "source": [
    "import pyflann\n",
    "\n",
    "start = time()\n",
    "flann = pyflann.FLANN()\n",
    "result, dists = flann.nn(x.numpy(), x.numpy(), k, algorithm=\"kmeans\", branching=32, iterations=7, checks=16)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0697c4b-15ae-42c8-b174-1c4135a3b83c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36414778-4228-4107-841c-c1e381684775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.466s\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import knn\n",
    "\n",
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23c967fe-b273-4717-b5be-ee20ac3b50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.215s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=2)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8abf861-b022-4f2e-aef0-3b1cd09326d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.076s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=4)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df017adf-57eb-4275-9a6d-8cfff59742af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.438s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=8)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d3b44-456c-407c-b5d5-abc2cf4fc534",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cuda = x.cuda()\n",
    "start = time()\n",
    "out = knn(x_cuda, x_cuda, k, batch_x=None, batch_y=None, num_workers=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')\n",
    "del x_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c7676-bc84-4464-a12c-ee992985c660",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GriSPy\n",
    "```\n",
    "pip install grispy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea9ee609-fd8d-4944-a52b-d8052fc98f97",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m grid \u001b[38;5;241m=\u001b[39m gsp\u001b[38;5;241m.\u001b[39mGriSPy(x\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m----> 5\u001b[0m dist, ind \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnearest_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeighbor search: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime() \u001b[38;5;241m-\u001b[39m start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:1206\u001b[0m, in \u001b[0;36mGriSPy.nearest_neighbors\u001b[0;34m(self, centres, n, kind)\u001b[0m\n\u001b[1;32m   1204\u001b[0m neighbors_distances \u001b[38;5;241m=\u001b[39m [EMPTY_ARRAY\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_centres)]\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(n_found):\n\u001b[0;32m-> 1206\u001b[0m     ndis_tmp, nidx_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell_neighbors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcentres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower_distance_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper_distance_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i_tmp, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(centres_lookup_ind[\u001b[38;5;241m~\u001b[39mn_found]):\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(nidx_tmp[i_tmp]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(neighbors_indices[i]):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:1089\u001b[0m, in \u001b[0;36mGriSPy.shell_neighbors\u001b[0;34m(self, centres, distance_lower_bound, distance_upper_bound, sorted, kind)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     vlds\u001b[38;5;241m.\u001b[39mvalidate_equalsize(centres, distance_upper_bound)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Get neighbors\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m neighbor_cells \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_neighbor_cells\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentres\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1096\u001b[0m neighbors_distances, neighbors_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_neighbor_distance(\n\u001b[1;32m   1097\u001b[0m     centres, neighbor_cells\n\u001b[1;32m   1098\u001b[0m )\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;66;03m# We need to generate mirror centres for periodic boundaries...\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:798\u001b[0m, in \u001b[0;36mGriSPy._get_neighbor_cells\u001b[0;34m(self, centres, distance_upper_bound, distance_lower_bound, shell_flag)\u001b[0m\n\u001b[1;32m    792\u001b[0m k_grids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    793\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(k_cell_min[i, k], k_cell_max[i, k] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m    795\u001b[0m ]\n\u001b[1;32m    796\u001b[0m k_grids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(\u001b[38;5;241m*\u001b[39mk_grids)\n\u001b[1;32m    797\u001b[0m neighbor_cells \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 798\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_grids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    799\u001b[0m ]\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Calculo la distancia de cada centro i a sus celdas vecinas,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# luego descarto las celdas que no toca el circulo definido por\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# la distancia\u001b[39;00m\n\u001b[1;32m    804\u001b[0m cells_physical \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_bins[neighbor_cells[i][:, k], k] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m cell_size[k]\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m    807\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import grispy as gsp\n",
    "\n",
    "start = time()\n",
    "grid = gsp.GriSPy(x.numpy())\n",
    "dist, ind = grid.nearest_neighbors(x.numpy(), n=k)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21847b0a-38e3-4a40-8e5c-fe8c889a5912",
   "metadata": {
    "tags": []
   },
   "source": [
    "### FRNN = Final\n",
    "https://github.com/lxxue/FRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39da9baf-6bd9-4b97-b7dc-56a2542595f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 0.346s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.partition.FRNN import frnn\n",
    "import torch\n",
    "\n",
    "radius = 1\n",
    "k_min = 5\n",
    "k_feat = 50\n",
    "k_adjacency = 10\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "\n",
    "# KNN on GPU. frnn_grid_points(x_QUERY, x_TARGET, ...) syntax\n",
    "distances, neighbors, _, _ = frnn.frnn_grid_points(data.pos.view(1, -1, 3).cuda(), data.pos.view(1, -1, 3).cuda(), None, None, k_feat + 1, radius, grid=None, return_nn=False, return_sorted=True)\n",
    "\n",
    "# Remove each point from its own neighborhood\n",
    "neighbors = neighbors[0][:, 1:].cpu()\n",
    "distances = distances[0][:, 1:].cpu()\n",
    "\n",
    "# Get the number of found neighbors for each point\n",
    "n_found_nn = (neighbors != -1).sum(dim=1)\n",
    "\n",
    "# Identify points which have less than k_min neighbors within R. Those\n",
    "# will be given 0-features\n",
    "idx_isolated = torch.where(n_found_nn < k_min)[0]\n",
    "\n",
    "# Identify points which have more than k_min and less than k neighbors\n",
    "# within R. For those, we oversample the neighbors to reach k_feat\n",
    "idx_partial = torch.where((k_min <= n_found_nn) & (n_found_nn < k_feat))[0]\n",
    "neighbors_partial = neighbors[idx_partial]\n",
    "distances_partial = distances[idx_partial]\n",
    "\n",
    "# Since the neighbors are sorted by increasing distance, the missing \n",
    "# neighbors will always be the last ones. This helps finding their \n",
    "# number and position, for oversampling.\n",
    "\n",
    "# For each missing neighbor, compute the size of the discrete set to \n",
    "# oversample from.\n",
    "n_valid = torch.repeat_interleave(n_found_nn[idx_partial], k_feat - n_found_nn[idx_partial])\n",
    "\n",
    "# Compute the oversampling row indices.\n",
    "idx_x_sampling = torch.repeat_interleave(\n",
    "    torch.arange(neighbors_partial.shape[0]), k_feat - n_found_nn[idx_partial])\n",
    "\n",
    "# Compute the oversampling column indices. The 0..9999 factor is a \n",
    "# security to handle the case where torch.rand is to close to 1.0, which\n",
    "# would yield incorrect sampling coordinates that would in result in \n",
    "# sampling '-1' indices (ie all we are trying to avoid here)\n",
    "idx_y_sampling = (n_valid * torch.rand(n_valid.shape[0]) * 0.9999).floor().long()\n",
    "\n",
    "# Apply the oversampling\n",
    "idx_missing = torch.where(neighbors_partial == -1)\n",
    "neighbors_partial[idx_missing] = neighbors_partial[idx_x_sampling, idx_y_sampling]\n",
    "distances_partial[idx_missing] = distances_partial[idx_x_sampling, idx_y_sampling]\n",
    "\n",
    "# Restore the oversampled neighborhods with the rest\n",
    "neighbors[idx_partial] = neighbors_partial\n",
    "distances[idx_partial] = distances_partial\n",
    "\n",
    "# Store the neighbors and distances as a Data object attribute\n",
    "data.neighbors = neighbors\n",
    "data.distances = distances\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')\n",
    "\n",
    "# IMPORTANT !!!\n",
    "#   - points with no neighbors within radius -> set to 0-feature !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e75bb774-c1d7-45ea-9bc4-aaf5b82be65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1, device='cuda:0'),\n",
       " tensor(0.0010, device='cuda:0'),\n",
       " tensor(0.0025, device='cuda:0'))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.min(), (idx == -1).sum() / idx.numel() * 100, (idx.min(dim=1).values == -1).sum() / idx.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed64ab-84dc-42e9-9750-9ed7cacaab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arange_interleave(sizes):\n",
    "    \"\"\"\n",
    "    Vectorized equivalent of:\n",
    "        ```torch.cat([torch.arange(x) for x in sizes])```\n",
    "    \"\"\"\n",
    "    assert sizes.dim() == 1, 'Only supports 1D tensors'\n",
    "    assert isinstance(sizes, torch.LongTensor), 'Only supports LongTensors'\n",
    "    assert sizes.ge(0).all(), 'Only supports positive integers'\n",
    "    \n",
    "    a = torch.cat((torch.LongTensor([0]), sizes[:-1]))\n",
    "    b = torch.cumsum(a, 0).long()\n",
    "    return torch.arange(sizes.sum()) - torch.repeat_interleave(b, sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b83d5-26d9-483a-8af6-e82aa12dc219",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pytorch3D\n",
    "```\n",
    "pip install -U fvcore\n",
    "pip install -U iopath\n",
    "pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py38_cu113_pyt1110/download.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da3c782a-6ded-4fa8-99bc-97abf128b634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch3d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch3d\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m pytorch3d\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mknn_points(x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), K\u001b[38;5;241m=\u001b[39mk)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch3d'"
     ]
    }
   ],
   "source": [
    "import pytorch3d\n",
    "\n",
    "start = time()\n",
    "pytorch3d.ops.knn_points(x.view(1, -1, 3), x.view(1, -1, 3), K=k)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b6976-3574-43c2-880f-7cc0b0c6ba0d",
   "metadata": {},
   "source": [
    "So it seems FRNN on GPU is the clear winner here !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5d1c5-db97-4d56-9b4e-2a7ec809270f",
   "metadata": {},
   "source": [
    "# Geometric features computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceed2a73-2e8e-4f7d-bb9e-17284e3f07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = data.pos\n",
    "k_geof = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c92e36-c264-4b57-ba96-27f9b7deed01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SPG C implem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc5f80-84cf-4d1b-b11b-663245e11e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import superpoint_transformer.partition.utils.libpoint_utils as point_utils\n",
    "\n",
    "start = time()\n",
    "geof = point_utils.compute_geof(xyz.numpy(), neighbors.flatten().numpy().astype('uint32'), k_geof).astype('float32')\n",
    "print(f'Geometric features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ba227-7cd4-4129-b184-2a034f87416a",
   "metadata": {},
   "source": [
    "Out of memory on my local machine ? Considered doing a chunked version to manage memory but it seems torch's CPU version can handle all once and quite fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15cb47-c547-45b0-ae27-914a9bcf1812",
   "metadata": {},
   "source": [
    "### TP3D-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d29229ed-a53b-4c5b-a31e-bb7e53383ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def batch_pca(xyz):\n",
    "    \"\"\"\n",
    "    Compute the PCA of a batch of point clouds of size (*, N, M).\n",
    "    \"\"\"\n",
    "    assert 2 <= xyz.dim() <= 3\n",
    "    xyz = xyz.unsqueeze(0) if xyz.dim() == 2 else xyz\n",
    "\n",
    "    pos_centered = xyz - xyz.mean(dim=1).unsqueeze(1)\n",
    "    cov_matrix = pos_centered.transpose(1, 2).bmm(pos_centered) / pos_centered.shape[1]\n",
    "    eigenval, eigenvect = torch.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # If Nan values are computed, return equal eigenvalues and\n",
    "    # Identity eigenvectors\n",
    "    idx_nan = torch.where(torch.logical_and(\n",
    "        eigenval.isnan().any(1), eigenvect.flatten(1).isnan().any(1)))\n",
    "    eigenval[idx_nan] = torch.ones(3, dtype=eigenval.dtype, device=xyz.device)\n",
    "    eigenvect[idx_nan] = torch.eye(3, dtype=eigenvect.dtype, device=xyz.device)\n",
    "\n",
    "    # Precision errors may cause close-to-zero eigenvalues to be\n",
    "    # negative. Hard-code these to zero\n",
    "    eigenval[torch.where(eigenval < 0)] = 0\n",
    "\n",
    "    return eigenval, eigenvect\n",
    "\n",
    "\n",
    "class PCAComputePointwise(object):\n",
    "    \"\"\"\n",
    "    Compute PCA for the local neighborhood of each point in the cloud.\n",
    "\n",
    "    Input data is expected to be stored in DENSE format.\n",
    "\n",
    "    Results are saved in `eigenvalues` and `eigenvectors` attributes.\n",
    "    `data.eigenvalues` is a tensor\n",
    "    :math:`(\\lambda_1, \\lambda_2, \\lambda_3)` such that\n",
    "    :math:`\\lambda_1 \\leq \\lambda_2 \\leq \\lambda_3`.\n",
    "    `data.eigenvectors` is 1x9 tensor containing the eigenvectors\n",
    "    associated with `data.eigenvalues`, concatenated in the same order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_neighbors: int, optional\n",
    "        Controls the maximum number of neighbors on which to compute\n",
    "        PCA. If `r=None`, `num_neighbors` will be used as K for\n",
    "        K-nearest neighbor search. Otherwise, `num_neighbors` will be\n",
    "        the maximum number of neighbors used in radial neighbor search.\n",
    "    r: float, optional\n",
    "        If not `None`, neighborhoods will be computed with a\n",
    "        radius-neighbor approach. If `None`, K-nearest neighbors will\n",
    "        be used.\n",
    "    use_full_pos: bool, optional\n",
    "        If True, the neighborhood search will be carried on the point\n",
    "        positions found in the `data.full_pos`. An error will be raised\n",
    "        if data carries no such attribute. See `GridSampling3D` for\n",
    "        producing `data.full_pos`.\n",
    "        If False, the neighbor search will be computed on `data.pos`.\n",
    "    use_cuda: bool, optional\n",
    "        If True, the computation will be carried on CUDA.\n",
    "    workers: int, optional\n",
    "        If not `None`, the features computation will be distributed\n",
    "        across the provided number of workers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, num_neighbors=40, r=None, use_full_pos=False, use_cuda=False,\n",
    "            use_faiss=True, ncells=None, nprobes=10, chunk_size=1000000):\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.r = r\n",
    "        self.use_full_pos = use_full_pos\n",
    "        self.use_cuda = use_cuda and torch.cuda.is_available()\n",
    "        self.use_faiss = use_faiss and torch.cuda.is_available()\n",
    "        self.ncells = ncells\n",
    "        self.nprobes = nprobes\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def _process(self, data: Data):\n",
    "        assert getattr(data, 'pos', None) is not None, \\\n",
    "            \"Data must contain a 'pos' attribute.\"\n",
    "        assert not self.use_full_pos \\\n",
    "               or getattr(data, 'full_pos', None) is not None, \\\n",
    "            \"Data must contain a 'full_pos' attribute.\"\n",
    "\n",
    "        # Recover the query and search clouds\n",
    "        xyz_query = data.pos\n",
    "        xyz_search = data.full_pos if self.use_full_pos else data.pos\n",
    "\n",
    "        # Move computation to CUDA if required\n",
    "        input_device = xyz_query.device\n",
    "        if self.use_cuda and not xyz_query.is_cuda and not self.use_faiss:\n",
    "            xyz_query = xyz_query.cuda()\n",
    "            xyz_search = xyz_search.cuda()\n",
    "\n",
    "        # Compute the neighborhoods\n",
    "        if self.r is not None:\n",
    "            # Radius-NN search with torch_points_kernel\n",
    "            sampler = RadiusNeighbourFinder(\n",
    "                self.r, self.num_neighbors, conv_type='DENSE')\n",
    "            neighbors = sampler.find_neighbours(\n",
    "                xyz_search.unsqueeze(0), xyz_query.unsqueeze(0))[0]\n",
    "        elif self.use_faiss:\n",
    "            # K-NN search with FAISS\n",
    "            nn_finder = FAISSGPUKNNNeighbourFinder(\n",
    "                self.num_neighbors, ncells=self.ncells, nprobes=self.nprobes)\n",
    "            neighbors = nn_finder(xyz_search, xyz_query, None, None)\n",
    "        else:\n",
    "            # K-NN search with KeOps. If the number of points is greater\n",
    "            # than 16 millions, KeOps requires double precision.\n",
    "            xyz_query = xyz_query.contiguous()\n",
    "            xyz_search = xyz_search.contiguous()\n",
    "            if xyz_search.shape[0] > 1.6e7:\n",
    "                xyz_query_keops = LazyTensor(xyz_query[:, None, :].double())\n",
    "                xyz_search_keops = LazyTensor(xyz_search[None, :, :].double())\n",
    "            else:\n",
    "                xyz_query_keops = LazyTensor(xyz_query[:, None, :])\n",
    "                xyz_search_keops = LazyTensor(xyz_search[None, :, :])\n",
    "            d_keops = ((xyz_query_keops - xyz_search_keops) ** 2).sum(dim=2)\n",
    "            neighbors = d_keops.argKmin(self.num_neighbors, dim=1)\n",
    "            # raise NotImplementedError(\n",
    "            #     \"Fast K-NN search has not been implemented yet. Please \"\n",
    "            #     \"consider using radius search instead.\")\n",
    "\n",
    "        # Compute PCA for each neighborhood\n",
    "        # Note: this is surprisingly slow on GPU, so better run on CPU\n",
    "        eigenvalues = []\n",
    "        eigenvectors = []\n",
    "        n_chunks = math.ceil(neighbors.shape[0] / self.chunk_size)\n",
    "        for i in range(n_chunks):\n",
    "            xyz_neigh_batch = xyz_search[\n",
    "                neighbors[i * self.chunk_size: (i + 1) * self.chunk_size]]\n",
    "            eval, evec = batch_pca(xyz_neigh_batch.cpu())\n",
    "            evec = evec.transpose(2, 1).flatten(1)\n",
    "            eigenvalues.append(eval)\n",
    "            eigenvectors.append(evec)\n",
    "        eigenvalues = torch.cat(eigenvalues, dim=0)\n",
    "        eigenvectors = torch.cat(eigenvectors, dim=0)\n",
    "\n",
    "        # Save eigendecomposition results in data attributes\n",
    "        data.eigenvalues = eigenvalues.to(input_device)\n",
    "        data.eigenvectors = eigenvectors.to(input_device)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in tq(data)]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        attr_repr = ', '.join([f'{k}={v}' for k, v in self.__dict__.items()])\n",
    "        return f'{self.__class__.__name__}({attr_repr})'\n",
    "\n",
    "\n",
    "class EigenFeatures(object):\n",
    "    \"\"\"\n",
    "    Compute local geometric features based on local eigenvalues and\n",
    "    eigenvectors.\n",
    "\n",
    "    The following local geometric features are computed and saved in\n",
    "    dedicated data attributes: `norm`, `scattering`, `linearity` and\n",
    "    `planarity`. The formulation of those is inspired from\n",
    "    \"Hierarchical extraction of urban objects from mobile laser\n",
    "    scanning data\" [Yang et al. 2015]\n",
    "\n",
    "    Data is expected to carry `eigenvectors` and `eigenvectors`\n",
    "    attributes:\n",
    "    `data.eigenvalues` is a tensor\n",
    "    :math:`(\\lambda_1, \\lambda_2, \\lambda_3)` such that\n",
    "    :math:`\\lambda_1 \\leq \\lambda_2 \\leq \\lambda_3`.\n",
    "    `data.eigenvectors` is 1x9 tensor containing the eigenvectors\n",
    "    associated with `data.eigenvalues`, concatenated in the same order.\n",
    "    See `PCAComputePointwise` for generating those.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    norm: bool, optional\n",
    "        If True, the normal to the local surface will be computed.\n",
    "    linearity: bool, optional\n",
    "        If True, the local linearity will be computed.\n",
    "    planarity: bool, optional\n",
    "        If True, the local planarity will be computed.\n",
    "    scattering: bool, optional\n",
    "        If True, the local scattering will be computed.\n",
    "    temperature: float, optional\n",
    "        If set to a float value, the returned features will be run\n",
    "        through a scaled softmax with temperature being the scale. Set\n",
    "        to None by default.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, norm=True, linearity=True, planarity=True,\n",
    "                 scattering=True, verticality=True, temperature=None):\n",
    "        self.norm = norm\n",
    "        self.linearity = linearity\n",
    "        self.planarity = planarity\n",
    "        self.scattering = scattering\n",
    "        self.verticality = verticality\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def _process(self, data: Data):\n",
    "        assert getattr(data, 'eigenvalues', None) is not None, \\\n",
    "            \"Data must contain an 'eigenvalues' attribute.\"\n",
    "        assert getattr(data, 'eigenvectors', None) is not None, \\\n",
    "            \"Data must contain an 'eigenvectors' attribute.\"\n",
    "\n",
    "        if self.norm:\n",
    "            # The normal is the eigenvector carried by the smallest\n",
    "            # eigenvalue\n",
    "            data.norm = data.eigenvectors[:, :3]\n",
    "\n",
    "        # Eigenvalues: 0 <= l0 <= l1 <= l2\n",
    "        # Following, [Yang et al. 2015] we use the sqrt of eigenvalues\n",
    "        v0 = data.eigenvalues[:, 0].sqrt().squeeze()\n",
    "        v1 = data.eigenvalues[:, 1].sqrt().squeeze()\n",
    "        v2 = data.eigenvalues[:, 2].sqrt().squeeze() + 1e-6\n",
    "        \n",
    "        e0 = eigenvectors[:, :, 0].abs() * eigenvalues[:, [0]]\n",
    "        e1 = eigenvectors[:, :, 1].abs() * eigenvalues[:, [1]]\n",
    "        e2 = eigenvectors[:, :, 2].abs() * eigenvalues[:, [2]]\n",
    "        u = e0 + e1 + e2\n",
    "\n",
    "        # Compute the eigen features\n",
    "        linearity = (v2 - v1) / v2\n",
    "        planarity = (v1 - v0) / v2\n",
    "        scattering = v0 / v2\n",
    "        verticality = u[:, 2] / torch.linalg.norm(u, dim=1)\n",
    "\n",
    "        # Compute the softmax version of the features, for more\n",
    "        # opinionated geometric information. As a heuristic, set\n",
    "        # temperature=5 for clouds of 30 points or more.\n",
    "        if self.temperature:\n",
    "            values = (self.temperature * torch.cat([\n",
    "                linearity.view(-1, 1),\n",
    "                planarity.view(-1, 1),\n",
    "                scattering.view(-1, 1)], dim=1)).exp()\n",
    "            values = values / values.sum(dim=1).view(-1, 1)\n",
    "            linearity = values[:, 0]\n",
    "            planarity = values[:, 1]\n",
    "            scattering = values[:, 2]\n",
    "\n",
    "        if self.linearity:\n",
    "            data.linearity = linearity\n",
    "\n",
    "        if self.planarity:\n",
    "            data.planarity = planarity\n",
    "\n",
    "        if self.scattering:\n",
    "            data.scattering = scattering\n",
    "        \n",
    "        if self.verticality:\n",
    "            data.verticality = verticality\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in data]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        attr_repr = ', '.join([f'{k}={v}' for k, v in self.__dict__.items()])\n",
    "        return f'{self.__class__.__name__}({attr_repr})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8150a7-6b3d-43b8-974b-ab231fe12e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: 54.819s\n"
     ]
    }
   ],
   "source": [
    "# On GPU\n",
    "xyz = torch.rand(10**6, 50, 3).cuda()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "batch_pca(xyz)\n",
    "torch.cuda.synchronize()\n",
    "print(f'PCA: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f2ae5b-98d2-424b-8dae-445c378a72d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: 1.981s\n"
     ]
    }
   ],
   "source": [
    "# On CPU\n",
    "xyz = torch.rand(10**6, 50, 3)\n",
    "\n",
    "start = time()\n",
    "batch_pca(xyz)\n",
    "print(f'PCA: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e7c6ab32-ad94-4e51-8213-684d2b8e2eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA CPU: 2.021s\n",
      "Geometric Features: 0.012s\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# On CPU\n",
    "xyz = torch.rand(10**6, 50, 3)\n",
    "\n",
    "start = time()\n",
    "eigenvalues, eigenvectors = batch_pca(xyz)\n",
    "print(f'PCA CPU: {time() - start:0.3f}s')\n",
    "\n",
    "# On CPU\n",
    "start = time()\n",
    "d = Data(x=xyz, eigenvalues=eigenvalues, eigenvectors=eigenvectors)\n",
    "d = EigenFeatures(norm=True, linearity=True, planarity=True, scattering=True, verticality=True, temperature=None)(d)\n",
    "print(f'Geometric Features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a579163f-8a89-4151-b286-96b8816bde1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA CPU: 2.037s\n",
      "Geometric Features: 0.137s\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# On CPU\n",
    "xyz = torch.rand(10**6, 50, 3)\n",
    "\n",
    "start = time()\n",
    "eigenvalues, eigenvectors = batch_pca(xyz)\n",
    "print(f'PCA CPU: {time() - start:0.3f}s')\n",
    "\n",
    "# On GPU\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "d = Data(x=xyz.cuda(), eigenvalues=eigenvalues.cuda(), eigenvectors=eigenvectors.cuda())\n",
    "d = EigenFeatures(norm=True, linearity=True, planarity=True, scattering=True, verticality=True, temperature=None)(d)\n",
    "torch.cuda.synchronize()\n",
    "print(f'Geometric Features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1eea8-cde4-4dbd-86c9-9a1c35a07efc",
   "metadata": {},
   "source": [
    "Surprisingly, torch's CPU implementation is faster both for computing PCA and geometric features is faster on CPU overall !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73d639-0171-4d78-a313-9d53b8d3f976",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "620370cd-ac41-4b09-b943-4beb7d6b4c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA CPU: 0.408s\n",
      "Geometric Features: 0.012s\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# On CPU\n",
    "start = time()\n",
    "eigenvalues, eigenvectors = batch_pca(data.pos[neighbors])\n",
    "data.eigenvalues = eigenvalues\n",
    "data.eigenvectors = eigenvectors\n",
    "print(f'PCA CPU: {time() - start:0.3f}s')\n",
    "\n",
    "# On CPU\n",
    "start = time()\n",
    "data = EigenFeatures(norm=True, linearity=True, planarity=True, scattering=True, verticality=True, temperature=None)(data)\n",
    "print(f'Geometric Features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99aa378-67a1-4f67-8a91-15f312ac4a09",
   "metadata": {},
   "source": [
    "# Point adjacency graph computation\n",
    "This graph is based on the nearest neighbor graph computed for geometric features. However, although features may require 30-50 neighbors to produce good partition, the adjacency graph benefits from using fewer neighbors (eg 10 in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "361c45a2-f148-4cb8-a00a-6609b868a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superpoint_transformer.partition.FRNN import frnn\n",
    "import torch\n",
    "\n",
    "radius = 1\n",
    "k_min = 5\n",
    "k_feat = 50\n",
    "k_adjacency = 10\n",
    "lambda_edge_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cfc4ee9-1a9e-4206-8024-65e1c888a4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric Features: 0.010s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "# Create the adjacency graph edges. NB: this graph is directed wrt \n",
    "# Pytorch Geometric, but cut-pursuit is happy with it\n",
    "source = torch.arange(data.num_nodes).repeat_interleave(k_adjacency)\n",
    "target = data.neighbors[:, :k_adjacency].flatten()\n",
    "data.edge_index = torch.stack((source, target))\n",
    "\n",
    "# Create the edge features for the adjacency graph\n",
    "distances = data.distances[:, :k_adjacency].flatten()\n",
    "data.edge_attr = 1 / (lambda_edge_weight + distances / distances.mean())\n",
    "\n",
    "# Gather the pointwise features that will be used for the partition\n",
    "# NB: we use a heuristic to increase the importance of verticality\n",
    "data.x = torch.column_stack((data.linearity.view(-1, 1), data.planarity.view(-1, 1), data.scattering.view(-1, 1), data.verticality.view(-1, 1) * 2, data.rgb))\n",
    "\n",
    "print(f'Geometric Features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad16898-7426-484e-825f-e6ecff4839ca",
   "metadata": {},
   "source": [
    "# Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f4bcc99-e8c1-4b4d-87cf-ed3c4392a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(file_path, \"superpoint_transformer/partition/grid_graph/python/bin\"))\n",
    "sys.path.append(os.path.join(file_path, \"superpoint_transformer/partition/parallel_cut_pursuit/python/wrappers\"))\n",
    "\n",
    "from grid_graph import edge_list_to_forward_star\n",
    "from cp_kmpp_d0_dist import cp_kmpp_d0_dist\n",
    "\n",
    "def pcp(data, reg_strength, cutoff=1, parallel=True, balance=True):\n",
    "    \"\"\"Partition the graph with parallel cut pursuit.\"\"\"\n",
    "    # Recover needed tensors from Data object\n",
    "    x = np.asfortranarray(data.x.numpy().T)\n",
    "    source = data.edge_index[[0]].numpy()\n",
    "    target = data.edge_index[[0]].numpy()\n",
    "    edge_weight = data.edge_attr.numpy()\n",
    "    \n",
    "    # Convert to forward-star graph representation\n",
    "    first_edge, adj_vertices, reindex = edge_list_to_forward_star(\n",
    "        data.num_nodes, data.edge_index.T.contiguous().numpy())\n",
    "    first_edge = first_edge.astype('uint32')\n",
    "    adj_vertices = adj_vertices.astype('uint32')\n",
    "        \n",
    "    if parallel:\n",
    "        max_thread = 0\n",
    "    else:\n",
    "        max_thread = 1\n",
    "    \n",
    "#     if return_intermediate:\n",
    "#         Comp, rX, it, Obj, Time, comp_List = cp_kmpp_d0_dist(\n",
    "#             1, x, first_edge, adj_vertices,\n",
    "#             edge_weights=reg_strength * edge_weight[reindex], min_comp_weight=cutoff,\n",
    "#             cp_dif_tol=1e-2, cp_it_max=10, split_damp_ratio=0.7, verbose=False, \n",
    "#             max_num_threads=max_thread, compute_Com=True, compute_Obj=True, compute_Time=True,\n",
    "#             balance_parallel_split=balance)\n",
    "#         return comp_List, Comp, (Obj, Time, rX)\n",
    "#     else:\n",
    "#         Comp, rX, it, comp_List = cp_kmpp_d0_dist(\n",
    "#             1, x, first_edge, adj_vertices,\n",
    "#             edge_weights=reg_strength * edge_weight[reindex], min_comp_weight=cutoff,\n",
    "#             cp_dif_tol=1e-2, cp_it_max=10, split_damp_ratio=0.7, verbose=False,\n",
    "#             max_num_threads=max_thread, compute_Com=True, balance_parallel_split=balance)\n",
    "#         return comp_List, Comp, []    \n",
    "    \n",
    "    #p2sp, sp_x, sp2p, Time\n",
    "    out = cp_kmpp_d0_dist(\n",
    "            1, x, first_edge, adj_vertices,\n",
    "            edge_weights=reg_strength * edge_weight[reindex], min_comp_weight=cutoff,\n",
    "            cp_dif_tol=1e-2, cp_it_max=10, split_damp_ratio=0.7, verbose=False, \n",
    "            max_num_threads=max_thread, compute_Time=True, compute_List=True,\n",
    "            balance_parallel_split=balance)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0f53c-ef68-4f14-9fa6-1124674c55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp(data, 0.5, cutoff=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2eb898-51f8-4c50-9b3f-1bdc30dd7db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04a04a-4e3c-44a7-9b5a-e18a5a65cf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb0f6e-5245-4238-ac47-449539d7b013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20ce487c-44fc-4a58-807e-6a20b1366d65",
   "metadata": {},
   "source": [
    "questions\n",
    "- cutoff 1 ?\n",
    "- max threads 1 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec9bf5-87de-4842-9ab9-c35720104a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCP computation\n",
    "components2, in_component2, tracks_single = pcp(features, graph_nn, args.reg_strength, 10)\n",
    "# en sachant que pour graph_nn tu n'as besoin que de graph_nn[\"source\"], graph_nn[\"target\"] et graph_nn[\"edge_weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4539ab-8b42-45c4-be14-e6888b0f3339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b9abee6-8787-47b4-9163-8ebfef9aaef5",
   "metadata": {},
   "source": [
    "# Superpoint graph computation\n",
    "In the original SPG implementation, the SP graph would be computed based on the pointwise Delaunay triangulation graph. This is super inefficient. Instead, we will compute the Delaunay triangulation on the superpoint level, which sould be much faster. However, to account for large and long-shaped superpoints, we will not work with the SP centroids only (Delaunay triangulation would not capture all adjacent SPs), but on random/farthest point samplings inside the SPs (as function of SP area/volume/number of points).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e226a-f4d4-48a8-99f5-722c5ab14fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spt] *",
   "language": "python",
   "name": "conda-env-spt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
