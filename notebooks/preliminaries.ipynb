{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea55bdc-f415-4f5c-9563-303ed17cbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys, os\n",
    "\n",
    "# file_path = os.path.dirname(os.path.abspath(__file__)) # this is for the .py script but does not work in a notebook\n",
    "file_path = os.path.dirname(os.path.abspath(''))\n",
    "sys.path.append(file_path)\n",
    "# sys.path.append(os.path.join(file_path, \"grid-graph/python/bin\"))\n",
    "# sys.path.append(os.path.join(file_path, \"parallel-cut-pursuit/python/wrappers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60125085-189f-4172-a95b-46fb07a6f970",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a15d1e7-b767-4f28-b22d-80dfecd266aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data 1/342: 0.071s\n",
      "Number of loaded points: 3201318 (3.00M)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import glob\n",
    "from superpoint_transformer.data import Data\n",
    "from superpoint_transformer.datasets.kitti360 import read_kitti360_window\n",
    "from superpoint_transformer.datasets.kitti360_config import KITTI360_NUM_CLASSES\n",
    "\n",
    "# DATA_ROOT\n",
    "DATA_ROOT = '/media/drobert-admin/DATA2'\n",
    "# DATA_ROOT = '/var/data/drobert'\n",
    "\n",
    "i_window = 0\n",
    "all_filepaths = sorted(glob.glob(DATA_ROOT + '/datasets/kitti360/shared/data_3d_semantics/*/static/*.ply'))\n",
    "filepath = all_filepaths[i_window]\n",
    "\n",
    "start = time()\n",
    "data = read_kitti360_window(filepath, semantic=True, instance=False, remap=True)\n",
    "print(f'Loading data {i_window+1}/{len(all_filepaths)}: {time() - start:0.3f}s')\n",
    "print(f'Number of loaded points: {data.num_nodes} ({data.num_nodes // 10**6:0.2f}M)')\n",
    "\n",
    "# Offset labels by 1 to account for unlabelled points -> !!!!!!!!!!!!!!!! IMPORTANT !!!!!!!!!!!!!!!!\n",
    "data.y[data.y == -1] = KITTI360_NUM_CLASSES\n",
    "KITTI360_NUM_CLASSES += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91920327-6541-4ef6-a9c3-503023654f3a",
   "metadata": {},
   "source": [
    "# Voxelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b243bd8d-4ed2-45ec-844b-1a58d134815c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f599db-b9c5-4e93-b4d3-2568c7216dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 0.154s\n",
      "Number of sampled points: 3201318 (3.00M, 100.0%)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.pool import voxel_grid\n",
    "\n",
    "start = time()\n",
    "data_sub = voxel_grid(data.pos, size=0.1)\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data_sub.shape[0]} ({data_sub.shape[0] / 10**6:0.2f}M, {100 * data_sub.shape[0] / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c89c00-81ee-4f91-b620-13acfbdbd82a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TorchPoints3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62d17bdf-2ef7-495f-837f-cf22087e8615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### import torch\n",
    "import re\n",
    "from torch_geometric.nn.pool import voxel_grid\n",
    "from torch_cluster import grid_cluster\n",
    "from torch_scatter import scatter_mean, scatter_add\n",
    "from torch_geometric.nn.pool.consecutive import consecutive_cluster\n",
    "\n",
    "\n",
    "def shuffle_data(data):\n",
    "    \"\"\" Shuffle the order of nodes in Data. Only `torch.Tensor` \n",
    "    attributes of size `Data.num_nodes` are affected.  \n",
    "    \n",
    "    Warning: this modifies the input Data object in-place\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Data\n",
    "    \"\"\"\n",
    "    num_points = data.pos.shape[0]\n",
    "    shuffle_idx = torch.randperm(num_points)\n",
    "    for key in set(data.keys):\n",
    "        item = data[key]\n",
    "        if torch.is_tensor(item) and num_points == item.shape[0]:\n",
    "            data[key] = item[shuffle_idx]\n",
    "    return data\n",
    "\n",
    "\n",
    "def group_data(\n",
    "        data, cluster=None, unique_pos_indices=None, mode=\"mean\", skip_keys=[], \n",
    "        bins={}):\n",
    "    \"\"\" Group data based on indices in cluster. The option ``mode`` \n",
    "    controls how data gets aggregated within each cluster.\n",
    "    \n",
    "    Warning: this modifies the input Data object in-place\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Data\n",
    "        [description]\n",
    "    cluster : torch.Tensor\n",
    "        Tensor of the same size as the number of points in data. Each \n",
    "        element is the cluster index of that point.\n",
    "    unique_pos_indices : torch.tensor\n",
    "        Tensor containing one index per cluster, this index will be used\n",
    "        to select features and labels\n",
    "    mode : str\n",
    "        Option to select how the features and labels for each voxel is \n",
    "        computed. Can be ``last`` or ``mean``. ``last`` selects the last \n",
    "        point falling in a voxel as the representent, ``mean`` takes the\n",
    "        average.\n",
    "    skip_keys: list\n",
    "        Keys of attributes to skip in the grouping\n",
    "    bins: dict\n",
    "        Dictionary holding ``{'key': n_bins}`` where ``key`` is a Data \n",
    "        attribute for which we would like to aggregate values into an \n",
    "        histogram and ``n_bins`` accounts for the corresponding number \n",
    "        of bins. This is typically needed when we want to aggregate \n",
    "        point labels without losing the distribution, as opposed to \n",
    "        majority voting.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keys for which voxel aggregation will be based on majority voting\n",
    "    _VOTING_KEYS = [\"y\", \"instance_labels\"]\n",
    "\n",
    "    # Keys for which voxel aggregation will be based on majority voting\n",
    "    _LAST_KEYS = [\"batch\", SaveOriginalPosId.KEY]\n",
    "\n",
    "    assert mode in [\"mean\", \"last\"]\n",
    "    if mode == \"mean\" and cluster is None:\n",
    "        raise ValueError(\n",
    "            \"In mean mode the cluster argument needs to be specified\")\n",
    "    if mode == \"last\" and unique_pos_indices is None:\n",
    "        raise ValueError(\n",
    "            \"In last mode the unique_pos_indices argument needs to be specified\")\n",
    "    \n",
    "    # Save the number of nodes here because the subsequent in-place \n",
    "    # modifications will affect it\n",
    "    num_nodes = data.num_nodes\n",
    "    \n",
    "    # Aggregate Data attributes for same-cluster points\n",
    "    for key, item in data:\n",
    "        \n",
    "        # `skip_keys` are not aggregated\n",
    "        if key in skip_keys:\n",
    "            continue\n",
    "        \n",
    "        # Edges cannot be aggregated\n",
    "        if bool(re.search(\"edge\", key)):\n",
    "            raise ValueError(\"Edges not supported. Wrong data type.\")\n",
    "        \n",
    "        # Only torch.Tensor attributes of size Data.num_nodes are \n",
    "        # considered for aggregation\n",
    "        if torch.is_tensor(item) and item.size(0) == num_nodes:\n",
    "                        \n",
    "            # For 'last' mode, use unique_pos_indices to pick values from \n",
    "            # a single point within each cluster. The same behavior is \n",
    "            # expected for the _LAST_KEYS\n",
    "            if mode == \"last\" or key in _LAST_KEYS:\n",
    "                data[key] = item[unique_pos_indices]\n",
    "            \n",
    "            # For 'mean' mode, the attributes will be aggregated \n",
    "            # depending on their nature\n",
    "            elif mode == \"mean\":\n",
    "                \n",
    "                # If the attribute is a boolean, temporarily convert is \n",
    "                # to integer to facilitate aggregation \n",
    "                is_item_bool = item.dtype == torch.bool\n",
    "                if is_item_bool:\n",
    "                    item = item.int()\n",
    "                \n",
    "                # For keys requiring a voting scheme or a histogram\n",
    "                if key in _VOTING_KEYS or key in bins.keys():\n",
    "                    \n",
    "                    assert item.ge(0).all(), \"Mean aggregation only supports positive integers\"\n",
    "                    assert item.dtype in [torch.uint8, torch.int, torch.long], \"Mean aggregation only supports positive integers\"\n",
    "                                        \n",
    "                    # Initialization\n",
    "                    voting = key not in bins.keys()\n",
    "                    n_bins = item.max() if voting else bins[key]\n",
    "                    \n",
    "                    # Convert values to one-hot encoding. Values are \n",
    "                    # temporarily offset to 0 to save some memory and \n",
    "                    # compute in one-hot encoding and scatter_add\n",
    "                    offset = item.min()\n",
    "                    item = torch.nn.functional.one_hot(item - offset)\n",
    "\n",
    "                    # Count number of occurrence of each value\n",
    "                    hist = scatter_add(item, cluster, dim=0)\n",
    "                    N = hist.shape[0]\n",
    "                    device = hist.device\n",
    "                        \n",
    "                    # Prepend 0 columns to the histogram for bins \n",
    "                    # removed due to offsetting\n",
    "                    bins_before = torch.zeros(\n",
    "                        (N, offset), device=device).long()\n",
    "                    hist = torch.cat((bins_before, hist), dim=1)\n",
    "                        \n",
    "                    # Append columns to the histogram for unobserved \n",
    "                    # classes/bins\n",
    "                    bins_after = torch.zeros(\n",
    "                        (N, n_bins - hist.shape[1]), device=device).long()\n",
    "                    hist = torch.cat((hist, bins_after), dim=1)\n",
    "                    \n",
    "                    # Either save the histogram or the majority vote\n",
    "                    data[key] = hist.argmax(dim=-1) if voting else hist\n",
    "                \n",
    "                # Standard behavior, where attributes are simply \n",
    "                # averaged across the clusters\n",
    "                else:\n",
    "                    data[key] = scatter_mean(item, cluster, dim=0)\n",
    "                    \n",
    "                # Convert back to boolean if need be \n",
    "                if is_item_bool:\n",
    "                    data[key] = data[key].bool()\n",
    "                    \n",
    "    return data\n",
    "\n",
    "\n",
    "class SaveOriginalPosId:\n",
    "    \"\"\"Adds the index of the point to the Data object attributes. This \n",
    "    allows tracking this point from the output back to the input\n",
    "    data object\n",
    "    \"\"\"\n",
    "\n",
    "    KEY = \"origin_id\"\n",
    "\n",
    "    def __init__(self, key=None):\n",
    "        self.KEY = key if key is not None else self.KEY\n",
    "\n",
    "    def _process(self, data):\n",
    "        if hasattr(data, self.KEY):\n",
    "            return data\n",
    "\n",
    "        setattr(data, self.KEY, torch.arange(0, data.pos.shape[0]))\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in data]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "class GridSampling3D:\n",
    "    \"\"\" Clusters 3D points into voxels with size :attr:`size`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    size: float\n",
    "        Size of a voxel (in each dimension).\n",
    "    quantize_coords: bool\n",
    "        If True, it will convert the points into their associated sparse\n",
    "        coordinates within the grid and store the value into a new\n",
    "        `coords` attribute.\n",
    "    mode: string:\n",
    "        The mode can be either `last` or `mean`.\n",
    "        If mode is `mean`, all the points and their features within a\n",
    "        cell will be averaged. If mode is `last`, one random points per\n",
    "        cell will be selected with its associated features.\n",
    "     bins: dict\n",
    "        Dictionary holding ``{'key': n_bins}`` where ``key`` is a Data \n",
    "        attribute for which we would like to aggregate values into an \n",
    "        histogram and ``n_bins`` accounts for the corresponding number \n",
    "        of bins. This is typically needed when we want to aggregate \n",
    "        point labels without losing the distribution, as opposed to \n",
    "        majority voting.\n",
    "    inplace: bool\n",
    "        Whether the input Data object should be modified in-place\n",
    "    verbose: bool\n",
    "        Verbosity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, size, quantize_coords=False, mode=\"mean\", bins={}, \n",
    "            inplace=False, verbose=False):\n",
    "        self.grid_size = size\n",
    "        self.quantize_coords = quantize_coords\n",
    "        self.mode = mode\n",
    "        self.bins = bins\n",
    "        self.inplace = inplace\n",
    "        if verbose:\n",
    "            log.warning(\n",
    "                \"If you need to keep track of the position of your points, use \"\n",
    "                \"SaveOriginalPosId transform before using GridSampling3D.\")\n",
    "\n",
    "            if self.mode == \"last\":\n",
    "                log.warning(\n",
    "                    \"The tensors within data will be shuffled each time this \"\n",
    "                    \"transform is applied. Be careful that if an attribute \"\n",
    "                    \"doesn't have the size of num_nodes, it won't be shuffled\")\n",
    "\n",
    "    def _process(self, data_in):\n",
    "        # In-place option will modify the input Data object directly\n",
    "        data = data_in if self.inplace else data_in.clone()\n",
    "        \n",
    "        # If the aggregation mode is 'last', shuffle the point order.\n",
    "        # Note that voxelization of point attributes will be stochastic\n",
    "        if self.mode == \"last\":\n",
    "            data = shuffle_data(data)\n",
    "        \n",
    "        # Convert point coordinates to the voxel grid coordinates\n",
    "        coords = torch.round((data.pos) / self.grid_size)\n",
    "        \n",
    "        # Match each point with a voxel identifier\n",
    "        if \"batch\" not in data:\n",
    "            cluster = grid_cluster(coords, torch.ones(3, device=coords.device))\n",
    "        else:\n",
    "            cluster = voxel_grid(coords, data.batch, 1)\n",
    "            \n",
    "        # Reindex the clusters to make sure the indices used are \n",
    "        # consecutive. Basically, we do not want cluster indices to span \n",
    "        # [0, i_max] without all in-between indices to be used, because\n",
    "        # this will affect the speed and output size of torch_scatter \n",
    "        # operations \n",
    "        cluster, unique_pos_indices = consecutive_cluster(cluster)\n",
    "        \n",
    "        # Perform voxel aggregation \n",
    "        data = group_data(\n",
    "            data, cluster, unique_pos_indices, mode=self.mode, bins=self.bins)\n",
    "        \n",
    "        # Optionally convert quantize the coordinates. This is useful \n",
    "        # for sparse convolution models \n",
    "        if self.quantize_coords:\n",
    "            data.coords = coords[unique_pos_indices].int()\n",
    "        \n",
    "        # Save the grid size in the Data attributes\n",
    "        data.grid_size = torch.tensor([self.grid_size])\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in data]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}(grid_size={}, quantize_coords={}, mode={})\".format(\n",
    "            self.__class__.__name__, self.grid_size, self.quantize_coords, \n",
    "            self.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8daf6b4d-a4cc-440a-96f4-539e51899066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 2.559s\n",
      "Number of sampled points: 2480151 (2.00M, 77.5%)\n"
     ]
    }
   ],
   "source": [
    "# CPU\n",
    "start = time()\n",
    "data_sub = GridSampling3D(size=voxel)(data)\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data_sub.num_nodes} ({data_sub.num_nodes / 10**6:0.2f}M, {100 * data_sub.num_nodes / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "e7e2fed1-f95e-43d6-8393-5256ee74d83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 0.120s\n",
      "Number of sampled points: 2480168 (2.00M, 77.5%)\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "data_sub = GridSampling3D(size=voxel)(data.cuda()).cpu()\n",
    "torch.cuda.synchronize()\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data_sub.num_nodes} ({data_sub.num_nodes / 10**6:0.2f}M, {100 * data_sub.num_nodes / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd9504-ee3f-4d55-92cc-de76f11769a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SPG C implem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f788d00b-6ddf-4f61-b708-41edc2c2669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 8.355s\n",
      "Number of sampled points: 2479935 (2.00M, 77.5%)\n"
     ]
    }
   ],
   "source": [
    "import superpoint_transformer.partition.utils.libpoint_utils as point_utils\n",
    "\n",
    "# WARNING: the pruning must know the number of classes. All labels are \n",
    "# offset to account for the -1 unlabeled points !\n",
    "start = time()\n",
    "xyz, rgb, labels, dump = point_utils.prune(data.pos.float().numpy(), voxel, (data.rgb * 255).byte().numpy(), data.y.byte().numpy() + 1, np.zeros(1, dtype='uint8'), KITTI360_NUM_CLASSES + 1, 0)\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {xyz.shape[0]} ({xyz.shape[0] // 10**6:0.2f}M, {100 * xyz.shape[0] / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff552778-a2ae-4a65-a313-ec805cc48a90",
   "metadata": {},
   "source": [
    "So it seems the C-based voxelization is not that fast. Can we somehow make it faster with more CPU cores ? Otherwise, will fallback to a custom implementation based on TP3D or PyG and keeping track of the in-voxel label distribution.\n",
    "\n",
    "And even increasing the number of CPU cores (on AI4GEO) gave the same results.\n",
    "\n",
    "The fastest is GPU-based TP3D-based computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecebd3-2791-4366-9483-efec84c96221",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0f8fa5-cdce-40d1-b907-50677eb6f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data voxelization at 0.05m: 1.895s\n",
      "Number of sampled points: 2480168 (2.48M, 77.5%)\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.transforms import GridSampling3D\n",
    "\n",
    "voxel = 0.05\n",
    "# voxel = 1\n",
    "\n",
    "# GPU\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "n_in = data.num_nodes\n",
    "data = GridSampling3D(size=voxel, bins={'y': KITTI360_NUM_CLASSES})(data.cuda()).cpu()\n",
    "torch.cuda.synchronize()\n",
    "print(f'Data voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data.num_nodes} ({data.num_nodes / 10**6:0.2f}M, {100 * data.num_nodes / n_in:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1154d-83fe-4ca7-96d7-b75aa05f2d50",
   "metadata": {},
   "source": [
    "# Neighbour search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9cf13-0001-4836-9dc7-3cddb0a896aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2f1226-bc57-45af-9d0a-0d3b8ca6c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 15.029s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "start = time()\n",
    "kdt = KDTree(x.numpy(), leaf_size=30, metric='euclidean')\n",
    "neighbors = kdt.query(x.numpy(), k=k, return_distance=False)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d1cf9-5120-45d7-b542-9121a7e78e1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FAISS-GPU\n",
    "```\n",
    "conda install -c pytorch faiss-gpu cudatoolkit=10.2\n",
    "pip install faiss-gpu cudatoolkit==10.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c50e50-ed80-4b20-af5c-3420dfe0fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def find_neighbours(x, y, k=10, ncells=None, nprobes=10):\n",
    "    # if batch_x is not None or batch_y is not None:\n",
    "    #     raise NotImplementedError(\n",
    "    #         \"FAISSGPUKNNNeighbourFinder does not support batches yet\")\n",
    "\n",
    "    x = x.view(-1, 1) if x.dim() == 1 else x\n",
    "    y = y.view(-1, 1) if y.dim() == 1 else y\n",
    "    x, y = x.contiguous(), y.contiguous()\n",
    "\n",
    "    # FAISS-GPU consumes numpy arrays\n",
    "    x_np = x.cpu().numpy()\n",
    "    y_np = y.cpu().numpy()\n",
    "\n",
    "    # Initialization\n",
    "    n_fit = x_np.shape[0]\n",
    "    d = x_np.shape[1]\n",
    "    gpu = faiss.StandardGpuResources()\n",
    "\n",
    "    # Heuristics to prevent k from being too large\n",
    "    k_max = 1024\n",
    "    k = min(k, n_fit, k_max)\n",
    "\n",
    "    # Heuristic to parameterize the number of cells for FAISS index,\n",
    "    # if not provided\n",
    "    if ncells is None:\n",
    "        f1 = 3.5 * np.sqrt(n_fit)\n",
    "        f2 = 1.6 * np.sqrt(n_fit)\n",
    "        if n_fit > 2 * 10 ** 6:\n",
    "            p = 1 / (1 + np.exp(2 * 10 ** 6 - n_fit))\n",
    "        else:\n",
    "            p = 0\n",
    "        ncells = int(p * f1 + (1 - p) * f2)\n",
    "\n",
    "    # Building a GPU IVFFlat index + Flat quantizer\n",
    "    torch.cuda.empty_cache()\n",
    "    quantizer = faiss.IndexFlatL2(d)  # the quantizer index\n",
    "    index = faiss.IndexIVFFlat(quantizer, d, ncells, faiss.METRIC_L2)  # the main index\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(gpu, 0, index)  # pass index it to GPU\n",
    "    gpu_index_flat.train(x_np)  # fit the cells to the training set distribution\n",
    "    gpu_index_flat.add(x_np)\n",
    "\n",
    "    # Querying the K-NN\n",
    "    gpu_index_flat.setNumProbes(nprobes)\n",
    "    return torch.LongTensor(gpu_index_flat.search(y_np, k)[1]).to(x.device)\n",
    "\n",
    "start = time()\n",
    "out = find_neighbours(x, x, k=k, ncells=None, nprobes=10)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64251d5-1bc9-48b2-996a-8e4f16af9ec4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### PyKeOps\n",
    "```\n",
    "pip install pykeops\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96a27be1-ae38-47c3-bd86-8a9c0847f3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 39.054s\n"
     ]
    }
   ],
   "source": [
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "start = time()\n",
    "# K-NN search with KeOps. If the number of points is greater\n",
    "# than 16 millions, KeOps requires double precision.\n",
    "xyz_query = x.contiguous()\n",
    "xyz_search = x.contiguous()\n",
    "if xyz_search.shape[0] > 1.6e7:\n",
    "    xyz_query_keops = LazyTensor(xyz_query[:, None, :].double())\n",
    "    xyz_search_keops = LazyTensor(xyz_search[None, :, :].double())\n",
    "else:\n",
    "    xyz_query_keops = LazyTensor(xyz_query[:, None, :])\n",
    "    xyz_search_keops = LazyTensor(xyz_search[None, :, :])\n",
    "d_keops = ((xyz_query_keops - xyz_search_keops) ** 2).sum(dim=2)\n",
    "neighbors = d_keops.argKmin(k, dim=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f2b55-d631-4462-a002-19ef72890144",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FLANN\n",
    "```\n",
    "conda install -c conda-forge pyflann -y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68542da-735f-4ec5-9d05-1eb978297738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Cannot load dynamic library. Did you compile FLANN?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyflann\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m flann \u001b[38;5;241m=\u001b[39m pyflann\u001b[38;5;241m.\u001b[39mFLANN()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/__init__.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load, save\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/index.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2010  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2010  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbindings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflann_ctypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_rn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/bindings/__init__.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#from pyflann_parameters import parameter_list, algorithm_names\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#from pyflann_parameters import centers_init_names, log_level_names\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflann_ctypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/bindings/flann_ctypes.py:173\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m flannlib \u001b[38;5;241m=\u001b[39m load_flann_library()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flannlib \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot load dynamic library. Did you compile FLANN?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFlannLib\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Cannot load dynamic library. Did you compile FLANN?"
     ]
    }
   ],
   "source": [
    "import pyflann\n",
    "\n",
    "start = time()\n",
    "flann = pyflann.FLANN()\n",
    "result, dists = flann.nn(x.numpy(), x.numpy(), k, algorithm=\"kmeans\", branching=32, iterations=7, checks=16)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0697c4b-15ae-42c8-b174-1c4135a3b83c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36414778-4228-4107-841c-c1e381684775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.466s\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import knn\n",
    "\n",
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23c967fe-b273-4717-b5be-ee20ac3b50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.215s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=2)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8abf861-b022-4f2e-aef0-3b1cd09326d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.076s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=4)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df017adf-57eb-4275-9a6d-8cfff59742af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.438s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=8)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d3b44-456c-407c-b5d5-abc2cf4fc534",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cuda = x.cuda()\n",
    "start = time()\n",
    "out = knn(x_cuda, x_cuda, k, batch_x=None, batch_y=None, num_workers=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')\n",
    "del x_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c7676-bc84-4464-a12c-ee992985c660",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GriSPy\n",
    "```\n",
    "pip install grispy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea9ee609-fd8d-4944-a52b-d8052fc98f97",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m grid \u001b[38;5;241m=\u001b[39m gsp\u001b[38;5;241m.\u001b[39mGriSPy(x\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m----> 5\u001b[0m dist, ind \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnearest_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeighbor search: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime() \u001b[38;5;241m-\u001b[39m start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:1206\u001b[0m, in \u001b[0;36mGriSPy.nearest_neighbors\u001b[0;34m(self, centres, n, kind)\u001b[0m\n\u001b[1;32m   1204\u001b[0m neighbors_distances \u001b[38;5;241m=\u001b[39m [EMPTY_ARRAY\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_centres)]\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(n_found):\n\u001b[0;32m-> 1206\u001b[0m     ndis_tmp, nidx_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell_neighbors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcentres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower_distance_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper_distance_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i_tmp, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(centres_lookup_ind[\u001b[38;5;241m~\u001b[39mn_found]):\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(nidx_tmp[i_tmp]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(neighbors_indices[i]):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:1089\u001b[0m, in \u001b[0;36mGriSPy.shell_neighbors\u001b[0;34m(self, centres, distance_lower_bound, distance_upper_bound, sorted, kind)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     vlds\u001b[38;5;241m.\u001b[39mvalidate_equalsize(centres, distance_upper_bound)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Get neighbors\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m neighbor_cells \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_neighbor_cells\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentres\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1096\u001b[0m neighbors_distances, neighbors_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_neighbor_distance(\n\u001b[1;32m   1097\u001b[0m     centres, neighbor_cells\n\u001b[1;32m   1098\u001b[0m )\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;66;03m# We need to generate mirror centres for periodic boundaries...\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:798\u001b[0m, in \u001b[0;36mGriSPy._get_neighbor_cells\u001b[0;34m(self, centres, distance_upper_bound, distance_lower_bound, shell_flag)\u001b[0m\n\u001b[1;32m    792\u001b[0m k_grids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    793\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(k_cell_min[i, k], k_cell_max[i, k] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m    795\u001b[0m ]\n\u001b[1;32m    796\u001b[0m k_grids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(\u001b[38;5;241m*\u001b[39mk_grids)\n\u001b[1;32m    797\u001b[0m neighbor_cells \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 798\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_grids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    799\u001b[0m ]\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Calculo la distancia de cada centro i a sus celdas vecinas,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# luego descarto las celdas que no toca el circulo definido por\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# la distancia\u001b[39;00m\n\u001b[1;32m    804\u001b[0m cells_physical \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_bins[neighbor_cells[i][:, k], k] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m cell_size[k]\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m    807\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import grispy as gsp\n",
    "\n",
    "start = time()\n",
    "grid = gsp.GriSPy(x.numpy())\n",
    "dist, ind = grid.nearest_neighbors(x.numpy(), n=k)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21847b0a-38e3-4a40-8e5c-fe8c889a5912",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FRNN = Final\n",
    "https://github.com/lxxue/FRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ff6e5-54a5-46c9-9ffb-7ef616e0f34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from superpoint_transformer.partition.FRNN import frnn\n",
    "import torch\n",
    "\n",
    "\n",
    "def _search_outliers(\n",
    "        xyz_query, xyz_search, k_min, r_max=1, recursive=False, q_in_s=False):\n",
    "    \"\"\"\n",
    "    Optionally recursive outlier search. The `xyz_query` and `xyz_search`\n",
    "    Search for points with less than `k_min` neighbors within a radius \n",
    "    of `r_max`. \n",
    "    \n",
    "    Since removing outliers may cause some points to become outliers \n",
    "    themselves, this problem can be tackled with the `recursive` option. \n",
    "    Note that this recursive search holds no garantee of reasonable \n",
    "    convergence as one could design a point cloud for given `k_min` and \n",
    "    `r_max` whose points would all recursively end up as outliers.  \n",
    "    \"\"\"\n",
    "    # Data initialization\n",
    "    xyz_query = xyz_query.view(1, -1, 3)\n",
    "    xyz_search = xyz_search.view(1, -1, 3)\n",
    "    device = xyz_query.device\n",
    "    \n",
    "    # KNN on GPU. Actual neighbor search now\n",
    "    neighbors = frnn.frnn_grid_points(\n",
    "        xyz_query, xyz_search, K=k_min + q_in_s, r=r_max)[1]\n",
    "    \n",
    "    # If the Query points are included in the Search points, remove each\n",
    "    # point from its own neighborhood\n",
    "    if q_in_s:\n",
    "        neighbors = neighbors[0][:, 1:]\n",
    "    \n",
    "    # Get the number of found neighbors for each point. Indeed, \n",
    "    # depending on the cloud properties and the chosen K and radius, \n",
    "    # some points may receive \"-1\" neighbors\n",
    "    n_found_nn = (neighbors != -1).sum(dim=1)\n",
    "\n",
    "    # Identify points which have less than k_min neighbor. Those are \n",
    "    # treated as outliers\n",
    "    mask_outliers = n_found_nn < k_min\n",
    "    idx_outliers = torch.where(mask_outliers)[0]\n",
    "    idx_inliers = torch.where(~mask_outliers)[0]\n",
    "    \n",
    "    # Exit here if not recursively searching for outliers \n",
    "    if not recursive:\n",
    "        return idx_outliers, idx_inliers\n",
    "    \n",
    "    # Identify the points affected by the removal of the outliers. Those\n",
    "    # inliers are potential outliers\n",
    "    idx_potential = torch.where(\n",
    "        torch.isin(neighbors[idx_inliers], idx_outliers).any(dim=1))[0]\n",
    "        \n",
    "    # Exit here if there are no potential new outliers among the inliers\n",
    "    if idx_potential.shape[0] == 0:\n",
    "        return idx_outliers, idx_inliers\n",
    "    \n",
    "    # Recursviely search actual outliers among the potential\n",
    "    xyz_query_sub = xyz_query[0, idx_inliers[idx_potential]]\n",
    "    xyz_search_sub = xyz_search[0, idx_inliers]\n",
    "    idx_outliers_sub, idx_inliers_sub = _search_outliers(\n",
    "        xyz_query_sub, xyz_search_sub, k_min, r_max=r_max, recursive=True, \n",
    "        q_in_s=True)\n",
    "    \n",
    "    # Update the outliers mask\n",
    "    mask_outliers[idx_inliers[idx_potential][idx_outliers_sub]] = True\n",
    "    idx_outliers = torch.where(mask_outliers)[0]\n",
    "    idx_inliers = torch.where(~mask_outliers)[0]\n",
    "    \n",
    "    return idx_outliers, idx_inliers\n",
    "\n",
    "\n",
    "def search_outliers(data, k_min, r_max=1, recursive=False):\n",
    "    \"\"\"\n",
    "    Search for points with less than `k_min` neighbors within a radius \n",
    "    of `r_max`. \n",
    "    \n",
    "    Since removing outliers may cause some points to become outliers \n",
    "    themselves, this problem can be tackled with the `recursive` option. \n",
    "    Note that this recursive search holds no garantee of reasonable \n",
    "    convergence as one could design a point cloud for given `k_min` and \n",
    "    `r_max` whose points would all recursively end up as outliers.  \n",
    "    \"\"\"       \n",
    "    # Actual outlier search, optionally recursive\n",
    "    idx_outliers, idx_inliers = _search_outliers(\n",
    "        data.pos, data.pos, k_min, r_max=r_max, recursive=recursive, \n",
    "        q_in_s=True)\n",
    "    \n",
    "    # Create a Data object for the inliers and outliers    \n",
    "    # Save the index for these isolated points in the Data object. This\n",
    "    # will help properly handle neighborhoods, features and adjacency  \n",
    "    # graph for those specific points. \n",
    "    # NB: it is important this attribute follows the \"*index\" naming \n",
    "    # convention, see:\n",
    "    # https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
    "    data_in = Data()\n",
    "    data_out = Data(outliers_index=idx_outliers)\n",
    "    for key, item in data:\n",
    "        if torch.is_tensor(item) and item.size(0) == data.num_nodes:\n",
    "            data_in[key] = data[key][idx_inliers]\n",
    "            data_out[key] = data[key][idx_outliers]\n",
    "\n",
    "    return data_in, data_out\n",
    "\n",
    "\n",
    "def oversample_partial_neighborhoods(neighbors, distances, k):\n",
    "    \"\"\"\n",
    "    Oversample partial neighborhoods with less than k points. Missing \n",
    "    neighbors are indicated by the \"-1\" index.\n",
    "    \n",
    "    Remarks\n",
    "      - Neighbors and distances are assumed to be sorted in order of \n",
    "      increasing distance\n",
    "      - All neighbors are assumed to have at least one valid neighbor. \n",
    "      See `search_outliers` to remove points with not enough neighbors \n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    assert neighbors.dim() == distances.dim() == 2\n",
    "    device = neighbors.device\n",
    "    \n",
    "    # Get the number of found neighbors for each point. Indeed, \n",
    "    # depending on the cloud properties and the chosen K and radius, \n",
    "    # some points may receive `-1` neighbors\n",
    "    n_found_nn = (neighbors != -1).sum(dim=1)\n",
    "        \n",
    "    # Identify points which have more than k_min and less than k \n",
    "    # neighbors within R. For those, we oversample the neighbors to \n",
    "    # reach k\n",
    "    idx_partial = torch.where(n_found_nn < k)[0]\n",
    "    neighbors_partial = neighbors[idx_partial]\n",
    "    distances_partial = distances[idx_partial]\n",
    "\n",
    "    # Since the neighbors are sorted by increasing distance, the missing \n",
    "    # neighbors will always be the last ones. This helps finding their \n",
    "    # number and position, for oversampling.\n",
    "\n",
    "    #*******************************************************************\n",
    "    # The above statement is actually INCORRECT because the outlier \n",
    "    # removal may produce \"-1\" neighbors at unexpected positions. So \n",
    "    # either we manage to treat this in a clean vectorized way, or we \n",
    "    # fall back to the 2-searches solution...\n",
    "    # Honestly, this feels like it is getting out of hand, let's keep \n",
    "    # things simple, since we are not going to save so much computation \n",
    "    # time with KNN wrt the partition.\n",
    "    #*******************************************************************\n",
    "    \n",
    "    # For each missing neighbor, compute the size of the discrete set to \n",
    "    # oversample from.\n",
    "    n_valid = n_found_nn[idx_partial].repeat_interleave(\n",
    "        k - n_found_nn[idx_partial])\n",
    "\n",
    "    # Compute the oversampling row indices.\n",
    "    idx_x_sampling = torch.arange(neighbors_partial.shape[0], device=device\n",
    "        ).repeat_interleave(k - n_found_nn[idx_partial])\n",
    "    \n",
    "    # Compute the oversampling column indices. The 0.9999 factor is a \n",
    "    # security to handle the case where torch.rand is to close to 1.0, \n",
    "    # which would yield incorrect sampling coordinates that would in \n",
    "    # result in sampling '-1' indices (ie all we try to avoid here)\n",
    "    idx_y_sampling = (n_valid * torch.rand(\n",
    "        n_valid.shape[0], device=device) * 0.9999).floor().long()\n",
    "\n",
    "    # Apply the oversampling\n",
    "    idx_missing = torch.where(neighbors_partial == -1)\n",
    "    neighbors_partial[idx_missing] = neighbors_partial[\n",
    "        idx_x_sampling, idx_y_sampling]\n",
    "    distances_partial[idx_missing] = distances_partial[\n",
    "        idx_x_sampling, idx_y_sampling]\n",
    "\n",
    "    # Restore the oversampled neighborhods with the rest\n",
    "    neighbors[idx_partial] = neighbors_partial\n",
    "    distances[idx_partial] = distances_partial\n",
    "    \n",
    "    return neighbors, distances\n",
    "    \n",
    "\n",
    "def search_neighbors(data, k, r_max=1):\n",
    "    # Data initialization\n",
    "    xyz_query = data.pos.view(1, -1, 3)\n",
    "    xyz_search = data.pos.view(1, -1, 3)\n",
    "    \n",
    "#     #--------------------------------\n",
    "#     # KNN on GPU. Search for outliers first\n",
    "#     _, neighbors, _, _ = frnn.frnn_grid_points(\n",
    "#         xyz_query, xyz_search, K=k_min + 1, r=r_max)\n",
    "    \n",
    "#     # Remove each point from its own neighborhood\n",
    "#     neighbors = neighbors[0][:, 1:]\n",
    "    \n",
    "#     # Get the number of found neighbors for each point. Indeed, \n",
    "#     # depending on the cloud properties and the chosen K and radius, \n",
    "#     # some points may receive `-1` neighbors\n",
    "#     n_found_nn = (neighbors != -1).sum(dim=1)\n",
    "\n",
    "#     # Identify points which have less than k_min neighbors within R. \n",
    "#     # Those are treated as outliers and will be discarded\n",
    "#     idx_isolated = torch.where(n_found_nn < k_min)[0]\n",
    "    \n",
    "#     # Save the outliers in a separate Data object\n",
    "#     outliers = Data(\n",
    "#         pos=data.pos[idx_isolated], rgb=data.rgb[idx_isolated], \n",
    "#         y=data.y[idx_isolated], idx_isolated=idx_isolated)\n",
    "    \n",
    "#     # KNN on GPU. Search for outliers first\n",
    "#     _, neighbors, _, _ = frnn.frnn_grid_points(\n",
    "#         xyz_query, xyz_search, K=k_min + 1, r=r_max)\n",
    "#     #--------------------------------\n",
    "    \n",
    "    # KNN on GPU. Actual neighbor search now\n",
    "    distances, neighbors, _, _ = frnn.frnn_grid_points(\n",
    "        xyz_query, xyz_search, K=k + 1, r=r_max)\n",
    "    \n",
    "    # Remove each point from its own neighborhood\n",
    "    neighbors = neighbors[0][:, 1:]\n",
    "    distances = distances[0][:, 1:]\n",
    "\n",
    "    # Oversample the neighborhoods where less than k points were found\n",
    "    neighbors, distances = oversample_partial_neighborhoods(\n",
    "        neighbors, distances, k)\n",
    "    \n",
    "    # Store the neighbors and distances as a Data object attribute\n",
    "    data.neighbors = neighbors.cpu()\n",
    "    data.distances = distances.cpu()\n",
    "    \n",
    "    # Save the index for these isolated points in the Data object. This\n",
    "    # will help properly handle neighborhoods, features and adjacency  \n",
    "    # graph for those specific points. \n",
    "    # NB: it is important this attribute follows the \"*index\" naming \n",
    "    # convention, see:\n",
    "    # https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
    "    # data.isolated_index = idx_isolated.cpu()\n",
    "    \n",
    "    return data\n",
    "\n",
    "# IMPORTANT !!!\n",
    "#   - points with no neighbors within radius -> set to 0-feature !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0a73b-a530-4e0e-9479-d4ac6d92411f",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17541b2-7dd3-4a7d-8adf-56c84474996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers search: 0.584s\n",
      "Neighbor search: 1.701s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.transforms import search_outliers, search_neighbors\n",
    "\n",
    "radius = 1\n",
    "# radius = 10\n",
    "k_min = 5\n",
    "k_feat = 30\n",
    "k_adjacency = 10\n",
    "\n",
    "data = data.cuda()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "data, data_outliers = search_outliers(data, k_min, r_max=radius, recursive=True)\n",
    "data_outliers = data_outliers.cpu()\n",
    "torch.cuda.synchronize()\n",
    "print(f'Outliers search: {time() - start:0.3f}s')\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "data = search_neighbors(data, k_feat, r_max=radius)\n",
    "# Make sure all points have k neighbors (no \"-1\" missing neighbors)\n",
    "assert (data.neighbors != -1).all(), \"Some points have incomplete neighborhoods, make sure to remove the outliers to avoid this issue.\"\n",
    "torch.cuda.synchronize()\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b83d5-26d9-483a-8af6-e82aa12dc219",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pytorch3D\n",
    "```\n",
    "pip install -U fvcore\n",
    "pip install -U iopath\n",
    "pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py38_cu113_pyt1110/download.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da3c782a-6ded-4fa8-99bc-97abf128b634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch3d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch3d\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m pytorch3d\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mknn_points(x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), K\u001b[38;5;241m=\u001b[39mk)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch3d'"
     ]
    }
   ],
   "source": [
    "import pytorch3d\n",
    "\n",
    "start = time()\n",
    "pytorch3d.ops.knn_points(x.view(1, -1, 3), x.view(1, -1, 3), K=k)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b6976-3574-43c2-880f-7cc0b0c6ba0d",
   "metadata": {},
   "source": [
    "So it seems FRNN on GPU is the clear winner here !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8fc2a8-cc69-40a1-9583-47e8ea497f54",
   "metadata": {},
   "source": [
    "# !!! ___ CAREFUL WITH CPU-CUDA MOVES ___ !!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5d1c5-db97-4d56-9b4e-2a7ec809270f",
   "metadata": {},
   "source": [
    "# Geometric features computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c92e36-c264-4b57-ba96-27f9b7deed01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SPG C implem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ba8949-851e-4c0b-b307-bdc029562c01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric features: 1.909s\n"
     ]
    }
   ],
   "source": [
    "import superpoint_transformer.partition.utils.libpoint_utils as point_utils\n",
    "\n",
    "data = data.cpu()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "geof = point_utils.compute_geometric_features(\n",
    "    data.pos.numpy(), data.neighbors.flatten().numpy().astype('uint32'), \n",
    "    np.arange(data.pos.shape[0] + 1).astype('uint32') * k_feat, False).astype('float32')  # IMPORTANT CAREFUL WITH UINT32 = 4G MAX\n",
    "print(f'Geometric features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ba227-7cd4-4129-b184-2a034f87416a",
   "metadata": {},
   "source": [
    "This is the fasest way of computing the geometric features. Surprisingly, the CPU implementation is faster than the TP3D-based GPU one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794dc55-e621-4087-b82e-259278253ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import superpoint_transformer.partition.utils.libpoint_utils as point_utils\n",
    "\n",
    "def compute_pointfeatures(\n",
    "        data, pos=True, radius=5, rgb=True, linearity=True, planarity=True,\n",
    "        scattering=True, verticality=True, normal=True, length=False, \n",
    "        surface=False, volume=False):\n",
    "    \"\"\" Compute the pointwise features that will be used for the \n",
    "    partition.\n",
    "    \n",
    "    All local geometric features assume the input ``Data`` has a \n",
    "    ``neighbors`` attribute, holding a ``(num_nodes, k)`` tensor of \n",
    "    indices. All k neighbors will be used for local geometric features \n",
    "    computation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pos: bool\n",
    "        Use point position.\n",
    "    radius: bool\n",
    "        Radius used to scale the point position features, to mitigate \n",
    "        the maximum superpoint size.\n",
    "    rgb: bool\n",
    "        Use rgb color. Assumes Data.rgb holds either [0, 1] flaots or \n",
    "        [0, 255] integers\n",
    "    linearity: bool\n",
    "        Use local linearity. Assumes ``Data.neighbors``.\n",
    "    lanarity: bool\n",
    "        Use local lanarity. Assumes ``Data.neighbors``.\n",
    "    scattering: bool\n",
    "        Use local scattering. Assumes ``Data.neighbors``.\n",
    "    verticality: bool\n",
    "        Use local verticality. Assumes ``Data.neighbors``.\n",
    "    normal: bool\n",
    "        Use local normal. Assumes ``Data.neighbors``.\n",
    "    length: bool\n",
    "        Use local length. Assumes ``Data.neighbors``.\n",
    "    surface: bool\n",
    "        Use local surface. Assumes ``Data.neighbors``.\n",
    "    volume: bool\n",
    "        Use local volume. Assumes ``Data.neighbors``.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Add xyz normalized. The scaling factor drives the maximum cluster\n",
    "    # size the partition may produce\n",
    "    if 'pos' in data.keys:\n",
    "        features.append(data.pos / radius)\n",
    "    \n",
    "    # Add rgb to the features. If colors are stored in int, we assume \n",
    "    # they are encoded in  [0, 255] and normalize them. Otherwise, we \n",
    "    # assume they have already been [0, 1] normalized\n",
    "    if rgb:\n",
    "        f = data.rgb\n",
    "        if f.type in [torch.uint8, torch.int, torch.long]:\n",
    "            f = f.float() / 255\n",
    "        features.append(f)\n",
    "    \n",
    "    # Add local geometric features\n",
    "    if any((linearity, planarity, scattering, verticality, normal)):\n",
    "        \n",
    "        # Prepare data for numpy boost interface\n",
    "        xyz = data.pos.cpu().numpy()\n",
    "        nn = data.neighbors.flatten().cpu().numpy().astype('uint32')  # !!!! IMPORTANT CAREFUL WITH UINT32 = 4 BILLION points MAXIMUM !!!!\n",
    "        k = data.neighbors.shape[1]\n",
    "        nn_ptr = np.arange(xyz.shape[0] + 1).astype('uint32') * k  # !!!! IMPORTANT CAREFUL WITH UINT32 = 4 BILLION points MAXIMUM !!!!\n",
    "        \n",
    "        # C++ geometric features computation on CPU\n",
    "        f = point_utils.compute_geometric_features(xyz, nn, nn_ptr, False)\n",
    "        f = torch.from_numpy(f.astype('float32'))\n",
    "        \n",
    "        # Heuristic to increase the importance of verticality\n",
    "        f[:, 3] *= 2\n",
    "        \n",
    "        # Select only required features\n",
    "        mask = (\n",
    "            [linearity, planarity, scattering, verticality] \n",
    "            + [normal] * 3 \n",
    "            + [length, surface, volume])\n",
    "        features.append(f[:, mask].to(data.pos.device))\n",
    "        \n",
    "    # Save all features in the Data.x attribute\n",
    "    data.x = torch.cat(features, dim=1).to(data.pos.device)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15cb47-c547-45b0-ae27-914a9bcf1812",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TP3D-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29229ed-a53b-4c5b-a31e-bb7e53383ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from superpoint_transformer.data import Data\n",
    "\n",
    "def batch_pca(xyz):\n",
    "    \"\"\"\n",
    "    Compute the PCA of a batch of point clouds of size (*, N, M).\n",
    "    \"\"\"\n",
    "    assert 2 <= xyz.dim() <= 3\n",
    "    xyz = xyz.unsqueeze(0) if xyz.dim() == 2 else xyz\n",
    "\n",
    "    pos_centered = xyz - xyz.mean(dim=1).unsqueeze(1)\n",
    "    cov_matrix = pos_centered.transpose(1, 2).bmm(pos_centered) / pos_centered.shape[1]\n",
    "    eigenval, eigenvect = torch.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # If Nan values are computed, return equal eigenvalues and\n",
    "    # Identity eigenvectors\n",
    "    idx_nan = torch.where(torch.logical_and(\n",
    "        eigenval.isnan().any(1), eigenvect.flatten(1).isnan().any(1)))\n",
    "    eigenval[idx_nan] = torch.ones(3, dtype=eigenval.dtype, device=xyz.device)\n",
    "    eigenvect[idx_nan] = torch.eye(3, dtype=eigenvect.dtype, device=xyz.device)\n",
    "\n",
    "    # Precision errors may cause close-to-zero eigenvalues to be\n",
    "    # negative. Hard-code these to zero\n",
    "    eigenval[torch.where(eigenval < 0)] = 0\n",
    "\n",
    "    return eigenval, eigenvect\n",
    "\n",
    "\n",
    "class PCAComputePointwise(object):\n",
    "    \"\"\"\n",
    "    Compute PCA for the local neighborhood of each point in the cloud.\n",
    "\n",
    "    Input data is expected to be stored in DENSE format.\n",
    "\n",
    "    Results are saved in `eigenvalues` and `eigenvectors` attributes.\n",
    "    `data.eigenvalues` is a tensor\n",
    "    :math:`(\\lambda_1, \\lambda_2, \\lambda_3)` such that\n",
    "    :math:`\\lambda_1 \\leq \\lambda_2 \\leq \\lambda_3`.\n",
    "    `data.eigenvectors` is 1x9 tensor containing the eigenvectors\n",
    "    associated with `data.eigenvalues`, concatenated in the same order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_neighbors: int, optional\n",
    "        Controls the maximum number of neighbors on which to compute\n",
    "        PCA. If `r=None`, `num_neighbors` will be used as K for\n",
    "        K-nearest neighbor search. Otherwise, `num_neighbors` will be\n",
    "        the maximum number of neighbors used in radial neighbor search.\n",
    "    r: float, optional\n",
    "        If not `None`, neighborhoods will be computed with a\n",
    "        radius-neighbor approach. If `None`, K-nearest neighbors will\n",
    "        be used.\n",
    "    use_full_pos: bool, optional\n",
    "        If True, the neighborhood search will be carried on the point\n",
    "        positions found in the `data.full_pos`. An error will be raised\n",
    "        if data carries no such attribute. See `GridSampling3D` for\n",
    "        producing `data.full_pos`.\n",
    "        If False, the neighbor search will be computed on `data.pos`.\n",
    "    use_cuda: bool, optional\n",
    "        If True, the computation will be carried on CUDA.\n",
    "    workers: int, optional\n",
    "        If not `None`, the features computation will be distributed\n",
    "        across the provided number of workers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, num_neighbors=40, r=None, use_full_pos=False, use_cuda=False,\n",
    "            use_faiss=True, ncells=None, nprobes=10, chunk_size=1000000):\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.r = r\n",
    "        self.use_full_pos = use_full_pos\n",
    "        self.use_cuda = use_cuda and torch.cuda.is_available()\n",
    "        self.use_faiss = use_faiss and torch.cuda.is_available()\n",
    "        self.ncells = ncells\n",
    "        self.nprobes = nprobes\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def _process(self, data: Data):\n",
    "        assert getattr(data, 'pos', None) is not None, \\\n",
    "            \"Data must contain a 'pos' attribute.\"\n",
    "        assert not self.use_full_pos \\\n",
    "               or getattr(data, 'full_pos', None) is not None, \\\n",
    "            \"Data must contain a 'full_pos' attribute.\"\n",
    "\n",
    "        # Recover the query and search clouds\n",
    "        xyz_query = data.pos\n",
    "        xyz_search = data.full_pos if self.use_full_pos else data.pos\n",
    "\n",
    "        # Move computation to CUDA if required\n",
    "        input_device = xyz_query.device\n",
    "        if self.use_cuda and not xyz_query.is_cuda and not self.use_faiss:\n",
    "            xyz_query = xyz_query.cuda()\n",
    "            xyz_search = xyz_search.cuda()\n",
    "\n",
    "        # Compute the neighborhoods\n",
    "        if self.r is not None:\n",
    "            # Radius-NN search with torch_points_kernel\n",
    "            sampler = RadiusNeighbourFinder(\n",
    "                self.r, self.num_neighbors, conv_type='DENSE')\n",
    "            neighbors = sampler.find_neighbours(\n",
    "                xyz_search.unsqueeze(0), xyz_query.unsqueeze(0))[0]\n",
    "        elif self.use_faiss:\n",
    "            # K-NN search with FAISS\n",
    "            nn_finder = FAISSGPUKNNNeighbourFinder(\n",
    "                self.num_neighbors, ncells=self.ncells, nprobes=self.nprobes)\n",
    "            neighbors = nn_finder(xyz_search, xyz_query, None, None)\n",
    "        else:\n",
    "            # K-NN search with KeOps. If the number of points is greater\n",
    "            # than 16 millions, KeOps requires double precision.\n",
    "            xyz_query = xyz_query.contiguous()\n",
    "            xyz_search = xyz_search.contiguous()\n",
    "            if xyz_search.shape[0] > 1.6e7:\n",
    "                xyz_query_keops = LazyTensor(xyz_query[:, None, :].double())\n",
    "                xyz_search_keops = LazyTensor(xyz_search[None, :, :].double())\n",
    "            else:\n",
    "                xyz_query_keops = LazyTensor(xyz_query[:, None, :])\n",
    "                xyz_search_keops = LazyTensor(xyz_search[None, :, :])\n",
    "            d_keops = ((xyz_query_keops - xyz_search_keops) ** 2).sum(dim=2)\n",
    "            neighbors = d_keops.argKmin(self.num_neighbors, dim=1)\n",
    "            # raise NotImplementedError(\n",
    "            #     \"Fast K-NN search has not been implemented yet. Please \"\n",
    "            #     \"consider using radius search instead.\")\n",
    "\n",
    "        # Compute PCA for each neighborhood\n",
    "        # Note: this is surprisingly slow on GPU, so better run on CPU\n",
    "        eigenvalues = []\n",
    "        eigenvectors = []\n",
    "        n_chunks = math.ceil(neighbors.shape[0] / self.chunk_size)\n",
    "        for i in range(n_chunks):\n",
    "            xyz_neigh_batch = xyz_search[\n",
    "                neighbors[i * self.chunk_size: (i + 1) * self.chunk_size]]\n",
    "            eval, evec = batch_pca(xyz_neigh_batch.cpu())\n",
    "            evec = evec.transpose(2, 1).flatten(1)\n",
    "            eigenvalues.append(eval)\n",
    "            eigenvectors.append(evec)\n",
    "        eigenvalues = torch.cat(eigenvalues, dim=0)\n",
    "        eigenvectors = torch.cat(eigenvectors, dim=0)\n",
    "\n",
    "        # Save eigendecomposition results in data attributes\n",
    "        data.eigenvalues = eigenvalues.to(input_device)\n",
    "        data.eigenvectors = eigenvectors.to(input_device)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in tq(data)]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        attr_repr = ', '.join([f'{k}={v}' for k, v in self.__dict__.items()])\n",
    "        return f'{self.__class__.__name__}({attr_repr})'\n",
    "\n",
    "\n",
    "class EigenFeatures(object):\n",
    "    \"\"\"\n",
    "    Compute local geometric features based on local eigenvalues and\n",
    "    eigenvectors.\n",
    "\n",
    "    The following local geometric features are computed and saved in\n",
    "    dedicated data attributes: `normal`, `scattering`, `linearity` and\n",
    "    `planarity`. The formulation of those is inspired from\n",
    "    \"Hierarchical extraction of urban objects from mobile laser\n",
    "    scanning data\" [Yang et al. 2015]\n",
    "\n",
    "    Data is expected to carry `eigenvectors` and `eigenvectors`\n",
    "    attributes:\n",
    "    `data.eigenvalues` is a tensor\n",
    "    :math:`(\\lambda_1, \\lambda_2, \\lambda_3)` such that\n",
    "    :math:`\\lambda_1 \\leq \\lambda_2 \\leq \\lambda_3`.\n",
    "    `data.eigenvectors` is 1x9 tensor containing the eigenvectors\n",
    "    associated with `data.eigenvalues`, concatenated in the same order.\n",
    "    See `PCAComputePointwise` for generating those.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    normal: bool, optional\n",
    "        If True, the normal to the local surface will be computed.\n",
    "    linearity: bool, optional\n",
    "        If True, the local linearity will be computed.\n",
    "    planarity: bool, optional\n",
    "        If True, the local planarity will be computed.\n",
    "    scattering: bool, optional\n",
    "        If True, the local scattering will be computed.\n",
    "    temperature: float, optional\n",
    "        If set to a float value, the returned features will be run\n",
    "        through a scaled softmax with temperature being the scale. Set\n",
    "        to None by default.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normal=True, linearity=True, planarity=True,\n",
    "                 scattering=True, verticality=True, temperature=None):\n",
    "        self.normal = normal\n",
    "        self.linearity = linearity\n",
    "        self.planarity = planarity\n",
    "        self.scattering = scattering\n",
    "        self.verticality = verticality\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def _process(self, data: Data):\n",
    "        assert getattr(data, 'eigenvalues', None) is not None, \\\n",
    "            \"Data must contain an 'eigenvalues' attribute.\"\n",
    "        assert getattr(data, 'eigenvectors', None) is not None, \\\n",
    "            \"Data must contain an 'eigenvectors' attribute.\"\n",
    "\n",
    "        if self.normal:\n",
    "            # The normal is the eigenvector carried by the smallest\n",
    "            # eigenvalue\n",
    "            data.normal = data.eigenvectors[:, :3]\n",
    "\n",
    "        # Eigenvalues: 0 <= l0 <= l1 <= l2\n",
    "        # Following, [Yang et al. 2015] we use the sqrt of eigenvalues\n",
    "        v0 = data.eigenvalues[:, 0].sqrt().squeeze()\n",
    "        v1 = data.eigenvalues[:, 1].sqrt().squeeze()\n",
    "        v2 = data.eigenvalues[:, 2].sqrt().squeeze() + 1e-6\n",
    "        \n",
    "        e0 = eigenvectors[:, :, 0].abs() * eigenvalues[:, [0]]\n",
    "        e1 = eigenvectors[:, :, 1].abs() * eigenvalues[:, [1]]\n",
    "        e2 = eigenvectors[:, :, 2].abs() * eigenvalues[:, [2]]\n",
    "        u = e0 + e1 + e2\n",
    "\n",
    "        # Compute the eigen features\n",
    "        linearity = (v2 - v1) / v2\n",
    "        planarity = (v1 - v0) / v2\n",
    "        scattering = v0 / v2\n",
    "        verticality = u[:, 2] / torch.linalg.norm(u, dim=1)\n",
    "\n",
    "        # Compute the softmax version of the features, for more\n",
    "        # opinionated geometric information. As a heuristic, set\n",
    "        # temperature=5 for clouds of 30 points or more.\n",
    "        if self.temperature:\n",
    "            values = (self.temperature * torch.cat([\n",
    "                linearity.view(-1, 1),\n",
    "                planarity.view(-1, 1),\n",
    "                scattering.view(-1, 1)], dim=1)).exp()\n",
    "            values = values / values.sum(dim=1).view(-1, 1)\n",
    "            linearity = values[:, 0]\n",
    "            planarity = values[:, 1]\n",
    "            scattering = values[:, 2]\n",
    "\n",
    "        if self.linearity:\n",
    "            data.linearity = linearity\n",
    "\n",
    "        if self.planarity:\n",
    "            data.planarity = planarity\n",
    "\n",
    "        if self.scattering:\n",
    "            data.scattering = scattering\n",
    "        \n",
    "        if self.verticality:\n",
    "            data.verticality = verticality\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in data]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        attr_repr = ', '.join([f'{k}={v}' for k, v in self.__dict__.items()])\n",
    "        return f'{self.__class__.__name__}({attr_repr})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8150a7-6b3d-43b8-974b-ab231fe12e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: 54.819s\n"
     ]
    }
   ],
   "source": [
    "# On GPU\n",
    "xyz = torch.rand(10**6, 50, 3).cuda()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "batch_pca(xyz)\n",
    "torch.cuda.synchronize()\n",
    "print(f'PCA: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f2ae5b-98d2-424b-8dae-445c378a72d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: 1.981s\n"
     ]
    }
   ],
   "source": [
    "# On CPU\n",
    "xyz = torch.rand(10**6, 50, 3)\n",
    "\n",
    "start = time()\n",
    "batch_pca(xyz)\n",
    "print(f'PCA: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e7c6ab32-ad94-4e51-8213-684d2b8e2eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA CPU: 2.021s\n",
      "Geometric Features: 0.012s\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# On CPU\n",
    "xyz = torch.rand(10**6, 50, 3)\n",
    "\n",
    "start = time()\n",
    "eigenvalues, eigenvectors = batch_pca(xyz)\n",
    "print(f'PCA CPU: {time() - start:0.3f}s')\n",
    "\n",
    "# On CPU\n",
    "start = time()\n",
    "d = Data(x=xyz, eigenvalues=eigenvalues, eigenvectors=eigenvectors)\n",
    "d = EigenFeatures(normal=True, linearity=True, planarity=True, scattering=True, verticality=True, temperature=None)(d)\n",
    "print(f'Geometric Features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a579163f-8a89-4151-b286-96b8816bde1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA CPU: 2.037s\n",
      "Geometric Features: 0.137s\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# On CPU\n",
    "xyz = torch.rand(10**6, 50, 3)\n",
    "\n",
    "start = time()\n",
    "eigenvalues, eigenvectors = batch_pca(xyz)\n",
    "print(f'PCA CPU: {time() - start:0.3f}s')\n",
    "\n",
    "# On GPU\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "d = Data(x=xyz.cuda(), eigenvalues=eigenvalues.cuda(), eigenvectors=eigenvectors.cuda())\n",
    "d = EigenFeatures(normal=True, linearity=True, planarity=True, scattering=True, verticality=True, temperature=None)(d)\n",
    "torch.cuda.synchronize()\n",
    "print(f'Geometric Features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63f8a0ee-901b-470a-8d52-e963ef278925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA CPU: 5.287s\n",
      "Geometric Features: 0.093s\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# On CPU\n",
    "start = time()\n",
    "eigenvalues, eigenvectors = batch_pca(data.pos[data.neighbors])\n",
    "data.eigenvalues = eigenvalues\n",
    "data.eigenvectors = eigenvectors\n",
    "print(f'PCA CPU: {time() - start:0.3f}s')\n",
    "\n",
    "# On CPU\n",
    "start = time()\n",
    "data = EigenFeatures(normal=True, linearity=True, planarity=True, scattering=True, verticality=True, temperature=None)(data)\n",
    "print(f'Geometric Features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1eea8-cde4-4dbd-86c9-9a1c35a07efc",
   "metadata": {},
   "source": [
    "Surprisingly, torch's CPU implementation is faster both for computing PCA and geometric features is faster on CPU overall !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f73d639-0171-4d78-a313-9d53b8d3f976",
   "metadata": {},
   "source": [
    "### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "620370cd-ac41-4b09-b943-4beb7d6b4c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric features: 2.147s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.transforms import compute_pointfeatures\n",
    "\n",
    "data = data.cpu()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "start = time()\n",
    "data = compute_pointfeatures(data, pos=True, radius=5, rgb=True, linearity=True, planarity=True, scattering=True, verticality=True, normal=False, length=False, surface=False, volume=False)\n",
    "print(f'Geometric features: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99aa378-67a1-4f67-8a91-15f312ac4a09",
   "metadata": {},
   "source": [
    "# Point adjacency graph computation\n",
    "This graph is based on the nearest neighbor graph computed for geometric features. However, although features may require 30-50 neighbors to produce good partition, the adjacency graph benefits from using fewer neighbors (eg 10 in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfc4ee9-1a9e-4206-8024-65e1c888a4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency graph: 0.369s\n"
     ]
    }
   ],
   "source": [
    "from superpoint_transformer.transforms import compute_ajacency_graph\n",
    "\n",
    "k_adjacency = 10\n",
    "lambda_edge_weight = 1\n",
    "\n",
    "start = time()\n",
    "data = compute_ajacency_graph(data, k_adjacency, lambda_edge_weight)\n",
    "print(f'Adjacency graph: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad16898-7426-484e-825f-e6ecff4839ca",
   "metadata": {},
   "source": [
    "# Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0f53c-ef68-4f14-9fa6-1124674c55ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from superpoint_transformer.transforms import compute_partition\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time()\n",
    "\n",
    "# Parallel cut-pursuit\n",
    "nag = compute_partition(data, 0.5, cutoff=10, verbose=True, iterations=10)\n",
    "# nag = compute_partition(data, 0.5, cutoff=10, verbose=True, iterations=5)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print(f'Partition num_nodes={data.num_nodes}, num_edges={data.num_edges}: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f8416-adc6-49e0-9a78-3d6abd20cb16",
   "metadata": {},
   "source": [
    "# Superpoint graph computation\n",
    "In the original SPG implementation, the SP graph would be computed based on the pointwise Delaunay triangulation graph. This is super inefficient. Instead, we will compute the Delaunay triangulation on the superpoint level, which sould be much faster. However, to account for large and long-shaped superpoints, we will not work with the SP centroids only (Delaunay triangulation would not capture all adjacent SPs), but on random/farthest point samplings inside the SPs (as function of SP area/volume/number of points).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15b1fb3f-90ee-4740-82a9-711b85e6a7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import Delaunay\n",
    "import itertools\n",
    "from torch_scatter import scatter_mean, scatter_std, scatter_min, scatter_sum\n",
    "import superpoint_transformer.partition.utils.libpoint_utils as point_utils\n",
    "from torch_geometric.nn.pool.consecutive import consecutive_cluster\n",
    "\n",
    "\n",
    "def sample_clusters(data, high=32, low=1, pointers=False):\n",
    "    \"\"\"Compute point indices for sampling points inside clusters saved\n",
    "    in a CSR format.\n",
    "    \"\"\"\n",
    "    #TODO: rename this function: sample_points_in_nag, sample_point_index, ...\n",
    "    #TODO: operate on NAG and careful with indices !\n",
    "    #TODO: must be able to sample on subset of sub too (for edge sampling)\n",
    "    #TODO: could optionally sample from i into j with i > j > 0...\n",
    "\n",
    "    # Compute the number of points that will be sampled from each\n",
    "    # cluster\n",
    "    if high > 0:\n",
    "        # k * tanh(x / k) is bounded by k, is ~x for low x and starts\n",
    "        # saturating at x~k\n",
    "        n_samples = (high * torch.tanh(\n",
    "            data.sub.sizes / high)).floor().long()\n",
    "    else:\n",
    "        # Fallback to sqrt sampling\n",
    "        n_samples = data.sub.sizes.sqrt().round().long()\n",
    "\n",
    "    # Make sure each cluster is sampled at least 'low' times\n",
    "    n_samples = n_samples.clamp(min=low)\n",
    "\n",
    "    # Sample values in [0, 1], these will be used to compute\n",
    "    # corresponding integer indices for vectorized sampling of the\n",
    "    # points directly from the CSR-format cluster-to-point data\n",
    "    samples = torch.rand(n_samples.sum())\n",
    "\n",
    "    # Convert the [0, 1] samples to integer indices in [0, n_samples]\n",
    "    # depending on the corresponding cluster sampling size. The 0.9999\n",
    "    # factor is a security to handle the case where torch.rand is to\n",
    "    # close to 1.0, which would yield incorrect sampling coordinates\n",
    "    n_bins = n_samples.repeat_interleave(n_samples)\n",
    "    samples = (samples * 0.9999 * n_bins).floor().long()\n",
    "\n",
    "    # As they are now, sampling indices are expressed in [0, sub_size].\n",
    "    # If we want to sample directly from the cluster-to-point CSR data,\n",
    "    # we need to express those coordinates in the aggregated CSR values,\n",
    "    # that is to say we need to apply the cumulative cluster sizes as\n",
    "    # offsets to the indices. This is typically what is stored in\n",
    "    # CSRData.pointers\n",
    "    offsets = data.sub.pointers[:-1].repeat_interleave(n_samples)\n",
    "    samples = samples + offsets\n",
    "\n",
    "    # Now we can get the sampled point indices\n",
    "    idx_samples = data.sub.points[samples]\n",
    "\n",
    "    # Return here if sampling pointers are not required\n",
    "    if not pointers:\n",
    "        return idx_samples\n",
    "\n",
    "    # Compute the pointers\n",
    "    ptr_samples = torch.cat([torch.LongTensor([0]), n_samples.cumsum(dim=0)])\n",
    "\n",
    "    return idx_samples, ptr_samples.contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8b94f-d2df-4b6d-b663-6c27050ba877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_cluster_graph(\n",
    "        i_level, nag, high_node=32, high_edge=64, low=5):\n",
    "    # TODO: WARNING the cluster geometric features will only work if we\n",
    "    #  enforced a cutoff on the minimum superpoint size ! Make sure you\n",
    "    #  enforce this\n",
    "\n",
    "    # TODO: define recursive sampling for super(n)point features\n",
    "    # TODO: define recursive edge for super(n)edge features\n",
    "    # TODO: return all eigenvectors from the C++ geometric features, for\n",
    "    #  superborder features computation\n",
    "    # TODO: other superedge ideas to better describe how 2 clusters\n",
    "    # relate and the geometry of their border (S=source, T=target):\n",
    "    # - avg distance S/T points in border to centroid S/T (how far\n",
    "    #   is the border from the cluster center)\n",
    "    # - angle of mean S->T direction wrt S/T principal components (is\n",
    "    #   the border along the long of short side of objects ?)\n",
    "    # - PCA of points in S/T cloud (is it linear border or surfacic\n",
    "    #   border ?)\n",
    "    # - mean dist of S->T along S/T normal (offset along the objects\n",
    "    #   normals, eg offsets between steps)\n",
    "\n",
    "    assert isinstance(nag, NAG)\n",
    "    assert i_level > 0\n",
    "\n",
    "    # Recover the i_level Data object we will be working on\n",
    "    data = nag[i_level]\n",
    "\n",
    "    # Aggregate some point attributes into the clusters. This is not\n",
    "    # performed dynamically since not all attributes can be aggregated\n",
    "    # (eg 'neighbors', 'distances', 'edge_index', 'edge_attr'...)\n",
    "    data_sub = nag[i_level - 1]\n",
    "\n",
    "    if 'pos' in data_sub.keys:\n",
    "        data.pos = scatter_mean(\n",
    "            data_sub.pos.cuda(), data_sub.super_index.cuda(), dim=0).cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if 'rgb' in data_sub.keys:\n",
    "        data.rgb = scatter_mean(\n",
    "            data_sub.rgb.cuda(), data_sub.super_index.cuda(), dim=0).cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if 'y' in data_sub.keys:\n",
    "        assert data_sub.y.dim() == 2, \\\n",
    "            \"Expected Data.y to hold `(num_nodes, num_classes)` \" \\\n",
    "            \"histograms, not single labels\"\n",
    "        data.y = scatter_sum(\n",
    "            data_sub.y.cuda(), data_sub.super_index.cuda(), dim=0).cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Sample points among the clusters. These will be used to compute\n",
    "    # cluster geometric features as well as cluster adjacency graph and\n",
    "    # edge features\n",
    "    idx_samples, ptr_samples = sample_clusters(\n",
    "        data, high=high_node, low=low, pointers=True)\n",
    "\n",
    "    # Compute cluster geometric features\n",
    "    xyz = nag[0].pos[idx_samples].cpu().numpy()\n",
    "    nn = np.arange(idx_samples.shape[0]).astype('uint32')  # !!!! IMPORTANT CAREFUL WITH UINT32 = 4 BILLION points MAXIMUM !!!!\n",
    "    nn_ptr = ptr_samples.cpu().numpy().astype('uint32')  # !!!! IMPORTANT CAREFUL WITH UINT32 = 4 BILLION points MAXIMUM !!!!\n",
    "\n",
    "    # Heuristic to avoid issues when a cluster sampling is such that\n",
    "    # it produces singular covariance matrix (eg the sampling only\n",
    "    # contains the same point repeated multiple times)\n",
    "    xyz = xyz + torch.rand(xyz.shape).numpy() * 1e-5\n",
    "\n",
    "    # C++ geometric features computation on CPU\n",
    "    f = point_utils.compute_geometric_features(xyz, nn, nn_ptr, False)\n",
    "    f = torch.from_numpy(f.astype('float32'))\n",
    "\n",
    "    # Recover length, surface and volume\n",
    "    data.length = f[:, 7].to(data.pos.device)\n",
    "    data.surface = f[:, 8].to(data.pos.device)\n",
    "    data.volume = f[:, 9].to(data.pos.device)\n",
    "    data.normal = f[:, 4:7].view(-1, 3).to(data.pos.device)\n",
    "\n",
    "    # Sample points among the clusters. These will be used to compute\n",
    "    # cluster adjacency graph and edge features. Note we sample more\n",
    "    # generously here than for cluster features, because we need to\n",
    "    # capture fine-grained adjacency\n",
    "    idx_samples, ptr_samples = sample_clusters(\n",
    "        data, high=high_edge, low=low, pointers=True)\n",
    "\n",
    "    # Delaunay triangulation on the sampled points\n",
    "    tri = Delaunay(nag[0].pos[idx_samples].numpy())\n",
    "\n",
    "    # Concatenate all edges of the triangulation. For now, we do not\n",
    "    # worry about directed/undirected graphs to mitigate memory and\n",
    "    # compute\n",
    "    pairs = torch.LongTensor(list(itertools.combinations(range(4), 2)))\n",
    "    edges = torch.from_numpy(np.hstack([\n",
    "        np.vstack((tri.vertices[:, i], tri.vertices[:, j]))\n",
    "        for i, j in pairs]).T).long()\n",
    "\n",
    "    # Now we are only interested in the edges connecting two different\n",
    "    # clusters and not in the intra-cluster connections. So we first\n",
    "    # identify the edges of interest. This step requires having access\n",
    "    # to the whole NAG, since we need to convert level-0 point indices\n",
    "    # into their corresponding level-i superpoint indices\n",
    "    idx_point_source = idx_samples[edges[:, 0]]\n",
    "    idx_point_target = idx_samples[edges[:, 1]]\n",
    "    idx_source = idx_point_source\n",
    "    idx_target = idx_point_target\n",
    "    for i in range(i_level):\n",
    "        idx_source = nag[i].super_index[idx_source]\n",
    "        idx_target = nag[i].super_index[idx_target]\n",
    "    inter_cluster = torch.where(idx_source != idx_target)[0]\n",
    "\n",
    "    # Now only consider the edges of interest (ie inter-cluster edges)\n",
    "    idx_point_source = idx_point_source[inter_cluster]\n",
    "    idx_point_target = idx_point_target[inter_cluster]\n",
    "    idx_source = idx_source[inter_cluster]\n",
    "    idx_target = idx_target[inter_cluster]\n",
    "\n",
    "    # Direction are the pointwise source->target vectors, based on which\n",
    "    # we will compute superedge descriptors. So far we are manipulating\n",
    "    # inter-cluster edges, but their may be multiple of those for a\n",
    "    # given source-target pair. Next, we want to aggregate those into\n",
    "    # \"superegdes\" and compute corresponding features (designated with\n",
    "    # 'se_')\n",
    "    direction = nag[0].pos[idx_point_target] - nag[0].pos[idx_point_source]\n",
    "    dist = torch.linalg.norm(direction, dim=1)\n",
    "\n",
    "    # Create unique and consecutive inter-cluster edge identifiers for\n",
    "    # torch_scatter operations. We use 'se' to designate 'superedge' (ie\n",
    "    # an edge between two clusters)\n",
    "    idx_se = idx_source + data.num_nodes * idx_target\n",
    "    idx_se, perm = consecutive_cluster(idx_se)\n",
    "    idx_se_source = idx_source[perm]\n",
    "    idx_se_target = idx_target[perm]\n",
    "    se = torch.vstack((idx_se_source, idx_se_target))\n",
    "\n",
    "    # We can now use torch_scatter operations to compute superedge\n",
    "    # features\n",
    "    se_direction = scatter_mean(direction.cuda(), idx_se.cuda(), dim=0).cpu()\n",
    "    se_dist = scatter_mean(dist.cuda(), idx_se.cuda(), dim=0).cpu()\n",
    "    se_min_dist = scatter_min(dist.cuda(), idx_se.cuda(), dim=0)[0].cpu()\n",
    "    se_std_dist = scatter_std(dist.cuda(), idx_se.cuda(), dim=0).cpu()\n",
    "\n",
    "    se_centroid_direction = data.pos[se[1]] - data.pos[se[0]]\n",
    "    se_centroid_dist = torch.linalg.norm(se_centroid_direction, dim=1)\n",
    "\n",
    "    se_normal_source = data.normal[se[0]]\n",
    "    se_normal_target = data.normal[se[1]]\n",
    "    se_normal_angle = (se_normal_source * se_normal_target).sum(dim=1)\n",
    "    se_angle_source = (se_direction * se_normal_source).sum(dim=1)\n",
    "    se_angle_target = (se_direction * se_normal_target).sum(dim=1)\n",
    "\n",
    "    se_length_ratio = data.length[se[0]] / (data.length[se[1]] + 1e-6)\n",
    "    se_surface_ratio = data.surface[se[0]] / (data.surface[se[1]] + 1e-6)\n",
    "    se_volume_ratio = data.volume[se[0]] / (data.volume[se[1]] + 1e-6)\n",
    "    se_size_ratio = data.sub_size[se[0]] / (data.sub_size[se[1]] + 1e-6)\n",
    "\n",
    "    # The superedges we have created so far are oriented. We need to\n",
    "    # create the edges and corresponding features for the Target->Source\n",
    "    # direction now\n",
    "    se = torch.cat((se, se.roll(1, 1)))\n",
    "\n",
    "    se_feat = [\n",
    "        torch.cat((se_dist, se_dist)),\n",
    "        torch.cat((se_min_dist, se_min_dist)),\n",
    "        torch.cat((se_std_dist, se_std_dist)),\n",
    "        torch.cat((se_centroid_dist, se_centroid_dist)),\n",
    "        torch.cat((se_normal_angle, se_normal_angle)),\n",
    "        torch.cat((se_angle_source, se_angle_target)),\n",
    "        torch.cat((se_angle_target, se_angle_source)),\n",
    "        torch.cat((se_length_ratio, 1 / (se_length_ratio + 1e-6))),\n",
    "        torch.cat((se_surface_ratio, 1 / (se_surface_ratio + 1e-6))),\n",
    "        torch.cat((se_volume_ratio, 1 / (se_volume_ratio + 1e-6))),\n",
    "        torch.cat((se_size_ratio, 1 / (se_size_ratio + 1e-6)))]\n",
    "\n",
    "    # Aggregate all edge features in a single tensor\n",
    "    se_feat = torch.vstack(se_feat).T\n",
    "\n",
    "    # Save superedges and superedge features in the Data object\n",
    "    data.edge_index = se\n",
    "    data.edge_attr = se_feat\n",
    "\n",
    "    # Restore the i_level Data object, if need be\n",
    "    nag._list[i_level] = data\n",
    "\n",
    "    return nag\n",
    "\n",
    "\n",
    "def compute_cluster_graph(nag, high_node=32, high_edge=64, low=5):\n",
    "    assert isinstance(nag, NAG)\n",
    "    for i_level in range(1, nag.num_levels):\n",
    "        nag = _compute_cluster_graph(\n",
    "            i_level, nag, high_node=high_node, high_edge=high_edge, low=low)\n",
    "    return nag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698a2be-926d-4799-b077-be2b9e5e10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from superpoint_transformer.transforms import compute_cluster_graph\n",
    "\n",
    "start = time()\n",
    "compute_cluster_graph(nag, high_node=32, high_edge=64, low=5)\n",
    "print(f'SP Graph computation: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badac300-dc44-4c7a-b5f4-1bf2ae47de12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bed7c7-7fff-43f6-a304-285a4d57a56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf584bc2-3745-49d5-8a04-973c27fb0700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a511758-3d94-4eaf-8106-bddd5de3e8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "171c6cc4-e3ee-4e3b-a8fd-a2eea2498166",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a2d2df-6065-4b43-a395-4a2c5a0cabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "\n",
    "# temp_dir = DATA_ROOT + '/datasets/kitti360/shared/temp' \n",
    "# os.makedirs(temp_dir, exist_ok=True) \n",
    "\n",
    "# torch.save((data, data_c), os.path.join(temp_dir, 'preliminaries.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bad66f78-bae6-42f5-9235-ac5db0d84164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import FixedPoints\n",
    "from superpoint_transformer.transforms import GridSampling3D\n",
    "import os.path as osp\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# PALETTE = np.array(plotly.colors.qualitative.Plotly)\n",
    "# PALETTE = np.array(plotly.colors.qualitative.Dark24)\n",
    "PALETTE = np.array(plotly.colors.qualitative.Light24)\n",
    "\n",
    "\n",
    "def rgb_to_plotly_rgb(rgb):\n",
    "    \"\"\"Convert torch.Tensor of float RGB values in [0, 1] to\n",
    "    plotly-friendly RGB format.\n",
    "    \"\"\"\n",
    "    assert isinstance(rgb, torch.Tensor) and rgb.max() <= 1.0 and rgb.dim() <= 2\n",
    "\n",
    "    if rgb.dim() == 1:\n",
    "        rgb = rgb.unsqueeze(0)\n",
    "\n",
    "    return [f\"rgb{tuple(x)}\" for x in (rgb * 255).int().numpy()]\n",
    "\n",
    "\n",
    "def hex_to_tensor(h):\n",
    "    h = h.lstrip('#')\n",
    "    rgb = tuple(int(h[i:i + 2], 16) for i in (0, 2, 4))\n",
    "    return torch.Tensor(rgb) / 255\n",
    "\n",
    "\n",
    "def feats_to_rgb(feats, normalize=False):\n",
    "    \"\"\"Convert features of the format M x N with N>=1 to an M x 3\n",
    "    tensor with values in [0, 1 for RGB visualization].\n",
    "    \"\"\"\n",
    "    is_normalized = False\n",
    "\n",
    "    if feats.dim() == 1:\n",
    "        feats = feats.unsqueeze(1)\n",
    "    elif feats.dim() > 2:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if feats.shape[1] == 3:\n",
    "        color = feats\n",
    "\n",
    "    elif feats.shape[1] == 1:\n",
    "        # If only 1 feature is found convert to a 3-channel\n",
    "        # repetition for grayscale visualization.\n",
    "        color = feats.repeat_interleave(3, 1)\n",
    "\n",
    "    elif feats.shape[1] == 2:\n",
    "        # If 2 features are found, add an extra channel.\n",
    "        color = torch.cat([feats, torch.ones(feats.shape[0], 1)], 1)\n",
    "\n",
    "    elif feats.shape[1] > 3:\n",
    "        # If more than 3 features or more are found, project\n",
    "        # features to a 3-dimensional space using N-simplex PCA\n",
    "        # Heuristics for clamping\n",
    "        #   - most features live in [0, 1]\n",
    "        #   - most n-simplex PCA features live in [-0.5, 0.6]\n",
    "        color = identity_PCA(feats, dim=3)\n",
    "        color = (torch.clamp(color, -0.5, 0.6) + 0.5) / 1.1\n",
    "        is_normalized = True\n",
    "\n",
    "    if normalize and not is_normalized:\n",
    "        # Unit-normalize the features in a hypercube of shared scale\n",
    "        # for nicer visualizations\n",
    "        if color.max() != color.min():\n",
    "            color = color - color.min(dim=0).values.view(1, -1)\n",
    "        color = color / (color.max(dim=0).values.view(1, -1) + 1e-6)\n",
    "\n",
    "    return color\n",
    "\n",
    "\n",
    "def identity_PCA(x, dim=3):\n",
    "    \"\"\"Reduce dimension of x based on PCA on the union of the n-simplex.\n",
    "    This is a way of reducing the dimension of x while treating all\n",
    "    input dimensions with the same importance, independently of the\n",
    "    input distribution in x.\n",
    "    \"\"\"\n",
    "    assert x.dim() == 2, f\"Expected x.dim()=2 but got x.dim()={x.dim()} instead\"\n",
    "\n",
    "    # Create z the union of the N-simplex\n",
    "    input_dim = x.shape[1]\n",
    "    z = torch.eye(input_dim)\n",
    "\n",
    "    # PCA on z\n",
    "    z_offset = z.mean(axis=0)\n",
    "    z_centered = z - z_offset\n",
    "    cov_matrix = z_centered.T.mm(z_centered) / len(z_centered)\n",
    "    _, eigenvectors = torch.linalg.eigh(cov_matrix)\n",
    "\n",
    "    # Apply the PCA on x\n",
    "    x_reduced = (x - z_offset).mm(eigenvectors[:, -dim:])\n",
    "\n",
    "    return x_reduced\n",
    "\n",
    "\n",
    "def visualize_3d(\n",
    "        data, data_c, figsize=800, width=None, height=None, class_names=None,\n",
    "        class_colors=None, class_opacities=None, voxel=0.1, max_points=100000,\n",
    "        pointsize=5, error_color=None, show_superpoint_number=False, **kwargs):\n",
    "    \n",
    "    # 3D visualization modes\n",
    "    modes = {'name': [], 'key': [], 'num_traces': []}\n",
    "\n",
    "    # Make copies of the data to be modified in this scope\n",
    "    data = data.clone()\n",
    "    data_c = data_c.clone()\n",
    "    \n",
    "    # Check whether a partition is available\n",
    "    has_partition = data_c is not None and data.p2c is not None\n",
    "\n",
    "    # Subsample to limit the drawing time\n",
    "    data.edge_index = None\n",
    "    data.edge_attr = None\n",
    "    data = GridSampling3D(voxel, mode='last')(data)\n",
    "    if data.num_nodes > max_points:\n",
    "        data = FixedPoints(\n",
    "            max_points, replace=False, allow_duplicates=False)(data)\n",
    "        \n",
    "    # Round to the cm for cleaner hover info\n",
    "    data.pos = (data.pos * 100).round() / 100\n",
    "    data_c.pos = (data_c.pos * 100).round() / 100\n",
    "\n",
    "    # Class colors initialization\n",
    "    if class_colors is not None and not isinstance(class_colors[0], str):\n",
    "        class_colors = [f\"rgb{tuple(x)}\" for x in class_colors]\n",
    "    else:\n",
    "        class_colors = None\n",
    "\n",
    "    # Prepare figure\n",
    "    width = width if width and height else figsize\n",
    "    height = height if width and height else int(figsize / 2)\n",
    "    margin = int(0.02 * min(width, height))\n",
    "    layout = go.Layout(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        scene=dict(aspectmode='data', ),  # preserve aspect ratio\n",
    "        margin=dict(l=margin, r=margin, b=margin, t=margin),\n",
    "        uirevision=True)\n",
    "    fig = go.Figure(layout=layout)\n",
    "    initialized_visibility = False\n",
    "    \n",
    "    # Draw a trace for RGB 3D point cloud\n",
    "    if getattr(data, 'rgb', None) is not None:\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                name='RGB',\n",
    "                x=data.pos[:, 0],\n",
    "                y=data.pos[:, 1],\n",
    "                z=data.pos[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=pointsize,\n",
    "                    color=rgb_to_plotly_rgb(data.rgb), ),\n",
    "                hoverinfo='x+y+z',\n",
    "                showlegend=False,\n",
    "                visible=not initialized_visibility, ))\n",
    "        modes['name'].append('RGB')\n",
    "        modes['key'].append('rgb')\n",
    "        modes['num_traces'].append(1)\n",
    "        initialized_visibility = True\n",
    "\n",
    "    # Draw a trace for labeled 3D point cloud\n",
    "    if getattr(data, 'y', None) is not None:\n",
    "        \n",
    "        # If labels are expressed as histograms, keep the most frequent\n",
    "        # one\n",
    "        if data.y.dim() == 2:\n",
    "            data.y = data.y.argmax(1)\n",
    "        \n",
    "        y = data.y.numpy()\n",
    "        n_y_traces = 0\n",
    "\n",
    "        for label in np.unique(y):\n",
    "            indices = np.where(y == label)[0]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    name=class_names[label] if class_names else f\"Class {label}\",\n",
    "                    opacity=class_opacities[label] if class_opacities else 1.0,\n",
    "                    x=data.pos[indices, 0],\n",
    "                    y=data.pos[indices, 1],\n",
    "                    z=data.pos[indices, 2],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=pointsize,\n",
    "                        color=class_colors[label] if class_colors else None, ),\n",
    "                    visible=not initialized_visibility, ))\n",
    "            n_y_traces += 1  # keep track of the number of traces\n",
    "\n",
    "        modes['name'].append('Labels')\n",
    "        modes['key'].append('y')\n",
    "        modes['num_traces'].append(n_y_traces)\n",
    "        initialized_visibility = True\n",
    "\n",
    "    # Draw a trace for predicted labels 3D point cloud\n",
    "    if getattr(data, 'pred', None) is not None:\n",
    "        pred = data.pred.numpy()\n",
    "        n_pred_traces = 0\n",
    "\n",
    "        for label in np.unique(pred):\n",
    "            indices = np.where(pred == label)[0]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    name=class_names[label] if class_names else f\"Class {label}\",\n",
    "                    opacity=class_opacities[label] if class_opacities else 1.0,\n",
    "                    x=data.pos[indices, 0],\n",
    "                    y=data.pos[indices, 1],\n",
    "                    z=data.pos[indices, 2],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=pointsize,\n",
    "                        color=class_colors[label] if class_colors else None, ),\n",
    "                    visible=not initialized_visibility, ))\n",
    "            n_pred_traces += 1  # keep track of the number of traces\n",
    "\n",
    "        modes['name'].append('Predictions')\n",
    "        modes['key'].append('pred')\n",
    "        modes['num_traces'].append(n_pred_traces)\n",
    "        initialized_visibility = True\n",
    "    \n",
    "    # Draw a trace for position-colored 3D point cloud\n",
    "    # radius = torch.norm(data.pos - data.pos.mean(dim=0), dim=1).max()\n",
    "    # data.pos_rgb = (data.pos - data.pos.mean(dim=0)) / (2 * radius) + 0.5\n",
    "    mini = data.pos.min(dim=0).values\n",
    "    maxi = data.pos.max(dim=0).values\n",
    "    data.pos_rgb = (data.pos - mini) / (maxi - mini + 1e-6)\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            name='Position RGB',\n",
    "            x=data.pos[:, 0],\n",
    "            y=data.pos[:, 1],\n",
    "            z=data.pos[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=pointsize,\n",
    "                color=rgb_to_plotly_rgb(data.pos_rgb), ),\n",
    "            hoverinfo='x+y+z',\n",
    "            showlegend=False,\n",
    "            visible=not initialized_visibility, ))\n",
    "    modes['name'].append('Position RGB')\n",
    "    modes['key'].append('position_rgb')\n",
    "    modes['num_traces'].append(1)\n",
    "    initialized_visibility = True\n",
    "    \n",
    "    # Draw a trace for the partition\n",
    "    if has_partition:\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                name='Superpoints',\n",
    "                x=data.pos[:, 0],\n",
    "                y=data.pos[:, 1],\n",
    "                z=data.pos[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=pointsize,\n",
    "                    color=PALETTE[data.p2c % len(PALETTE)], ),\n",
    "                hoverinfo='x+y+z',\n",
    "                showlegend=False,\n",
    "                visible=not initialized_visibility, ))\n",
    "        modes['name'].append('Superpoints')\n",
    "        modes['key'].append('superpoints')\n",
    "        modes['num_traces'].append(1)\n",
    "        initialized_visibility = True\n",
    "    \n",
    "    # Draw a trace for 3D point cloud features\n",
    "    if getattr(data, 'x', None) is not None:\n",
    "        # Recover the features and convert them to an RGB format for\n",
    "        # visualization.\n",
    "        data.feat_3d = feats_to_rgb(data.x, normalize=True)\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                name='Features 3D',\n",
    "                x=data.pos[:, 0],\n",
    "                y=data.pos[:, 1],\n",
    "                z=data.pos[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=pointsize,\n",
    "                    color=rgb_to_plotly_rgb(data.feat_3d), ),\n",
    "                hoverinfo='x+y+z',\n",
    "                showlegend=False,\n",
    "                visible=not initialized_visibility, ))\n",
    "        modes['name'].append('Features 3D')\n",
    "        modes['key'].append('x')\n",
    "        modes['num_traces'].append(1)\n",
    "        initialized_visibility = True\n",
    "    \n",
    "    # Add a trace for prediction errors\n",
    "    has_error = getattr(data, 'y', None) is not None \\\n",
    "                and getattr(data, 'pred', None) is not None\n",
    "    if has_error:\n",
    "        indices = np.where(data.pred.numpy() != data.y.numpy())[0]\n",
    "        error_color = f\"rgb{tuple(error_color)}\" \\\n",
    "            if error_color is not None else 'rgb(255, 0, 0)'\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                name='Errors',\n",
    "                opacity=1.0,\n",
    "                x=data.pos[indices, 0],\n",
    "                y=data.pos[indices, 1],\n",
    "                z=data.pos[indices, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=pointsize,\n",
    "                    color=error_color, ),\n",
    "                showlegend=False,\n",
    "                visible=False, ))\n",
    "        modes['name'].append('Errors')\n",
    "        modes['key'].append('error')\n",
    "        modes['num_traces'].append(1)\n",
    "        \n",
    "    # Draw cluster centroid positions\n",
    "    if has_partition:\n",
    "        idx_sp = np.arange(data_c.num_nodes)\n",
    "        sp_traces = []\n",
    "        sp_traces.append(len(fig.data))\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                name=f\"Superpoint centroids\",\n",
    "                x=data_c.pos[:, 0],\n",
    "                y=data_c.pos[:, 1],\n",
    "                z=data_c.pos[:, 2],\n",
    "                mode='markers+text',\n",
    "                marker=dict(\n",
    "                    symbol='diamond',\n",
    "                    line_width=2,\n",
    "                    size=pointsize + 2,\n",
    "                    color=PALETTE[idx_sp % len(PALETTE)], ),\n",
    "                text=[f\"<b>{i}</b>\" for i in idx_sp] if show_superpoint_number else '',\n",
    "                textposition=\"bottom center\",\n",
    "                textfont=dict(size=16),\n",
    "                hoverinfo='x+y+z+name',\n",
    "                showlegend=False,\n",
    "                visible=True, ))\n",
    "    \n",
    "    # Traces visibility for interactive point cloud coloring\n",
    "    def trace_visibility(mode):\n",
    "        visibilities = np.array([d.visible for d in fig.data], dtype='bool')\n",
    "\n",
    "        # Traces visibility for interactive point cloud coloring\n",
    "        i_mode = modes['key'].index(mode)\n",
    "        a = sum(modes['num_traces'][:i_mode])\n",
    "        b = sum(modes['num_traces'][:i_mode + 1])\n",
    "        n_traces = sum(modes['num_traces'])\n",
    "\n",
    "        visibilities[:n_traces] = False\n",
    "        visibilities[a:b] = True\n",
    "\n",
    "        return [{\"visible\": visibilities.tolist()}]\n",
    "\n",
    "    # Create the buttons that will serve for toggling trace visibility\n",
    "    updatemenus = [\n",
    "        dict(\n",
    "            buttons=[dict(label=name, method='update', args=trace_visibility(key))\n",
    "                     for name, key in zip(modes['name'], modes['key']) if key != 'error'],\n",
    "            pad={'r': 10, 't': 10},\n",
    "            showactive=True,\n",
    "            type='dropdown',\n",
    "            direction='right',\n",
    "            xanchor='left',\n",
    "            x=0.02,\n",
    "            yanchor='top',\n",
    "            y=1.02, ),\n",
    "    ]\n",
    "    \n",
    "    if has_error:\n",
    "        updatemenus.append(\n",
    "            dict(\n",
    "                buttons=[dict(\n",
    "                    method='restyle',\n",
    "                    label='Error',\n",
    "                    visible=True,\n",
    "                    args=[{'visible': True, },\n",
    "                          [sum(modes['num_traces'][:modes['key'].index('error')])]],\n",
    "                    args2=[{'visible': False, },\n",
    "                           [sum(modes['num_traces'][:modes['key'].index('error')])]], )],\n",
    "                pad={'r': 10, 't': 10},\n",
    "                showactive=False,\n",
    "                type='buttons',\n",
    "                xanchor='left',\n",
    "                x=1.02,\n",
    "                yanchor='top',\n",
    "                y=1.02, ),\n",
    "        )\n",
    "    fig.update_layout(updatemenus=updatemenus)\n",
    "\n",
    "    # Place the legend on the left\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            yanchor=\"middle\",\n",
    "            y=0.5,\n",
    "            xanchor=\"right\",\n",
    "            x=0.99))\n",
    "\n",
    "    # Hide all axes and no background\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='',\n",
    "            yaxis_title='',\n",
    "            zaxis_title='',\n",
    "            xaxis=dict(\n",
    "                autorange=True,\n",
    "                showgrid=False,\n",
    "                ticks='',\n",
    "                showticklabels=False,\n",
    "                backgroundcolor=\"rgba(0, 0, 0, 0)\"\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                autorange=True,\n",
    "                showgrid=False,\n",
    "                ticks='',\n",
    "                showticklabels=False,\n",
    "                backgroundcolor=\"rgba(0, 0, 0, 0)\"\n",
    "            ),\n",
    "            zaxis=dict(\n",
    "                autorange=True,\n",
    "                showgrid=False,\n",
    "                ticks='',\n",
    "                showticklabels=False,\n",
    "                backgroundcolor=\"rgba(0, 0, 0, 0)\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    output = {'figure': fig, 'data': data}\n",
    "    \n",
    "    if has_partition:\n",
    "        output['sp_traces'] = sp_traces\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def show(\n",
    "        data, data_c, path=None, title=None, no_output=True, **kwargs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    assert isinstance(data, Data)\n",
    "    assert isinstance(data_c, Data)\n",
    "\n",
    "    # Sanitize title and path\n",
    "    if title is None:\n",
    "        title = \"Multimodal data\"\n",
    "    if path is not None:\n",
    "        if osp.isdir(path):\n",
    "            path = osp.join(path, f\"{title}.html\")\n",
    "        else:\n",
    "            path = osp.splitext(path)[0] + '.html'\n",
    "        fig_html = f'<h1 style=\"text-align: center;\">{title}</h1>'\n",
    "\n",
    "    # Draw a figure for 3D data visualization\n",
    "    out_3d = visualize_3d(data, data_c, **kwargs)\n",
    "    if no_output:\n",
    "        if path is None:\n",
    "            out_3d['figure'].show(config={'displayModeBar': False})\n",
    "        else:\n",
    "            fig_html += figure_html(out_3d['figure'])\n",
    "\n",
    "    if path is not None:\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(fig_html)\n",
    "\n",
    "    if not no_output:\n",
    "        return out_3d\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f472969-deff-44a9-add0-3294812967ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "temp_dir = DATA_ROOT + '/datasets/kitti360/shared/temp' \n",
    "data, data_c = torch.load(os.path.join(temp_dir, 'preliminaries.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558168c-9f5d-48c1-bfd0-f82f5c435928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superpoint_transformer.datasets.kitti360 import CLASS_NAMES, CLASS_COLORS\n",
    "    \n",
    "show(\n",
    "    data, data_c, figsize=1000, width=None, height=None, class_names=CLASS_NAMES,\n",
    "    class_colors=CLASS_COLORS, class_opacities=None, voxel=0.1, max_points=500000,\n",
    "    pointsize=3, error_color=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df07845-c9f6-4ecc-ace9-2adf6c4256ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a2598a4f-e6fe-4b86-92d7-0a7bc2925b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2478764"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.p2c.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce487c-44fc-4a58-807e-6a20b1366d65",
   "metadata": {},
   "source": [
    "questions\n",
    "- give courses for post doc ETH ?\n",
    "- paper says Z normalized over point cloud... but code seems to just take z*2 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4539ab-8b42-45c4-be14-e6888b0f3339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1398492c-b3e6-40d4-a220-f9d2147f4918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 20, 20, 30, 30, 30, 40, 40, 40, 40, 50, 50, 50, 50, 50, 60, 60, 60,\n",
      "        60, 60, 60, 70, 70, 70, 70, 70, 70, 70, 80, 80, 80, 80, 80, 80, 80, 80,\n",
      "        90, 90, 90, 90, 90, 90, 90, 90, 90])\n",
      "tensor([0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6,\n",
      "        6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "tensor([ 0,  2,  5,  9, 14, 20, 27, 35, 44])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.pool.consecutive import consecutive_cluster\n",
    "\n",
    "a = torch.arange(10).repeat_interleave(torch.arange(10)) * 10\n",
    "print(a)\n",
    "\n",
    "b, perm = consecutive_cluster(a)\n",
    "print(b)\n",
    "print(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "169e226a-f4d4-48a8-99f5-722c5ab14fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape == a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4d1329f-1245-4f44-8a35-aee869aa516a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10, 20, 30, 40, 50, 60, 70, 80, 90]),\n",
       " tensor([0, 1, 2, 3, 4, 5, 6, 7, 8]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[perm], b[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2eaadd-4bbf-4e2b-89b3-a9979af47e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ee49883e-8ffd-4f10-be81-fa65bca8d493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 1, 0, 2])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.arange(10)\n",
    "idx = torch.LongTensor([5, 2, 7, 1])\n",
    "\n",
    "# Get view-level indices for images to keep\n",
    "view_idx = torch.where((values[..., None] == idx).any(-1))[0]\n",
    "\n",
    "# Index the values\n",
    "values = values[view_idx]\n",
    "\n",
    "# Update the image indices. To do so, create a tensor of indices\n",
    "# idx_gen so that the desired output can be computed with simple\n",
    "# indexation idx_gen[images]. This avoids using map() or\n",
    "# numpy.vectorize alternatives.\n",
    "idx_gen = torch.full(\n",
    "    (idx.max() + 1,), -1, dtype=torch.int64, )\n",
    "idx_gen = idx_gen.scatter_(\n",
    "    0, idx, torch.arange(idx.shape[0], ))\n",
    "idx_gen[values]  # values[0] holds image indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6a273fbf-985d-442e-bda0-0c4eac2f4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, i = torch.randint(3, (10,)).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3df02072-f316-48f9-85cf-0c1591fcdf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).equal(torch.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "18a1bcb4-3338-4e7f-8c32-6339b28793b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).flip(0).sort().values.equal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a02cfe4d-ff57-41c7-8d99-8b35a5f3f15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24787640"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "05d4c9a4-33d1-4a7a-814a-82c602653265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4 50 60 70 80 90] [50 60 70 80 90]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(10)\n",
    "b = a[5:]\n",
    "b *= 10\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953324a-d40f-498d-997f-8eb199f11f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spt] *",
   "language": "python",
   "name": "conda-env-spt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
