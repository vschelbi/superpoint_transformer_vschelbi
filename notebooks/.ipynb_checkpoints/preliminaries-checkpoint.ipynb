{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea55bdc-f415-4f5c-9563-303ed17cbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "import matplotlib.image as mpimg\n",
    "import argparse\n",
    "import ast\n",
    "\n",
    "# file_path = os.path.dirname(os.path.abspath(__file__)) # this is for the .py script but does not work in a notebook\n",
    "file_path = os.path.dirname(os.path.abspath(''))\n",
    "sys.path.append(file_path)\n",
    "# sys.path.append(os.path.join(file_path, \"grid-graph/python/bin\"))\n",
    "# sys.path.append(os.path.join(file_path, \"parallel-cut-pursuit/python/wrappers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60125085-189f-4172-a95b-46fb07a6f970",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a15d1e7-b767-4f28-b22d-80dfecd266aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data 1/342: 0.086s\n",
      "Number of loaded points: 3201318 (3.00M)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import glob\n",
    "from torch_geometric.data import Data\n",
    "from superpoint_transformer.datasets.kitti360 import read_kitti360_window\n",
    "from superpoint_transformer.datasets.kitti360_config import KITTI360_NUM_CLASSES\n",
    "\n",
    "i_window = 0\n",
    "all_filepaths = sorted(glob.glob('/media/drobert-admin/DATA2/datasets/kitti360/shared/data_3d_semantics/*/static/*.ply'))\n",
    "filepath = all_filepaths[i_window]\n",
    "\n",
    "start = time()\n",
    "data = read_kitti360_window(filepath, semantic=True, instance=False, remap=True)\n",
    "print(f'Loading data {i_window+1}/{len(all_filepaths)}: {time() - start:0.3f}s')\n",
    "print(f'Number of loaded points: {data.num_nodes} ({data.num_nodes // 10**6:0.2f}M)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91920327-6541-4ef6-a9c3-503023654f3a",
   "metadata": {},
   "source": [
    "# Voxelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197855e3-11ee-43f1-b79d-afbd1713a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b243bd8d-4ed2-45ec-844b-1a58d134815c",
   "metadata": {},
   "source": [
    "### Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f599db-b9c5-4e93-b4d3-2568c7216dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 0.157s\n",
      "Number of loaded points: 3201318 (3.00M, 1.0%)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.pool import voxel_grid\n",
    "\n",
    "start = time()\n",
    "data_sub = voxel_grid(data.pos, size=0.1)\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data_sub.shape[0]} ({data_sub.shape[0] // 10**6:0.2f}M, {100 * data_sub.shape[0] / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c89c00-81ee-4f91-b620-13acfbdbd82a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TorchPoints3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b396ee7-b7b3-4c9f-9605-93dcf3d9ebb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from torch_geometric.nn.pool import voxel_grid\n",
    "from torch_cluster import grid_cluster\n",
    "from torch_scatter import scatter_mean, scatter_add\n",
    "from torch_geometric.nn.pool.consecutive import consecutive_cluster\n",
    "MAPPING_KEY = 'mapping_index'\n",
    "\n",
    "# Label will be the majority label in each voxel\n",
    "_INTEGER_LABEL_KEYS = [\"y\", \"instance_labels\"]\n",
    "\n",
    "def group_data(data, cluster=None, unique_pos_indices=None, mode=\"last\", skip_keys=[]):\n",
    "    \"\"\" Group data based on indices in cluster.\n",
    "    The option ``mode`` controls how data gets agregated within each cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Data\n",
    "        [description]\n",
    "    cluster : torch.Tensor\n",
    "        Tensor of the same size as the number of points in data. Each element is the cluster index of that point.\n",
    "    unique_pos_indices : torch.tensor\n",
    "        Tensor containing one index per cluster, this index will be used to select features and labels\n",
    "    mode : str\n",
    "        Option to select how the features and labels for each voxel is computed. Can be ``last`` or ``mean``.\n",
    "        ``last`` selects the last point falling in a voxel as the representent, ``mean`` takes the average.\n",
    "    skip_keys: list\n",
    "        Keys of attributes to skip in the grouping\n",
    "    \"\"\"\n",
    "\n",
    "    assert mode in [\"mean\", \"last\"]\n",
    "    if mode == \"mean\" and cluster is None:\n",
    "        raise ValueError(\"In mean mode the cluster argument needs to be specified\")\n",
    "    if mode == \"last\" and unique_pos_indices is None:\n",
    "        raise ValueError(\"In last mode the unique_pos_indices argument needs to be specified\")\n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    for key, item in data:\n",
    "        if bool(re.search(\"edge\", key)):\n",
    "            raise ValueError(\"Edges not supported. Wrong data type.\")\n",
    "        if key in skip_keys:\n",
    "            continue\n",
    "\n",
    "        if torch.is_tensor(item) and item.size(0) == num_nodes:\n",
    "            if mode == \"last\" or key == \"batch\" \\\n",
    "                    or key == SaveOriginalPosId.KEY \\\n",
    "                    or key == MAPPING_KEY:\n",
    "                data[key] = item[unique_pos_indices]\n",
    "            elif mode == \"mean\":\n",
    "                is_item_bool = item.dtype == torch.bool\n",
    "                if is_item_bool:\n",
    "                    item = item.int()\n",
    "                if key in _INTEGER_LABEL_KEYS:\n",
    "                    item_min = item.min()\n",
    "                    item = torch.nn.functional.one_hot(item - item_min)\n",
    "                    item = scatter_add(item, cluster, dim=0)\n",
    "                    data[key] = item.argmax(dim=-1) + item_min\n",
    "                else:\n",
    "                    data[key] = scatter_mean(item, cluster, dim=0)\n",
    "                if is_item_bool:\n",
    "                    data[key] = data[key].bool()\n",
    "    return data\n",
    "\n",
    "class SaveOriginalPosId:\n",
    "    \"\"\" Transform that adds the index of the point to the data object\n",
    "    This allows us to track this point from the output back to the input data object\n",
    "    \"\"\"\n",
    "\n",
    "    KEY = \"origin_id\"\n",
    "\n",
    "    def __init__(self, key=None):\n",
    "        self.KEY = key if key is not None else self.KEY\n",
    "\n",
    "    def _process(self, data):\n",
    "        if hasattr(data, self.KEY):\n",
    "            return data\n",
    "\n",
    "        setattr(data, self.KEY, torch.arange(0, data.pos.shape[0]))\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in data]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "class GridSampling3D:\n",
    "    \"\"\" Clusters points into voxels with size :attr:`size`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    size: float\n",
    "        Size of a voxel (in each dimension).\n",
    "    quantize_coords: bool\n",
    "        If True, it will convert the points into their associated sparse\n",
    "        coordinates within the grid and store the value into a new\n",
    "        `coords` attribute.\n",
    "    mode: string:\n",
    "        The mode can be either `last` or `mean`.\n",
    "        If mode is `mean`, all the points and their features within a\n",
    "        cell will be averaged. If mode is `last`, one random points per\n",
    "        cell will be selected with its associated features.\n",
    "    setattr_full_pos: bool\n",
    "        If True, the input point positions will be saved into a new\n",
    "        'full_pos' attribute. This memory-costly step may reveal\n",
    "        necessary for subsequent local feature computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, quantize_coords=False, mode=\"mean\", verbose=False,\n",
    "                 setattr_full_pos=False):\n",
    "        self._grid_size = size\n",
    "        self._quantize_coords = quantize_coords\n",
    "        self._mode = mode\n",
    "        self._setattr_full_pos = setattr_full_pos\n",
    "        if verbose:\n",
    "            log.warning(\n",
    "                \"If you need to keep track of the position of your points, use \"\n",
    "                \"SaveOriginalPosId transform before using GridSampling3D.\")\n",
    "\n",
    "            if self._mode == \"last\":\n",
    "                log.warning(\n",
    "                    \"The tensors within data will be shuffled each time this \"\n",
    "                    \"transform is applied. Be careful that if an attribute \"\n",
    "                    \"doesn't have the size of num_points, it won't be shuffled\")\n",
    "\n",
    "    def _process(self, data):\n",
    "        if self._mode == \"last\":\n",
    "            data = shuffle_data(data)\n",
    "\n",
    "        full_pos = data.pos\n",
    "        coords = torch.round((data.pos) / self._grid_size)\n",
    "        if \"batch\" not in data:\n",
    "            cluster = grid_cluster(coords, torch.tensor([1, 1, 1]))\n",
    "        else:\n",
    "            cluster = voxel_grid(coords, data.batch, 1)\n",
    "        cluster, unique_pos_indices = consecutive_cluster(cluster)\n",
    "\n",
    "        data = group_data(data, cluster, unique_pos_indices, mode=self._mode)\n",
    "        if self._quantize_coords:\n",
    "            data.coords = coords[unique_pos_indices].int()\n",
    "\n",
    "        data.grid_size = torch.tensor([self._grid_size])\n",
    "\n",
    "        # Keep track of the initial full-resolution point cloud for\n",
    "        # later use. Typically needed for local features computation.\n",
    "        # However, for obvious memory-wary considerations, it is\n",
    "        # recommended to delete the 'full_pos' attribute as soon as it\n",
    "        # is no longer needed.\n",
    "        if self._setattr_full_pos:\n",
    "            data.full_pos = full_pos\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, list):\n",
    "            data = [self._process(d) for d in data]\n",
    "        else:\n",
    "            data = self._process(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}(grid_size={}, quantize_coords={}, mode={})\".format(\n",
    "            self.__class__.__name__, self._grid_size, self._quantize_coords, self._mode\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8daf6b4d-a4cc-440a-96f4-539e51899066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data  voxelization at 0.05m: 2.496s\n",
      "Number of loaded points: 2480151 (2.00M, 1.0%)\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "data_sub = GridSampling3D(size=voxel)(data)\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {data_sub.num_nodes} ({data_sub.num_nodes // 10**6:0.2f}M, {100 * data_sub.num_nodes / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd9504-ee3f-4d55-92cc-de76f11769a3",
   "metadata": {},
   "source": [
    "### Loïc's C implem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f788d00b-6ddf-4f61-b708-41edc2c2669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "======== pruning ========\n",
      "Data  voxelization at 0.05m: 7.412s\n",
      "Number of sampled points: 2479935 (2.00M, 0.8%)\n",
      "=========================\n",
      "Voxelization into 3612 x 3216 x 335 grid\n",
      "Reduced from 3201318 to 2479935 points (77.46%)\n"
     ]
    }
   ],
   "source": [
    "import superpoint_transformer.partition.utils.libpoint_utils as point_utils\n",
    "\n",
    "# WARNING: the pruning must know the number of classes. All labels are \n",
    "# offset to account for the -1 unlabeled points !\n",
    "start = time()\n",
    "xyz, rgb, labels, dump = point_utils.prune(data.pos.float().numpy(), voxel, (data.rgb * 255).byte().numpy(), data.y.byte().numpy() + 1, np.zeros(1, dtype='uint8'), KITTI360_NUM_CLASSES + 1, 0)\n",
    "print(f'Data  voxelization at {voxel}m: {time() - start:0.3f}s')\n",
    "print(f'Number of sampled points: {xyz.shape[0]} ({xyz.shape[0] // 10**6:0.2f}M, {100 * xyz.shape[0] / data.num_nodes:0.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88b8d626-8fef-408c-a4b5-3bda999e7708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.774660624155426"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyz.shape[0] / data.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa89e7-8677-415b-ab58-29ddf027c699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb5154-6cd9-424a-a44f-b84ac4e3db4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcd1154d-83fe-4ca7-96d7-b75aa05f2d50",
   "metadata": {},
   "source": [
    "# Neighbour search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9fe250a-43b1-4ded-b3f0-8de9b288adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.pos\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9cf13-0001-4836-9dc7-3cddb0a896aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2f1226-bc57-45af-9d0a-0d3b8ca6c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 15.029s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "start = time()\n",
    "kdt = KDTree(x.numpy(), leaf_size=30, metric='euclidean')\n",
    "neighbors = kdt.query(x.numpy(), k=k, return_distance=False)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d1cf9-5120-45d7-b542-9121a7e78e1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FAISS-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c50e50-ed80-4b20-af5c-3420dfe0fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "def find_neighbours(x, y, k=10, ncells=None, nprobes=10):\n",
    "    # if batch_x is not None or batch_y is not None:\n",
    "    #     raise NotImplementedError(\n",
    "    #         \"FAISSGPUKNNNeighbourFinder does not support batches yet\")\n",
    "\n",
    "    x = x.view(-1, 1) if x.dim() == 1 else x\n",
    "    y = y.view(-1, 1) if y.dim() == 1 else y\n",
    "    x, y = x.contiguous(), y.contiguous()\n",
    "\n",
    "    # FAISS-GPU consumes numpy arrays\n",
    "    x_np = x.cpu().numpy()\n",
    "    y_np = y.cpu().numpy()\n",
    "\n",
    "    # Initialization\n",
    "    n_fit = x_np.shape[0]\n",
    "    d = x_np.shape[1]\n",
    "    gpu = faiss.StandardGpuResources()\n",
    "\n",
    "    # Heuristics to prevent k from being too large\n",
    "    k_max = 1024\n",
    "    k = min(k, n_fit, k_max)\n",
    "\n",
    "    # Heuristic to parameterize the number of cells for FAISS index,\n",
    "    # if not provided\n",
    "    if ncells is None:\n",
    "        f1 = 3.5 * np.sqrt(n_fit)\n",
    "        f2 = 1.6 * np.sqrt(n_fit)\n",
    "        if n_fit > 2 * 10 ** 6:\n",
    "            p = 1 / (1 + np.exp(2 * 10 ** 6 - n_fit))\n",
    "        else:\n",
    "            p = 0\n",
    "        ncells = int(p * f1 + (1 - p) * f2)\n",
    "\n",
    "    # Building a GPU IVFFlat index + Flat quantizer\n",
    "    torch.cuda.empty_cache()\n",
    "    quantizer = faiss.IndexFlatL2(d)  # the quantizer index\n",
    "    index = faiss.IndexIVFFlat(quantizer, d, ncells, faiss.METRIC_L2)  # the main index\n",
    "    gpu_index_flat = faiss.index_cpu_to_gpu(gpu, 0, index)  # pass index it to GPU\n",
    "    gpu_index_flat.train(x_np)  # fit the cells to the training set distribution\n",
    "    gpu_index_flat.add(x_np)\n",
    "\n",
    "    # Querying the K-NN\n",
    "    gpu_index_flat.setNumProbes(nprobes)\n",
    "    return torch.LongTensor(gpu_index_flat.search(y_np, k)[1]).to(x.device)\n",
    "\n",
    "start = time()\n",
    "out = find_neighbours(x, x, k=k, ncells=None, nprobes=10)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64251d5-1bc9-48b2-996a-8e4f16af9ec4",
   "metadata": {},
   "source": [
    "### PyKeOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96a27be1-ae38-47c3-bd86-8a9c0847f3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 39.054s\n"
     ]
    }
   ],
   "source": [
    "from pykeops.torch import LazyTensor\n",
    "\n",
    "start = time()\n",
    "# K-NN search with KeOps. If the number of points is greater\n",
    "# than 16 millions, KeOps requires double precision.\n",
    "xyz_query = x.contiguous()\n",
    "xyz_search = x.contiguous()\n",
    "if xyz_search.shape[0] > 1.6e7:\n",
    "    xyz_query_keops = LazyTensor(xyz_query[:, None, :].double())\n",
    "    xyz_search_keops = LazyTensor(xyz_search[None, :, :].double())\n",
    "else:\n",
    "    xyz_query_keops = LazyTensor(xyz_query[:, None, :])\n",
    "    xyz_search_keops = LazyTensor(xyz_search[None, :, :])\n",
    "d_keops = ((xyz_query_keops - xyz_search_keops) ** 2).sum(dim=2)\n",
    "neighbors = d_keops.argKmin(k, dim=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f2b55-d631-4462-a002-19ef72890144",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FLANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68542da-735f-4ec5-9d05-1eb978297738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Cannot load dynamic library. Did you compile FLANN?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyflann\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m flann \u001b[38;5;241m=\u001b[39m pyflann\u001b[38;5;241m.\u001b[39mFLANN()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/__init__.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load, save\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/index.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2010  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2010  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbindings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflann_ctypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_rn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/bindings/__init__.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  Marius Muja (mariusm@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Copyright 2008-2009  David G. Lowe (lowe@cs.ubc.ca). All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#from pyflann_parameters import parameter_list, algorithm_names\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#from pyflann_parameters import centers_init_names, log_level_names\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflann_ctypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyflann/bindings/flann_ctypes.py:173\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m flannlib \u001b[38;5;241m=\u001b[39m load_flann_library()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flannlib \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot load dynamic library. Did you compile FLANN?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFlannLib\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Cannot load dynamic library. Did you compile FLANN?"
     ]
    }
   ],
   "source": [
    "import pyflann\n",
    "\n",
    "start = time()\n",
    "flann = pyflann.FLANN()\n",
    "result, dists = flann.nn(x.numpy(), x.numpy(), k, algorithm=\"kmeans\", branching=32, iterations=7, checks=16)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0697c4b-15ae-42c8-b174-1c4135a3b83c",
   "metadata": {},
   "source": [
    "## Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36414778-4228-4107-841c-c1e381684775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.466s\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import knn\n",
    "\n",
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23c967fe-b273-4717-b5be-ee20ac3b50be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.215s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=2)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8abf861-b022-4f2e-aef0-3b1cd09326d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.076s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=4)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df017adf-57eb-4275-9a6d-8cfff59742af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor search: 9.438s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "out = knn(x, x, k, batch_x=None, batch_y=None, num_workers=8)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d3b44-456c-407c-b5d5-abc2cf4fc534",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cuda = x.cuda()\n",
    "start = time()\n",
    "out = knn(x_cuda, x_cuda, k, batch_x=None, batch_y=None, num_workers=1)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')\n",
    "del x_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c7676-bc84-4464-a12c-ee992985c660",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### GriSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea9ee609-fd8d-4944-a52b-d8052fc98f97",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m grid \u001b[38;5;241m=\u001b[39m gsp\u001b[38;5;241m.\u001b[39mGriSPy(x\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m----> 5\u001b[0m dist, ind \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnearest_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeighbor search: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime() \u001b[38;5;241m-\u001b[39m start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:1206\u001b[0m, in \u001b[0;36mGriSPy.nearest_neighbors\u001b[0;34m(self, centres, n, kind)\u001b[0m\n\u001b[1;32m   1204\u001b[0m neighbors_distances \u001b[38;5;241m=\u001b[39m [EMPTY_ARRAY\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_centres)]\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(n_found):\n\u001b[0;32m-> 1206\u001b[0m     ndis_tmp, nidx_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell_neighbors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcentres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower_distance_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupper_distance_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mn_found\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i_tmp, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(centres_lookup_ind[\u001b[38;5;241m~\u001b[39mn_found]):\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(nidx_tmp[i_tmp]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(neighbors_indices[i]):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:1089\u001b[0m, in \u001b[0;36mGriSPy.shell_neighbors\u001b[0;34m(self, centres, distance_lower_bound, distance_upper_bound, sorted, kind)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     vlds\u001b[38;5;241m.\u001b[39mvalidate_equalsize(centres, distance_upper_bound)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# Get neighbors\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m neighbor_cells \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_neighbor_cells\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentres\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistance_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1096\u001b[0m neighbors_distances, neighbors_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_neighbor_distance(\n\u001b[1;32m   1097\u001b[0m     centres, neighbor_cells\n\u001b[1;32m   1098\u001b[0m )\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;66;03m# We need to generate mirror centres for periodic boundaries...\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/grispy/core.py:798\u001b[0m, in \u001b[0;36mGriSPy._get_neighbor_cells\u001b[0;34m(self, centres, distance_upper_bound, distance_lower_bound, shell_flag)\u001b[0m\n\u001b[1;32m    792\u001b[0m k_grids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    793\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(k_cell_min[i, k], k_cell_max[i, k] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m    795\u001b[0m ]\n\u001b[1;32m    796\u001b[0m k_grids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(\u001b[38;5;241m*\u001b[39mk_grids)\n\u001b[1;32m    797\u001b[0m neighbor_cells \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 798\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_grids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    799\u001b[0m ]\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Calculo la distancia de cada centro i a sus celdas vecinas,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# luego descarto las celdas que no toca el circulo definido por\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# la distancia\u001b[39;00m\n\u001b[1;32m    804\u001b[0m cells_physical \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_bins[neighbor_cells[i][:, k], k] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m cell_size[k]\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m    807\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import grispy as gsp\n",
    "\n",
    "start = time()\n",
    "grid = gsp.GriSPy(x.numpy())\n",
    "dist, ind = grid.nearest_neighbors(x.numpy(), n=k)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21847b0a-38e3-4a40-8e5c-fe8c889a5912",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FRNN\n",
    "https://github.com/lxxue/FRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39da9baf-6bd9-4b97-b7dc-56a2542595f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_C' from partially initialized module 'frnn' (most likely due to a circular import) (/home/ign.fr/drobert-admin/projects/FRNN/frnn/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(file_path), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFRNN\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfrnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# first time there is no cached grid\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/FRNN/frnn/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from .frnn import frnn_grid_points, frnn_grid_points_with_timing, _C\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frnn_grid_points, frnn_gather, frnn_bf_points, _C\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[0;32m~/projects/FRNN/frnn/frnn.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Union\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _C\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Function\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m once_differentiable\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_C' from partially initialized module 'frnn' (most likely due to a circular import) (/home/ign.fr/drobert-admin/projects/FRNN/frnn/__init__.py)"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.join(os.path.dirname(file_path), \"FRNN\"))\n",
    "import frnn\n",
    "\n",
    "start = time()\n",
    "# first time there is no cached grid\n",
    "dists, idxs, nn, grid = frnn.frnn_grid_points(x.view(1, -1, 3), x.view(1, -1, 3), None, None, k, -1, grid=None, return_nn=False, return_sorted=True)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')\n",
    "\n",
    "\n",
    "# # if points2 and r don't change, we can reuse the grid\n",
    "# dists, idxs, nn, grid = frnn.frnn_grid_points(\n",
    "#     points1, points2, lengths1, lengths2, K, r, grid=grid, return_nn=False, return_sorted=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b83d5-26d9-483a-8af6-e82aa12dc219",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pytorch3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da3c782a-6ded-4fa8-99bc-97abf128b634",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch3d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytorch3d\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m      4\u001b[0m pytorch3d\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mknn_points(x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), K\u001b[38;5;241m=\u001b[39mk)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch3d'"
     ]
    }
   ],
   "source": [
    "import pytorch3d\n",
    "\n",
    "start = time()\n",
    "pytorch3d.ops.knn_points(x.view(1, -1, 3), x.view(1, -1, 3), K=k)\n",
    "print(f'Neighbor search: {time() - start:0.3f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99aa378-67a1-4f67-8a91-15f312ac4a09",
   "metadata": {},
   "source": [
    "# Graph computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87298010-2690-45b5-831b-f6d97fe8700c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d26d4c-9013-45b1-8c27-2bba27586b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCP computation\n",
    "components2, in_component2, tracks_single = pcp(features, graph_nn, args.reg_strength, 10)\n",
    "# en sachant que pour graph_nn tu n'as besoin que de graph_nn[\"source\"], graph_nn[\"target\"] et graph_nn[\"edge_weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1818ebfa-f384-411a-9bb5-1a6b139ccff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c19242-1669-4a10-bbed-392baee2a112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bad16898-7426-484e-825f-e6ecff4839ca",
   "metadata": {},
   "source": [
    "# Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4bcc99-e8c1-4b4d-87cf-ed3c4392a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_graph import edge_list_to_forward_star\n",
    "from cp_kmpp_d0_dist import cp_kmpp_d0_dist\n",
    "\n",
    "def pcp(features, graph_nn, reg_strength, cutoff, parallel=True, return_intermediate=False, balance=True):\n",
    "    \"\"\"\n",
    "    parallel cut pursuit\n",
    "    \"\"\"\n",
    "    # Convert to forward-star graph representation\n",
    "    first_edge, adj_vertices, reindex = edge_list_to_forward_star(\n",
    "        features.shape[0], np.concatenate((graph_nn[\"source\"][:, None], \n",
    "        graph_nn[\"target\"][:, None]), 1))\n",
    "    \n",
    "    if parallel:\n",
    "        max_thread = 0\n",
    "    else:\n",
    "        max_thread = 1\n",
    "    \n",
    "    if return_intermediate:\n",
    "        Comp, rX, it, Obj, Time, comp_List = cp_kmpp_d0_dist(\n",
    "            1, np.asfortranarray(features.T), first_edge, adj_vertices,\n",
    "            edge_weights=reg_strength * graph_nn[\"edge_weight\"][reindex], min_comp_weight=cutoff,\n",
    "            cp_dif_tol=1e-2, cp_it_max=10, split_damp_ratio=0.7, verbose=False, \n",
    "            max_num_threads=max_thread, compute_Com=True, compute_Obj=True, compute_Time=True,\n",
    "            balance_parallel_split=balance)\n",
    "        return comp_List, Comp, (Obj, Time, rX)\n",
    "    else:\n",
    "        Comp, rX, it, comp_List = cp_kmpp_d0_dist(\n",
    "            1, np.asfortranarray(features.T), first_edge, adj_vertices,\n",
    "            edge_weights=reg_strength * graph_nn[\"edge_weight\"][reindex], min_comp_weight=cutoff,\n",
    "            cp_dif_tol=1e-2, cp_it_max=10, split_damp_ratio=0.7, verbose=False,\n",
    "            max_num_threads=max_thread, compute_Com=True, balance_parallel_split=balance)\n",
    "        return comp_List, Comp, []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spt] *",
   "language": "python",
   "name": "conda-env-spt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
